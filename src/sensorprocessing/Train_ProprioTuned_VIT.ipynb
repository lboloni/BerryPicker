{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Vision Transformer (ViT)\n",
    "\n",
    "We create a sensor processing model using Vision Transformer (ViT) based visual encoding finetuned with proprioception.\n",
    "\n",
    "We start with a pretrained ViT model, then train it to:\n",
    "1. Create a meaningful 128 or 258 dimensional latent representation\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n",
    "The sensor processing object associated with the trained model is in sensorprocessing/sp_vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config, Experiment\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/ssheikholeslami/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/ssheikholeslami/SaharaBerryPickerData/settings-sahara.yaml\n",
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_propriotuned_Vit/vit_large_256_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_propriotuned_Vit/vit_large_256 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# *** Initialize the variables with default values \n",
    "# *** This cell should be tagged as parameters     \n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit\"\n",
    "# Other possible configurations:\n",
    "# run = \"vit_base_128\"  # ViT Base\n",
    "# run = \"vit_large_128\" # ViT Large\n",
    "# run = \"vit_base_256\"  # ViT Base\n",
    "run = \"vit_large_256\" # ViT Large\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    external_path.exists()\n",
    "    Config().set_experiment_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training.\n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate.\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        print(f\"Loading demonstrations from {task_dir}\")\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                continue\n",
    "            print(f\"Processing demonstration: {demo_dir.name}\")\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "            for i in range(bcd.trim_from, bcd.trim_to):\n",
    "                sensor_readings, _ = bcd.get_image(i)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = bcd.get_a(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "        print(f\"Saved {len(inputlist)} training examples\")\n",
    "\n",
    "    # Separate the training and validation data.\n",
    "    # We will be shuffling the demonstrations\n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length)\n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int(length * 0.67)\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    print(f\"Created {retval['inputs_training'].size(0)} training examples and {retval['inputs_validation'].size(0)} validation examples\")\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1168 training examples and 576 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT model with proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Latent dimension: 256\n",
      "  Image size: 224x224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 256\n",
      "Created latent representation: 1024 → 512 → 256\n",
      "Created proprioceptor: 256 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Warning: Model file /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/proprioception_mlp.pth does not exist. Using untrained model.\n",
      "Model created successfully\n",
      "Parameters accessed successfully\n",
      "Total parameters: 304046086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the ViT model with proprioception\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "\n",
    "# Debug code\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "    # Check individual components\n",
    "    try:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        print(\"Backbone parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing backbone parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        projection_params = model.projection.parameters()\n",
    "        print(\"Projection parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing projection parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        proprioceptor_params = model.proprioceptor.parameters()\n",
    "        print(\"Proprioceptor parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing proprioceptor parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000260.pth (Epoch 260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3915913/3274243283.py:207: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best validation loss: 0.0140\n",
      "Continuing training for 40 more epochs\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000261.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0122\n",
      "Epoch [261/300], Train Loss: 0.0182, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000262.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000259.pth\n",
      "Epoch [262/300], Train Loss: 0.0180, Val Loss: 0.0124\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000263.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000261.pth\n",
      "Epoch [263/300], Train Loss: 0.0182, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000264.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000262.pth\n",
      "Epoch [264/300], Train Loss: 0.0176, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000265.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000263.pth\n",
      "Epoch [265/300], Train Loss: 0.0180, Val Loss: 0.0124\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000266.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000264.pth\n",
      "Epoch [266/300], Train Loss: 0.0186, Val Loss: 0.0129\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000267.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000265.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0120\n",
      "Epoch [267/300], Train Loss: 0.0176, Val Loss: 0.0120\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000268.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000266.pth\n",
      "Epoch [268/300], Train Loss: 0.0174, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000269.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000267.pth\n",
      "Epoch [269/300], Train Loss: 0.0183, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000270.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000268.pth\n",
      "Epoch [270/300], Train Loss: 0.0178, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000271.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000269.pth\n",
      "Epoch [271/300], Train Loss: 0.0177, Val Loss: 0.0127\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000272.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000270.pth\n",
      "Epoch [272/300], Train Loss: 0.0180, Val Loss: 0.0124\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000273.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000271.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0118\n",
      "Epoch [273/300], Train Loss: 0.0180, Val Loss: 0.0118\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000274.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000272.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0116\n",
      "Epoch [274/300], Train Loss: 0.0182, Val Loss: 0.0116\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000275.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000273.pth\n",
      "Epoch [275/300], Train Loss: 0.0179, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000276.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000274.pth\n",
      "Epoch [276/300], Train Loss: 0.0176, Val Loss: 0.0119\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000277.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000275.pth\n",
      "Epoch [277/300], Train Loss: 0.0180, Val Loss: 0.0121\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000278.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000276.pth\n",
      "Epoch [278/300], Train Loss: 0.0185, Val Loss: 0.0121\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000279.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000277.pth\n",
      "Epoch [279/300], Train Loss: 0.0179, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000280.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000278.pth\n",
      "Epoch [280/300], Train Loss: 0.0190, Val Loss: 0.0117\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000281.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000279.pth\n",
      "Epoch [281/300], Train Loss: 0.0179, Val Loss: 0.0119\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000282.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000280.pth\n",
      "Epoch [282/300], Train Loss: 0.0178, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000283.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000281.pth\n",
      "Epoch [283/300], Train Loss: 0.0181, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000284.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000282.pth\n",
      "Epoch [284/300], Train Loss: 0.0186, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000285.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000283.pth\n",
      "Epoch [285/300], Train Loss: 0.0177, Val Loss: 0.0128\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000286.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000284.pth\n",
      "Epoch [286/300], Train Loss: 0.0187, Val Loss: 0.0121\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000287.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000285.pth\n",
      "Epoch [287/300], Train Loss: 0.0180, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000288.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000286.pth\n",
      "Epoch [288/300], Train Loss: 0.0182, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000289.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000287.pth\n",
      "Epoch [289/300], Train Loss: 0.0183, Val Loss: 0.0129\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000290.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000288.pth\n",
      "Epoch [290/300], Train Loss: 0.0187, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000291.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000289.pth\n",
      "Epoch [291/300], Train Loss: 0.0174, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000292.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000290.pth\n",
      "Epoch [292/300], Train Loss: 0.0183, Val Loss: 0.0120\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000293.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000291.pth\n",
      "Epoch [293/300], Train Loss: 0.0174, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000294.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000292.pth\n",
      "Epoch [294/300], Train Loss: 0.0177, Val Loss: 0.0122\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000295.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000293.pth\n",
      "Epoch [295/300], Train Loss: 0.0183, Val Loss: 0.0119\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000296.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000294.pth\n",
      "Epoch [296/300], Train Loss: 0.0180, Val Loss: 0.0119\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000297.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000295.pth\n",
      "Epoch [297/300], Train Loss: 0.0173, Val Loss: 0.0128\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000298.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000296.pth\n",
      "Epoch [298/300], Train Loss: 0.0177, Val Loss: 0.0119\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000299.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000297.pth\n",
      "Epoch [299/300], Train Loss: 0.0179, Val Loss: 0.0121\n",
      "Checkpoint saved: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000300.pth\n",
      "Deleted old checkpoint: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/checkpoints/epoch_000298.pth\n",
      "Epoch [300/300], Train Loss: 0.0178, Val Loss: 0.0122\n",
      "Training complete. Best validation loss: 0.0116\n",
      "Best model saved to /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/proprioception_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                        log_interval=1, start_epoch=0):\n",
    "    \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "    Args:\n",
    "        model: ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "        start_epoch: Starting epoch for resumed training\n",
    "    \"\"\"\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Create checkpoints directory if it doesn't exist\n",
    "    model_dir = os.path.dirname(modelfile)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Path for best model in checkpoints directory\n",
    "    best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "\n",
    "    # List to keep track of saved checkpoint files\n",
    "    saved_checkpoints = []\n",
    "\n",
    "    # Find existing checkpoints to add to our tracking list\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    existing_checkpoints = glob.glob(pattern)\n",
    "    for checkpoint in existing_checkpoints:\n",
    "        match = re.search(r'epoch_(\\d+)\\.pth$', checkpoint)\n",
    "        if match:\n",
    "            epoch_num = int(match.group(1))\n",
    "            if epoch_num < start_epoch:  # Only add checkpoints from before our start epoch\n",
    "                saved_checkpoints.append((epoch_num, checkpoint))\n",
    "\n",
    "    # Sort by epoch number\n",
    "    saved_checkpoints.sort()\n",
    "\n",
    "    # Keep only the 2 most recent existing checkpoints\n",
    "    while len(saved_checkpoints) > 2:\n",
    "        epoch_num, oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "        try:\n",
    "            os.remove(oldest_checkpoint)\n",
    "            print(f\"Deleted old existing checkpoint: {oldest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "    # Convert to just filenames for simplicity\n",
    "    saved_checkpoints = [checkpoint for _, checkpoint in saved_checkpoints]\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Save checkpoint for current epoch - using 6-digit epoch number format\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, f\"epoch_{epoch+1:06d}.pth\")\n",
    "\n",
    "        # Create checkpoint with all necessary information to resume training\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "\n",
    "        if scheduler is not None:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "        # Add to our list of saved checkpoints\n",
    "        saved_checkpoints.append(checkpoint_file)\n",
    "\n",
    "        # Keep only the 2 most recent checkpoints\n",
    "        while len(saved_checkpoints) > 2:\n",
    "            oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "            try:\n",
    "                os.remove(oldest_checkpoint)\n",
    "                print(f\"Deleted old checkpoint: {oldest_checkpoint}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Track the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save a copy of the best model state\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            # Save as best_model.pth in checkpoints directory\n",
    "            torch.save(best_model_state, best_model_path)\n",
    "            print(f\"  New best model saved in checkpoints with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model to the final location only at the end of training\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, modelfile)\n",
    "        print(f\"Best model saved to {modelfile}\")\n",
    "    else:\n",
    "        # If for some reason we don't have a best model, save the final one\n",
    "        torch.save(model.state_dict(), modelfile)\n",
    "        print(f\"Final model saved to {modelfile}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def find_latest_checkpoint(model_dir):\n",
    "    \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None, 0\n",
    "\n",
    "    # Look for checkpoint files\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "\n",
    "    # Extract epoch numbers and find the latest one\n",
    "    epoch_numbers = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(re.search(r'epoch_(\\d+)\\.pth$', file).group(1))\n",
    "            epoch_numbers.append((epoch, file))\n",
    "        except (ValueError, IndexError, AttributeError):\n",
    "            continue\n",
    "\n",
    "    if not epoch_numbers:\n",
    "        return None, 0\n",
    "\n",
    "    # Sort and get the latest\n",
    "    epoch_numbers.sort(reverse=True)\n",
    "    latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "    return latest_file, latest_epoch\n",
    "\n",
    "# Main code to use the updated training function\n",
    "model_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "modelfile = model_dir / exp[\"proprioception_mlp_model_file\"]\n",
    "epochs = exp.get(\"epochs\", 300)  # Default to 300 epochs\n",
    "\n",
    "# Check for latest checkpoint or existing model\n",
    "latest_checkpoint, start_epoch = find_latest_checkpoint(model_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Continue training from the next epoch\n",
    "    print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing final model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Optional: evaluate the loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Start fresh training\n",
    "    print(f\"Starting fresh training for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Training new model for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                        log_interval=1, start_epoch=0):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#         start_epoch: Starting epoch for resumed training\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Create directory for checkpoint files if it doesn't exist\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "#     # Base filename for checkpoints\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Save checkpoint for every epoch\n",
    "#         checkpoint_file = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch+1}.pt\")\n",
    "\n",
    "#         # Create checkpoint with all necessary information to resume training\n",
    "#         checkpoint = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'best_val_loss': best_val_loss,\n",
    "#             'train_loss': avg_train_loss,\n",
    "#             'val_loss': avg_val_loss\n",
    "#         }\n",
    "\n",
    "#         if scheduler is not None:\n",
    "#             checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "#         torch.save(checkpoint, checkpoint_file)\n",
    "#         print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model separately\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#         # Delete older checkpoints (keep only every 2nd epoch)\n",
    "#         if epoch >= 2:  # Start deleting after we have some checkpoints\n",
    "#             old_checkpoint = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch-1}.pt\")\n",
    "#             if os.path.exists(old_checkpoint) and (epoch-1) % 2 != 0:  # Delete if not divisible by 2\n",
    "#                 try:\n",
    "#                     os.remove(old_checkpoint)\n",
    "#                     print(f\"Deleted old checkpoint: {old_checkpoint}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to delete checkpoint {old_checkpoint}: {e}\")\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model\n",
    "\n",
    "# def find_latest_checkpoint(modelfile):\n",
    "#     \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Look for checkpoint files\n",
    "#     pattern = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_*.pt\")\n",
    "#     checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "#     if not checkpoint_files:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Extract epoch numbers and find the latest one\n",
    "#     epoch_numbers = []\n",
    "#     for file in checkpoint_files:\n",
    "#         try:\n",
    "#             epoch = int(file.split('_epoch_')[1].split('.pt')[0])\n",
    "#             epoch_numbers.append((epoch, file))\n",
    "#         except (ValueError, IndexError):\n",
    "#             continue\n",
    "\n",
    "#     if not epoch_numbers:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Sort and get the latest\n",
    "#     epoch_numbers.sort(reverse=True)\n",
    "#     latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "#     return latest_file, latest_epoch\n",
    "\n",
    "# # Modified main code to use the updated training function\n",
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check for latest checkpoint or existing model\n",
    "# latest_checkpoint, start_epoch = find_latest_checkpoint(modelfile)\n",
    "\n",
    "# if latest_checkpoint:\n",
    "#     print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "#     checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#     if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "#         lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#     best_val_loss = checkpoint['best_val_loss']\n",
    "#     print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#     # Continue training from the next epoch\n",
    "#     print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "#         start_epoch=start_epoch\n",
    "#     )\n",
    "\n",
    "# elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing final model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # Ask if we should continue training\n",
    "#     print(f\"Starting fresh training for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )\n",
    "\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                         device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                         log_interval=1):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check if model already exists\n",
    "# if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Latent dimension: 256\n",
      "  Image size: 224x224\n",
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 256\n",
      "Created latent representation: 1024 → 512 → 256\n",
      "Created proprioceptor: 256 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading ViT encoder weights from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/proprioception_mlp.pth\n",
      "\n",
      "Testing sensor processing on random examples:\n",
      "--------------------------------------------------\n",
      "Example 1:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.11979929 0.64397836 0.59729326 0.37931362 0.10096735 0.35096252]\n",
      "\n",
      "Example 2:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.87659013 0.45942882 0.7176128  0.7811049  0.9491138  0.50810045]\n",
      "\n",
      "Example 3:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.82048774 0.04234495 0.32823715 0.6387378  0.84225965 0.1409282 ]\n",
      "\n",
      "Example 4:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.792582   0.68682444 0.30891448 0.7993048  0.51806885 0.940006  ]\n",
      "\n",
      "Example 5:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.3620077  0.49729246 0.3934126  0.60049564 0.8528386  0.11399435]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the sensor processing module using the trained model\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "\n",
    "# Test it on a few validation examples\n",
    "def test_sensor_processing(sp, test_images, test_targets, n_samples=5):\n",
    "    \"\"\"Test the sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_images):\n",
    "        n_samples = len(test_images)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_images))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting sensor processing on random examples:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and target\n",
    "        image = test_images[idx].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        # Process the image to get the latent representation\n",
    "        latent = sp.process(image)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Image shape: {image.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "# Test the sensor processing\n",
    "test_sensor_processing(sp, inputs_validation, targets_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representation shape: torch.Size([1, 256])\n",
      "Robot position prediction shape: torch.Size([1, 6])\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify that the encoding method works correctly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample image\n",
    "    sample_image = inputs_validation[0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the latent representation using encode\n",
    "    latent = model.encode(sample_image)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get the robot position prediction using forward\n",
    "    position = model.forward(sample_image)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Check that the latent representation has the expected size\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    # Check that the position prediction has the expected size\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large_256/proprioception_mlp.pth\n",
      "\n",
      "Training complete!\n",
      "Vision Transformer type: vit_l_16\n",
      "Latent space dimension: 256\n",
      "Output dimension (robot DOF): 6\n",
      "Use the VitSensorProcessing class to load and use this model for inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and print summary\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the VitSensorProcessing class to load and use this model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
