{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Vision Transformer (ViT)\n",
    "\n",
    "We create a sensor processing model using Vision Transformer (ViT) based visual encoding finetuned with proprioception.\n",
    "\n",
    "We start with a pretrained ViT model, then train it to:\n",
    "1. Create a meaningful 128 or 258 dimensional latent representation\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n",
    "The sensor processing object associated with the trained model is in sensorprocessing/sp_vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config, Experiment\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "import sensorprocessing.sp_helper as sp_helper\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values\n",
    "# *** This cell should be tagged as parameters\n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit\"\n",
    "# Other possible configurations:\n",
    "# run = \"vit_base_128\"  # ViT Base\n",
    "run = \"vit_large_128\" # ViT Large\n",
    "# run = \"vit_base_256\"  # ViT Base\n",
    "# run = \"vit_large_256\" # ViT Large\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "flow_name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/sa641631/.config/BerryPicker/mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_Vit/vit_large_128 successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option 1: Use flow-style setup (recommended)\n",
    "if flow_name:\n",
    "    from visual_proprioception import visproprio_helper\n",
    "    exprun_path, result_path = visproprio_helper.external_setup(\n",
    "        flow_name,\n",
    "        pathlib.Path(Config()[\"flows_path\"]).expanduser()\n",
    "    )\n",
    "\n",
    "# Option 2: Use papermill-style paths (when called from Flow)\n",
    "elif external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "\n",
    "    if data_path:\n",
    "        data_path = pathlib.Path(data_path)\n",
    "        assert data_path.exists()\n",
    "        Config().set_results_path(data_path)\n",
    "\n",
    "# Option 3: Use default paths (no external_path or flow_name set)\n",
    "# Just uses ~/WORK/BerryPicker/data/ and source experiment_configs/\n",
    "\n",
    "# The experiment/run we are going to run\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_Vit/vit_large_128 successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# if external_path:\n",
    "#     external_path = pathlib.Path(external_path)\n",
    "#     assert external_path.exists()\n",
    "#     Config().set_exprun_path(external_path)\n",
    "#     Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "#     Config().copy_experiment(\"robot_al5d\")\n",
    "#     Config().copy_experiment(\"demonstration\")\n",
    "# if data_path:\n",
    "#     data_path = pathlib.Path(data_path)\n",
    "#     assert data_path.exists()\n",
    "#     Config().set_results_path(data_path)\n",
    "\n",
    "# # This is an example of how to run an exprun variant\n",
    "# # Config().create_exprun_variant(\"sensorprocessing_propriotuned_cnn\",\"resnet50_128\", {\"epochs\": 17}, new_run_name=\"boo\")\n",
    "\n",
    "# # The experiment/run we are going to run: the specified model will be created\n",
    "# exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "# exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(exp: Experiment, exp_robot: Experiment):\n",
    "    \"\"\"Loads the training images specified in the exp/run. Processes them as two tensors as input and target data for proprioception training.\n",
    "    Caches the processed results into the input and target file specified in the exp/run.\n",
    "\n",
    "    Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    proprioception_input_path = pathlib.Path(exp.data_dir(), \"proprio_input.pth\")\n",
    "    proprioception_target_path = pathlib.Path(exp.data_dir(), \"proprio_target.pth\")\n",
    "\n",
    "    if proprioception_input_path.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_path, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_path, weights_only=True)\n",
    "    else:\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "        transform = sp_helper.get_transform_to_sp(exp)\n",
    "        for val in exp[\"training_data\"]:\n",
    "            run, demo_name, camera = val\n",
    "            #run = val[0]\n",
    "            #demo_name = val[1]\n",
    "            #camera = val[2]\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "            for i in range(demo.metadata[\"maxsteps\"]):\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                rp = demo.get_action(i, \"rc-position-target\", exp_robot)\n",
    "                # rp = RobotPosition.from_vector(exp_robot, a)\n",
    "                anorm = rp.to_normalized_vector(exp_robot)\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_path)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_path)\n",
    "\n",
    "    # Separate the training and validation data.\n",
    "    # We will be shuffling the demonstrations\n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length)\n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "modelfile = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "\n",
    "\n",
    "# data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "# data_dir.mkdir(parents=True, exist_ok=True)\n",
    "# print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# task = exp[\"proprioception_training_task\"]\n",
    "# proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "# proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "\n",
    "tr = load_images_as_proprioception_training(exp, exp_robot)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT model with proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Latent dimension: 128\n",
      "  Image size: [224, 224]\n",
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 128\n",
      "Created latent representation: 1024 → 512 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Warning: Model file /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_large_128/proprioception_mlp.pth does not exist. Using untrained model.\n",
      "Model created successfully\n",
      "Parameters accessed successfully\n",
      "Total parameters: 304004998\n"
     ]
    }
   ],
   "source": [
    "# Create the ViT model with proprioception\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "\n",
    "# Debug code\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "    # Check individual components\n",
    "    try:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        print(\"Backbone parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing backbone parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        projection_params = model.projection.parameters()\n",
    "        print(\"Projection parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing projection parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        proprioceptor_params = model.proprioceptor.parameters()\n",
    "        print(\"Proprioceptor parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing proprioceptor parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_large_128/checkpoints/epoch_000007.pth (Epoch 7)\n",
      "Previous best validation loss: 0.0228\n",
      "Continuing training for 293 more epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 219\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# Continue training from the next epoch\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContinuing training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m more epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     model = \u001b[43mtrain_and_save_proprioception_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_epoch\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m modelfile.exists() \u001b[38;5;129;01mand\u001b[39;00m exp.get(\u001b[33m\"\u001b[39m\u001b[33mreload_existing_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading existing final model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mtrain_and_save_proprioception_model\u001b[39m\u001b[34m(model, criterion, optimizer, modelfile, device, epochs, scheduler, log_interval, start_epoch)\u001b[39m\n\u001b[32m     75\u001b[39m batch_y = batch_y.to(device)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Forward pass through the full model (including proprioceptor)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m loss = criterion(predictions, batch_y)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/sensorprocessing/../sensorprocessing/sp_vit.py:167\u001b[39m, in \u001b[36mViTEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    164\u001b[39m x = \u001b[38;5;28mself\u001b[39m.resize(x)\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Use the full ViT model with our custom head\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m latent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Resize and normalize input\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.size(\u001b[32m2\u001b[39m) != x.size(\u001b[32m3\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m x.size(\u001b[32m2\u001b[39m) != \u001b[38;5;28mself\u001b[39m.resize.size[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torchvision/models/vision_transformer.py:298\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    295\u001b[39m batch_class_token = \u001b[38;5;28mself\u001b[39m.class_token.expand(n, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m x = torch.cat([batch_class_token, x], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[32m    301\u001b[39m x = x[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torchvision/models/vision_transformer.py:157\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    155\u001b[39m torch._assert(\u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m3\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m + \u001b[38;5;28mself\u001b[39m.pos_embedding\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torchvision/models/vision_transformer.py:114\u001b[39m, in \u001b[36mEncoderBlock.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    112\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    113\u001b[39m x, _ = \u001b[38;5;28mself\u001b[39m.self_attention(x, x, x, need_weights=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m x = x + \u001b[38;5;28minput\u001b[39m\n\u001b[32m    117\u001b[39m y = \u001b[38;5;28mself\u001b[39m.ln_2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/BerryPicker/vm/berrypickervenv/lib/python3.11/site-packages/torch/_VF.py:27\u001b[39m, in \u001b[36mVFModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(name)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.vf = torch._C._VariableFunctions\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.vf, name)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                        log_interval=1, start_epoch=0):\n",
    "    \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "    Args:\n",
    "        model: ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "        start_epoch: Starting epoch for resumed training\n",
    "    \"\"\"\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Create checkpoints directory if it doesn't exist\n",
    "    model_dir = os.path.dirname(modelfile)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Path for best model in checkpoints directory\n",
    "    best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "\n",
    "    # List to keep track of saved checkpoint files\n",
    "    saved_checkpoints = []\n",
    "\n",
    "    # Find existing checkpoints to add to our tracking list\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    existing_checkpoints = glob.glob(pattern)\n",
    "    for checkpoint in existing_checkpoints:\n",
    "        match = re.search(r'epoch_(\\d+)\\.pth$', checkpoint)\n",
    "        if match:\n",
    "            epoch_num = int(match.group(1))\n",
    "            if epoch_num < start_epoch:  # Only add checkpoints from before our start epoch\n",
    "                saved_checkpoints.append((epoch_num, checkpoint))\n",
    "\n",
    "    # Sort by epoch number\n",
    "    saved_checkpoints.sort()\n",
    "\n",
    "    # Keep only the 2 most recent existing checkpoints\n",
    "    while len(saved_checkpoints) > 2:\n",
    "        epoch_num, oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "        try:\n",
    "            os.remove(oldest_checkpoint)\n",
    "            print(f\"Deleted old existing checkpoint: {oldest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "    # Convert to just filenames for simplicity\n",
    "    saved_checkpoints = [checkpoint for _, checkpoint in saved_checkpoints]\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Save checkpoint for current epoch - using 6-digit epoch number format\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, f\"epoch_{epoch+1:06d}.pth\")\n",
    "\n",
    "        # Create checkpoint with all necessary information to resume training\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "\n",
    "        if scheduler is not None:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "        # Add to our list of saved checkpoints\n",
    "        saved_checkpoints.append(checkpoint_file)\n",
    "\n",
    "        # Keep only the 2 most recent checkpoints\n",
    "        while len(saved_checkpoints) > 2:\n",
    "            oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "            try:\n",
    "                os.remove(oldest_checkpoint)\n",
    "                print(f\"Deleted old checkpoint: {oldest_checkpoint}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Track the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save a copy of the best model state\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            # Save as best_model.pth in checkpoints directory\n",
    "            torch.save(best_model_state, best_model_path)\n",
    "            print(f\"  New best model saved in checkpoints with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model to the final location only at the end of training\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, modelfile)\n",
    "        print(f\"Best model saved to {modelfile}\")\n",
    "    else:\n",
    "        # If for some reason we don't have a best model, save the final one\n",
    "        torch.save(model.state_dict(), modelfile)\n",
    "        print(f\"Final model saved to {modelfile}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def find_latest_checkpoint(model_dir):\n",
    "    \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None, 0\n",
    "\n",
    "    # Look for checkpoint files\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "\n",
    "    # Extract epoch numbers and find the latest one\n",
    "    epoch_numbers = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(re.search(r'epoch_(\\d+)\\.pth$', file).group(1))\n",
    "            epoch_numbers.append((epoch, file))\n",
    "        except (ValueError, IndexError, AttributeError):\n",
    "            continue\n",
    "\n",
    "    if not epoch_numbers:\n",
    "        return None, 0\n",
    "\n",
    "    # Sort and get the latest\n",
    "    epoch_numbers.sort(reverse=True)\n",
    "    latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "    return latest_file, latest_epoch\n",
    "\n",
    "# Main code to use the updated training function\n",
    "model_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "modelfile = model_dir / exp[\"proprioception_mlp_model_file\"]\n",
    "epochs = exp.get(\"epochs\", 300)  # Default to 300 epochs\n",
    "\n",
    "# Check for latest checkpoint or existing model\n",
    "latest_checkpoint, start_epoch = find_latest_checkpoint(model_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Continue training from the next epoch\n",
    "    print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing final model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Optional: evaluate the loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Start fresh training\n",
    "    print(f\"Starting fresh training for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Training new model for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                        log_interval=1, start_epoch=0):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#         start_epoch: Starting epoch for resumed training\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Create directory for checkpoint files if it doesn't exist\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "#     # Base filename for checkpoints\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Save checkpoint for every epoch\n",
    "#         checkpoint_file = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch+1}.pt\")\n",
    "\n",
    "#         # Create checkpoint with all necessary information to resume training\n",
    "#         checkpoint = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'best_val_loss': best_val_loss,\n",
    "#             'train_loss': avg_train_loss,\n",
    "#             'val_loss': avg_val_loss\n",
    "#         }\n",
    "\n",
    "#         if scheduler is not None:\n",
    "#             checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "#         torch.save(checkpoint, checkpoint_file)\n",
    "#         print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model separately\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#         # Delete older checkpoints (keep only every 2nd epoch)\n",
    "#         if epoch >= 2:  # Start deleting after we have some checkpoints\n",
    "#             old_checkpoint = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch-1}.pt\")\n",
    "#             if os.path.exists(old_checkpoint) and (epoch-1) % 2 != 0:  # Delete if not divisible by 2\n",
    "#                 try:\n",
    "#                     os.remove(old_checkpoint)\n",
    "#                     print(f\"Deleted old checkpoint: {old_checkpoint}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to delete checkpoint {old_checkpoint}: {e}\")\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model\n",
    "\n",
    "# def find_latest_checkpoint(modelfile):\n",
    "#     \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Look for checkpoint files\n",
    "#     pattern = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_*.pt\")\n",
    "#     checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "#     if not checkpoint_files:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Extract epoch numbers and find the latest one\n",
    "#     epoch_numbers = []\n",
    "#     for file in checkpoint_files:\n",
    "#         try:\n",
    "#             epoch = int(file.split('_epoch_')[1].split('.pt')[0])\n",
    "#             epoch_numbers.append((epoch, file))\n",
    "#         except (ValueError, IndexError):\n",
    "#             continue\n",
    "\n",
    "#     if not epoch_numbers:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Sort and get the latest\n",
    "#     epoch_numbers.sort(reverse=True)\n",
    "#     latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "#     return latest_file, latest_epoch\n",
    "\n",
    "# # Modified main code to use the updated training function\n",
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check for latest checkpoint or existing model\n",
    "# latest_checkpoint, start_epoch = find_latest_checkpoint(modelfile)\n",
    "\n",
    "# if latest_checkpoint:\n",
    "#     print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "#     checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#     if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "#         lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#     best_val_loss = checkpoint['best_val_loss']\n",
    "#     print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#     # Continue training from the next epoch\n",
    "#     print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "#         start_epoch=start_epoch\n",
    "#     )\n",
    "\n",
    "# elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing final model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # Ask if we should continue training\n",
    "#     print(f\"Starting fresh training for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )\n",
    "\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                         device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                         log_interval=1):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check if model already exists\n",
    "# if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_b_16\n",
      "  Latent dimension: 128\n",
      "  Image size: 224x224\n",
      "Using vit_b_16 with output dimension 768\n",
      "Created projection network: 768 → 512 → 256 → 128\n",
      "Created latent representation: 768 → 512 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading ViT encoder weights from /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_128/proprioception_mlp.pth\n",
      "\n",
      "Testing sensor processing on random examples:\n",
      "--------------------------------------------------\n",
      "Example 1:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.34495702 0.3687918  0.5551634  0.67185307 0.27943787 0.36896074]\n",
      "\n",
      "Example 2:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.19277683 0.7817534  0.36145413 0.26623115 0.8637374  0.07571162]\n",
      "\n",
      "Example 3:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.82048774 0.2852021  0.4699038  0.8281899  0.5589263  0.1725149 ]\n",
      "\n",
      "Example 4:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.5378262  0.5674465  0.7083333  0.33121446 0.36834922 0.4771756 ]\n",
      "\n",
      "Example 5:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.68525875 0.17850569 0.5411512  0.89081293 0.9955605  0.00693344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the sensor processing module using the trained model\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "\n",
    "# Test it on a few validation examples\n",
    "def test_sensor_processing(sp, test_images, test_targets, n_samples=5):\n",
    "    \"\"\"Test the sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_images):\n",
    "        n_samples = len(test_images)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_images))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting sensor processing on random examples:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and target\n",
    "        image = test_images[idx].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        # Process the image to get the latent representation\n",
    "        latent = sp.process(image)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Image shape: {image.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "# Test the sensor processing\n",
    "test_sensor_processing(sp, inputs_validation, targets_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representation shape: torch.Size([1, 128])\n",
      "Robot position prediction shape: torch.Size([1, 6])\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify that the encoding method works correctly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample image\n",
    "    sample_image = inputs_validation[0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the latent representation using encode\n",
    "    latent = model.encode(sample_image)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get the robot position prediction using forward\n",
    "    position = model.forward(sample_image)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Check that the latent representation has the expected size\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    # Check that the position prediction has the expected size\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_128/proprioception_mlp.pth\n",
      "\n",
      "Training complete!\n",
      "Vision Transformer type: vit_b_16\n",
      "Latent space dimension: 128\n",
      "Output dimension (robot DOF): 6\n",
      "Use the VitSensorProcessing class to load and use this model for inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and print summary\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the VitSensorProcessing class to load and use this model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
