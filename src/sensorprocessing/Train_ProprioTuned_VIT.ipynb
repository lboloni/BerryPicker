{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Vision Transformer (ViT)\n",
    "\n",
    "We create a sensor processing model using Vision Transformer (ViT) based visual encoding finetuned with proprioception.\n",
    "\n",
    "We start with a pretrained ViT model, then train it to:\n",
    "1. Create a meaningful 128 or 258 dimensional latent representation\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n",
    "The sensor processing object associated with the trained model is in sensorprocessing/sp_vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config, Experiment\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "import sensorprocessing.sp_helper as sp_helper\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values\n",
    "# *** This cell should be tagged as parameters\n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit\"\n",
    "# Other possible configurations:\n",
    "# run = \"vit_base_128\"  # ViT Base\n",
    "# run = \"vit_large_128\" # ViT Large\n",
    "run = \"vit_base_256_009\"  # ViT Base\n",
    "# run = \"vit_large_256\" # ViT Large\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: sensorprocessing_propriotuned_Vit/vit_base_256_005 successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option Use papermill-style paths (when called from Flow)\n",
    "# To:\n",
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path).expanduser()  # Add .expanduser()\n",
    "    external_path.mkdir(parents=True, exist_ok=True)  # Create if needed instead of assert\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")  # Match the experiment name!\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path).expanduser()  # Add .expanduser()\n",
    "    data_path.mkdir(parents=True, exist_ok=True)  # Create if needed\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "# Option 3: Use default paths (no external_path or flow_name set)\n",
    "# Just uses ~/WORK/BerryPicker/data/ and source experiment_configs/\n",
    "\n",
    "# The experiment/run we are going to run\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(exp: Experiment, exp_robot: Experiment):\n",
    "    \"\"\"Loads the training images specified in the exp/run. Processes them as two tensors as input and target data for proprioception training.\n",
    "    Caches the processed results into the input and target file specified in the exp/run.\n",
    "\n",
    "    Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    proprioception_input_path = pathlib.Path(exp.data_dir(), \"proprio_input.pth\")\n",
    "    proprioception_target_path = pathlib.Path(exp.data_dir(), \"proprio_target.pth\")\n",
    "\n",
    "    if proprioception_input_path.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_path, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_path, weights_only=True)\n",
    "    else:\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "        transform = sp_helper.get_transform_to_sp(exp)\n",
    "        for val in exp[\"training_data\"]:\n",
    "            run, demo_name, camera = val\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "            for i in range(demo.metadata[\"maxsteps\"]):\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                rp = demo.get_action(i, \"rc-position-target\", exp_robot)\n",
    "                anorm = rp.to_normalized_vector(exp_robot)\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_path)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_path)\n",
    "\n",
    "    # Separate the training and validation data.\n",
    "    # We will be shuffling the demonstrations\n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length)\n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: demonstration/freeform successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# # Create output directory if it doesn't exist\n",
    "# modelfile = pathlib.Path(\n",
    "#     exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "\n",
    "\n",
    "# # data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "# # data_dir.mkdir(parents=True, exist_ok=True)\n",
    "# # print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# # task = exp[\"proprioception_training_task\"]\n",
    "# # proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "# # proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "\n",
    "# tr = load_images_as_proprioception_training(exp, exp_robot)\n",
    "# inputs_training = tr[\"inputs_training\"]\n",
    "# targets_training = tr[\"targets_training\"]\n",
    "# inputs_validation = tr[\"inputs_validation\"]\n",
    "# targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "\n",
    "\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "\n",
    "if modelfile.exists():\n",
    "    print(\"*** Train-ProprioTuned-ViT ***: NOT training; model already exists\")\n",
    "else:\n",
    "    # Load data only when training\n",
    "    tr = load_images_as_proprioception_training(exp, exp_robot)\n",
    "    inputs_training = tr[\"inputs_training\"]\n",
    "    targets_training = tr[\"targets_training\"]\n",
    "    inputs_validation = tr[\"inputs_validation\"]\n",
    "    targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    batch_size = exp.get('batch_size', 32)\n",
    "    train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "    test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT model with proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_b_16\n",
      "  Latent dimension: 256\n",
      "  Image size: [224, 224]\n",
      "Using vit_b_16 with output dimension 768\n",
      "Created projection network: 768 → 512 → 256 → 256\n",
      "Created latent representation: 768 → 512 → 256\n",
      "Created proprioceptor: 256 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Warning: Model file /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/proprioception_mlp.pth does not exist. Using untrained model.\n",
      "Model created successfully\n",
      "Parameters accessed successfully\n",
      "Total parameters: 86412038\n"
     ]
    }
   ],
   "source": [
    "# Create the ViT model with proprioception\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "\n",
    "# Debug code\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "    # Check individual components\n",
    "    try:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        print(\"Backbone parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing backbone parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        projection_params = model.projection.parameters()\n",
    "        print(\"Projection parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing projection parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        proprioceptor_params = model.proprioceptor.parameters()\n",
    "        print(\"Proprioceptor parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing proprioceptor parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DataLoaders for batching\n",
    "# batch_size = exp.get('batch_size', 32)\n",
    "# train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "# test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model for 100 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000001.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0415\n",
      "Epoch [1/100], Train Loss: 0.1135, Val Loss: 0.0415\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000002.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0310\n",
      "Epoch [2/100], Train Loss: 0.0361, Val Loss: 0.0310\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000003.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000001.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0259\n",
      "Epoch [3/100], Train Loss: 0.0318, Val Loss: 0.0259\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000004.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000002.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0239\n",
      "Epoch [4/100], Train Loss: 0.0293, Val Loss: 0.0239\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000005.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000003.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0233\n",
      "Epoch [5/100], Train Loss: 0.0290, Val Loss: 0.0233\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000006.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000004.pth\n",
      "Epoch [6/100], Train Loss: 0.0281, Val Loss: 0.0240\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000007.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000005.pth\n",
      "Epoch [7/100], Train Loss: 0.0288, Val Loss: 0.0269\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000008.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000006.pth\n",
      "Epoch [8/100], Train Loss: 0.0282, Val Loss: 0.0256\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000009.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000007.pth\n",
      "Epoch [9/100], Train Loss: 0.0297, Val Loss: 0.0282\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000010.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000008.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0218\n",
      "Epoch [10/100], Train Loss: 0.0265, Val Loss: 0.0218\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000011.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000009.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0215\n",
      "Epoch [11/100], Train Loss: 0.0248, Val Loss: 0.0215\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000012.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000010.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0207\n",
      "Epoch [12/100], Train Loss: 0.0244, Val Loss: 0.0207\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000013.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000011.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0198\n",
      "Epoch [13/100], Train Loss: 0.0246, Val Loss: 0.0198\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000014.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000012.pth\n",
      "Epoch [14/100], Train Loss: 0.0235, Val Loss: 0.0204\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000015.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000013.pth\n",
      "Epoch [15/100], Train Loss: 0.0238, Val Loss: 0.0204\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000016.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000014.pth\n",
      "Epoch [16/100], Train Loss: 0.0232, Val Loss: 0.0199\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000017.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000015.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0182\n",
      "Epoch [17/100], Train Loss: 0.0235, Val Loss: 0.0182\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000018.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000016.pth\n",
      "Epoch [18/100], Train Loss: 0.0236, Val Loss: 0.0198\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000019.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000017.pth\n",
      "Epoch [19/100], Train Loss: 0.0238, Val Loss: 0.0194\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000020.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000018.pth\n",
      "Epoch [20/100], Train Loss: 0.0227, Val Loss: 0.0188\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000021.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000019.pth\n",
      "Epoch [21/100], Train Loss: 0.0239, Val Loss: 0.0205\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000022.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000020.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0166\n",
      "Epoch [22/100], Train Loss: 0.0218, Val Loss: 0.0166\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000023.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000021.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0159\n",
      "Epoch [23/100], Train Loss: 0.0216, Val Loss: 0.0159\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000024.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000022.pth\n",
      "Epoch [24/100], Train Loss: 0.0207, Val Loss: 0.0191\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000025.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000023.pth\n",
      "Epoch [25/100], Train Loss: 0.0201, Val Loss: 0.0167\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000026.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000024.pth\n",
      "Epoch [26/100], Train Loss: 0.0203, Val Loss: 0.0170\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000027.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000025.pth\n",
      "Epoch [27/100], Train Loss: 0.0209, Val Loss: 0.0162\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000028.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000026.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0159\n",
      "Epoch [28/100], Train Loss: 0.0205, Val Loss: 0.0159\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000029.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000027.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0151\n",
      "Epoch [29/100], Train Loss: 0.0197, Val Loss: 0.0151\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000030.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000028.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0149\n",
      "Epoch [30/100], Train Loss: 0.0190, Val Loss: 0.0149\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000031.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000029.pth\n",
      "Epoch [31/100], Train Loss: 0.0192, Val Loss: 0.0157\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000032.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000030.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0136\n",
      "Epoch [32/100], Train Loss: 0.0193, Val Loss: 0.0136\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000033.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000031.pth\n",
      "Epoch [33/100], Train Loss: 0.0195, Val Loss: 0.0148\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000034.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000032.pth\n",
      "Epoch [34/100], Train Loss: 0.0190, Val Loss: 0.0148\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000035.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000033.pth\n",
      "Epoch [35/100], Train Loss: 0.0181, Val Loss: 0.0142\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000036.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000034.pth\n",
      "Epoch [36/100], Train Loss: 0.0187, Val Loss: 0.0163\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000037.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000035.pth\n",
      "Epoch [37/100], Train Loss: 0.0180, Val Loss: 0.0146\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000038.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000036.pth\n",
      "Epoch [38/100], Train Loss: 0.0179, Val Loss: 0.0142\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000039.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000037.pth\n",
      "Epoch [39/100], Train Loss: 0.0175, Val Loss: 0.0141\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000040.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000038.pth\n",
      "Epoch [40/100], Train Loss: 0.0178, Val Loss: 0.0149\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000041.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000039.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0133\n",
      "Epoch [41/100], Train Loss: 0.0183, Val Loss: 0.0133\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000042.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000040.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0131\n",
      "Epoch [42/100], Train Loss: 0.0174, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000043.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000041.pth\n",
      "Epoch [43/100], Train Loss: 0.0174, Val Loss: 0.0133\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000044.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000042.pth\n",
      "Epoch [44/100], Train Loss: 0.0173, Val Loss: 0.0137\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000045.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000043.pth\n",
      "Epoch [45/100], Train Loss: 0.0168, Val Loss: 0.0136\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000046.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000044.pth\n",
      "Epoch [46/100], Train Loss: 0.0174, Val Loss: 0.0140\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000047.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000045.pth\n",
      "Epoch [47/100], Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000048.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000046.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0131\n",
      "Epoch [48/100], Train Loss: 0.0169, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000049.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000047.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0131\n",
      "Epoch [49/100], Train Loss: 0.0174, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000050.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000048.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0125\n",
      "Epoch [50/100], Train Loss: 0.0170, Val Loss: 0.0125\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000051.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000049.pth\n",
      "Epoch [51/100], Train Loss: 0.0176, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000052.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000050.pth\n",
      "Epoch [52/100], Train Loss: 0.0174, Val Loss: 0.0134\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000053.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000051.pth\n",
      "Epoch [53/100], Train Loss: 0.0169, Val Loss: 0.0133\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000054.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000052.pth\n",
      "Epoch [54/100], Train Loss: 0.0175, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000055.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000053.pth\n",
      "Epoch [55/100], Train Loss: 0.0174, Val Loss: 0.0138\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000056.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000054.pth\n",
      "Epoch [56/100], Train Loss: 0.0174, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000057.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000055.pth\n",
      "Epoch [57/100], Train Loss: 0.0169, Val Loss: 0.0128\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000058.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000056.pth\n",
      "Epoch [58/100], Train Loss: 0.0170, Val Loss: 0.0135\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000059.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000057.pth\n",
      "Epoch [59/100], Train Loss: 0.0173, Val Loss: 0.0135\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000060.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000058.pth\n",
      "Epoch [60/100], Train Loss: 0.0165, Val Loss: 0.0128\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000061.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000059.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0124\n",
      "Epoch [61/100], Train Loss: 0.0168, Val Loss: 0.0124\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000062.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000060.pth\n",
      "Epoch [62/100], Train Loss: 0.0168, Val Loss: 0.0139\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000063.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000061.pth\n",
      "Epoch [63/100], Train Loss: 0.0173, Val Loss: 0.0134\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000064.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000062.pth\n",
      "Epoch [64/100], Train Loss: 0.0171, Val Loss: 0.0136\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000065.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000063.pth\n",
      "Epoch [65/100], Train Loss: 0.0174, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000066.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000064.pth\n",
      "Epoch [66/100], Train Loss: 0.0166, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000067.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000065.pth\n",
      "  New best model saved in checkpoints with validation loss: 0.0123\n",
      "Epoch [67/100], Train Loss: 0.0165, Val Loss: 0.0123\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000068.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000066.pth\n",
      "Epoch [68/100], Train Loss: 0.0170, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000069.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000067.pth\n",
      "Epoch [69/100], Train Loss: 0.0173, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000070.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000068.pth\n",
      "Epoch [70/100], Train Loss: 0.0167, Val Loss: 0.0148\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000071.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000069.pth\n",
      "Epoch [71/100], Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000072.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000070.pth\n",
      "Epoch [72/100], Train Loss: 0.0175, Val Loss: 0.0125\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000073.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000071.pth\n",
      "Epoch [73/100], Train Loss: 0.0168, Val Loss: 0.0133\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000074.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000072.pth\n",
      "Epoch [74/100], Train Loss: 0.0169, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000075.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000073.pth\n",
      "Epoch [75/100], Train Loss: 0.0166, Val Loss: 0.0125\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000076.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000074.pth\n",
      "Epoch [76/100], Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000077.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000075.pth\n",
      "Epoch [77/100], Train Loss: 0.0171, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000078.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000076.pth\n",
      "Epoch [78/100], Train Loss: 0.0164, Val Loss: 0.0136\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000079.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000077.pth\n",
      "Epoch [79/100], Train Loss: 0.0165, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000080.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000078.pth\n",
      "Epoch [80/100], Train Loss: 0.0172, Val Loss: 0.0137\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000081.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000079.pth\n",
      "Epoch [81/100], Train Loss: 0.0166, Val Loss: 0.0129\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000082.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000080.pth\n",
      "Epoch [82/100], Train Loss: 0.0167, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000083.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000081.pth\n",
      "Epoch [83/100], Train Loss: 0.0169, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000084.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000082.pth\n",
      "Epoch [84/100], Train Loss: 0.0168, Val Loss: 0.0125\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000085.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000083.pth\n",
      "Epoch [85/100], Train Loss: 0.0171, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000086.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000084.pth\n",
      "Epoch [86/100], Train Loss: 0.0169, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000087.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000085.pth\n",
      "Epoch [87/100], Train Loss: 0.0168, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000088.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000086.pth\n",
      "Epoch [88/100], Train Loss: 0.0170, Val Loss: 0.0126\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000089.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000087.pth\n",
      "Epoch [89/100], Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000090.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000088.pth\n",
      "Epoch [90/100], Train Loss: 0.0172, Val Loss: 0.0127\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000091.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000089.pth\n",
      "Epoch [91/100], Train Loss: 0.0169, Val Loss: 0.0128\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000092.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000090.pth\n",
      "Epoch [92/100], Train Loss: 0.0165, Val Loss: 0.0130\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000093.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000091.pth\n",
      "Epoch [93/100], Train Loss: 0.0167, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000094.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000092.pth\n",
      "Epoch [94/100], Train Loss: 0.0171, Val Loss: 0.0132\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000095.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000093.pth\n",
      "Epoch [95/100], Train Loss: 0.0171, Val Loss: 0.0136\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000096.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000094.pth\n",
      "Epoch [96/100], Train Loss: 0.0172, Val Loss: 0.0124\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000097.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000095.pth\n",
      "Epoch [97/100], Train Loss: 0.0168, Val Loss: 0.0129\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000098.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000096.pth\n",
      "Epoch [98/100], Train Loss: 0.0174, Val Loss: 0.0135\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000099.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000097.pth\n",
      "Epoch [99/100], Train Loss: 0.0168, Val Loss: 0.0131\n",
      "Checkpoint saved: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000100.pth\n",
      "Deleted old checkpoint: /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/checkpoints/epoch_000098.pth\n",
      "Epoch [100/100], Train Loss: 0.0168, Val Loss: 0.0125\n",
      "Training complete. Best validation loss: 0.0123\n",
      "Best model saved to /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/proprioception_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                        log_interval=1, start_epoch=0):\n",
    "    \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "    Args:\n",
    "        model: ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "        start_epoch: Starting epoch for resumed training\n",
    "    \"\"\"\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Create checkpoints directory if it doesn't exist\n",
    "    model_dir = os.path.dirname(modelfile)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Path for best model in checkpoints directory\n",
    "    best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "\n",
    "    # List to keep track of saved checkpoint files\n",
    "    saved_checkpoints = []\n",
    "\n",
    "    # Find existing checkpoints to add to our tracking list\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    existing_checkpoints = glob.glob(pattern)\n",
    "    for checkpoint in existing_checkpoints:\n",
    "        match = re.search(r'epoch_(\\d+)\\.pth$', checkpoint)\n",
    "        if match:\n",
    "            epoch_num = int(match.group(1))\n",
    "            if epoch_num < start_epoch:  # Only add checkpoints from before our start epoch\n",
    "                saved_checkpoints.append((epoch_num, checkpoint))\n",
    "\n",
    "    # Sort by epoch number\n",
    "    saved_checkpoints.sort()\n",
    "\n",
    "    # Keep only the 2 most recent existing checkpoints\n",
    "    while len(saved_checkpoints) > 2:\n",
    "        epoch_num, oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "        try:\n",
    "            os.remove(oldest_checkpoint)\n",
    "            print(f\"Deleted old existing checkpoint: {oldest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "    # Convert to just filenames for simplicity\n",
    "    saved_checkpoints = [checkpoint for _, checkpoint in saved_checkpoints]\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Save checkpoint for current epoch - using 6-digit epoch number format\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, f\"epoch_{epoch+1:06d}.pth\")\n",
    "\n",
    "        # Create checkpoint with all necessary information to resume training\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "\n",
    "        if scheduler is not None:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "        # Add to our list of saved checkpoints\n",
    "        saved_checkpoints.append(checkpoint_file)\n",
    "\n",
    "        # Keep only the 2 most recent checkpoints\n",
    "        while len(saved_checkpoints) > 2:\n",
    "            oldest_checkpoint = saved_checkpoints.pop(0)  # Remove the oldest\n",
    "            try:\n",
    "                os.remove(oldest_checkpoint)\n",
    "                print(f\"Deleted old checkpoint: {oldest_checkpoint}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Track the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save a copy of the best model state\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            # Save as best_model.pth in checkpoints directory\n",
    "            torch.save(best_model_state, best_model_path)\n",
    "            print(f\"  New best model saved in checkpoints with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model to the final location only at the end of training\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, modelfile)\n",
    "        print(f\"Best model saved to {modelfile}\")\n",
    "    else:\n",
    "        # If for some reason we don't have a best model, save the final one\n",
    "        torch.save(model.state_dict(), modelfile)\n",
    "        print(f\"Final model saved to {modelfile}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def find_latest_checkpoint(model_dir):\n",
    "    \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None, 0\n",
    "\n",
    "    # Look for checkpoint files\n",
    "    pattern = os.path.join(checkpoint_dir, \"epoch_*.pth\")\n",
    "    checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "\n",
    "    # Extract epoch numbers and find the latest one\n",
    "    epoch_numbers = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(re.search(r'epoch_(\\d+)\\.pth$', file).group(1))\n",
    "            epoch_numbers.append((epoch, file))\n",
    "        except (ValueError, IndexError, AttributeError):\n",
    "            continue\n",
    "\n",
    "    if not epoch_numbers:\n",
    "        return None, 0\n",
    "\n",
    "    # Sort and get the latest\n",
    "    epoch_numbers.sort(reverse=True)\n",
    "    latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "    return latest_file, latest_epoch\n",
    "\n",
    "# Main code to use the updated training function\n",
    "model_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "modelfile = model_dir / exp[\"proprioception_mlp_model_file\"]\n",
    "epochs = exp.get(\"epochs\", 300)  # Default to 300 epochs\n",
    "\n",
    "# Check for latest checkpoint or existing model\n",
    "latest_checkpoint, start_epoch = find_latest_checkpoint(model_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Continue training from the next epoch\n",
    "    print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing final model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Optional: evaluate the loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Start fresh training\n",
    "    print(f\"Starting fresh training for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Training new model for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                        log_interval=1, start_epoch=0):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model with checkpointing and resume capability\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#         start_epoch: Starting epoch for resumed training\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Create directory for checkpoint files if it doesn't exist\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "#     # Base filename for checkpoints\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Save checkpoint for every epoch\n",
    "#         checkpoint_file = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch+1}.pt\")\n",
    "\n",
    "#         # Create checkpoint with all necessary information to resume training\n",
    "#         checkpoint = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'best_val_loss': best_val_loss,\n",
    "#             'train_loss': avg_train_loss,\n",
    "#             'val_loss': avg_val_loss\n",
    "#         }\n",
    "\n",
    "#         if scheduler is not None:\n",
    "#             checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "#         torch.save(checkpoint, checkpoint_file)\n",
    "#         print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model separately\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#         # Delete older checkpoints (keep only every 2nd epoch)\n",
    "#         if epoch >= 2:  # Start deleting after we have some checkpoints\n",
    "#             old_checkpoint = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_{epoch-1}.pt\")\n",
    "#             if os.path.exists(old_checkpoint) and (epoch-1) % 2 != 0:  # Delete if not divisible by 2\n",
    "#                 try:\n",
    "#                     os.remove(old_checkpoint)\n",
    "#                     print(f\"Deleted old checkpoint: {old_checkpoint}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to delete checkpoint {old_checkpoint}: {e}\")\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model\n",
    "\n",
    "# def find_latest_checkpoint(modelfile):\n",
    "#     \"\"\"Find the latest checkpoint file to resume training\"\"\"\n",
    "#     checkpoint_dir = os.path.dirname(modelfile)\n",
    "#     base_filename = os.path.basename(modelfile).split('.')[0]\n",
    "\n",
    "#     # Look for checkpoint files\n",
    "#     pattern = os.path.join(checkpoint_dir, f\"{base_filename}_epoch_*.pt\")\n",
    "#     checkpoint_files = glob.glob(pattern)\n",
    "\n",
    "#     if not checkpoint_files:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Extract epoch numbers and find the latest one\n",
    "#     epoch_numbers = []\n",
    "#     for file in checkpoint_files:\n",
    "#         try:\n",
    "#             epoch = int(file.split('_epoch_')[1].split('.pt')[0])\n",
    "#             epoch_numbers.append((epoch, file))\n",
    "#         except (ValueError, IndexError):\n",
    "#             continue\n",
    "\n",
    "#     if not epoch_numbers:\n",
    "#         return None, 0\n",
    "\n",
    "#     # Sort and get the latest\n",
    "#     epoch_numbers.sort(reverse=True)\n",
    "#     latest_epoch, latest_file = epoch_numbers[0]\n",
    "\n",
    "#     return latest_file, latest_epoch\n",
    "\n",
    "# # Modified main code to use the updated training function\n",
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check for latest checkpoint or existing model\n",
    "# latest_checkpoint, start_epoch = find_latest_checkpoint(modelfile)\n",
    "\n",
    "# if latest_checkpoint:\n",
    "#     print(f\"Resuming training from checkpoint: {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "#     checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#     if 'scheduler_state_dict' in checkpoint and lr_scheduler is not None:\n",
    "#         lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#     best_val_loss = checkpoint['best_val_loss']\n",
    "#     print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#     # Continue training from the next epoch\n",
    "#     print(f\"Continuing training for {epochs - start_epoch} more epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler,\n",
    "#         start_epoch=start_epoch\n",
    "#     )\n",
    "\n",
    "# elif modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing final model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # Ask if we should continue training\n",
    "#     print(f\"Starting fresh training for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )\n",
    "\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "#                                         device=\"cpu\", epochs=20, scheduler=None,\n",
    "#                                         log_interval=1):\n",
    "#     \"\"\"Trains and saves the ViT proprioception model\n",
    "\n",
    "#     Args:\n",
    "#         model: ViT model with proprioception\n",
    "#         criterion: Loss function\n",
    "#         optimizer: Optimizer\n",
    "#         modelfile: Path to save the model\n",
    "#         device: Training device (cpu/cuda)\n",
    "#         epochs: Number of training epochs\n",
    "#         scheduler: Optional learning rate scheduler\n",
    "#         log_interval: How often to print logs\n",
    "#     \"\"\"\n",
    "#     # Ensure model is on the right device\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "\n",
    "#     # Keep track of the best validation loss\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Training loop\n",
    "#     num_epochs = epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_X, batch_y in train_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             # Forward pass through the full model (including proprioceptor)\n",
    "#             predictions = model.forward(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_X, batch_y in test_loader:\n",
    "#                 batch_X = batch_X.to(device)\n",
    "#                 batch_y = batch_y.to(device)\n",
    "#                 predictions = model(batch_X)\n",
    "#                 loss = criterion(predictions, batch_y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "#         # Update learning rate if scheduler is provided\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(avg_val_loss)\n",
    "\n",
    "#         # Save the best model\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), modelfile)\n",
    "#             print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#         # Log progress\n",
    "#         if (epoch + 1) % log_interval == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "#     # Final evaluation\n",
    "#     print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "# epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# # Check if model already exists\n",
    "# if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "#     print(f\"Loading existing model from {modelfile}\")\n",
    "#     model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "#     # Optional: evaluate the loaded model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             predictions = model(batch_X)\n",
    "#             loss = criterion(predictions, batch_y)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(test_loader)\n",
    "#         print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Training new model for {epochs} epochs\")\n",
    "#     model = train_and_save_proprioception_model(\n",
    "#         model, criterion, optimizer, modelfile,\n",
    "#         device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_b_16\n",
      "  Latent dimension: 256\n",
      "  Image size: [224, 224]\n",
      "Using vit_b_16 with output dimension 768\n",
      "Created projection network: 768 → 512 → 256 → 256\n",
      "Created latent representation: 768 → 512 → 256\n",
      "Created proprioceptor: 256 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading ViT encoder weights from /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/proprioception_mlp.pth\n",
      "\n",
      "Testing sensor processing on random examples:\n",
      "--------------------------------------------------\n",
      "Example 1:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.44849464 0.5948256  0.4752168  0.78996444 0.09980548 0.4089145 ]\n",
      "\n",
      "Example 2:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.19872235 0.36896387 0.41966107 0.39732587 0.5613271  0.18225819]\n",
      "\n",
      "Example 3:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.66993386 0.33589852 0.44724858 0.006627   0.27500698 0.1705494 ]\n",
      "\n",
      "Example 4:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.23968594 0.01168595 0.9279559  0.2379951  0.86591303 0.29666167]\n",
      "\n",
      "Example 5:\n",
      "  Image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent shape: (256,)\n",
      "  Target position: [0.19872235 0.05267958 0.2029944  0.39732587 0.5613271  0.18225819]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the sensor processing module using the trained model\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "\n",
    "# Test it on a few validation examples\n",
    "def test_sensor_processing(sp, test_images, test_targets, n_samples=5):\n",
    "    \"\"\"Test the sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_images):\n",
    "        n_samples = len(test_images)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_images))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting sensor processing on random examples:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and target\n",
    "        image = test_images[idx].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        # Process the image to get the latent representation\n",
    "        latent = sp.process(image)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Image shape: {image.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "# Test the sensor processing\n",
    "test_sensor_processing(sp, inputs_validation, targets_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representation shape: torch.Size([1, 256])\n",
      "Robot position prediction shape: torch.Size([1, 6])\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify that the encoding method works correctly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample image\n",
    "    sample_image = inputs_validation[0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the latent representation using encode\n",
    "    latent = model.encode(sample_image)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get the robot position prediction using forward\n",
    "    position = model.forward(sample_image)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Check that the latent representation has the expected size\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    # Check that the position prediction has the expected size\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/sa641631/WORK/BerryPicker/data/sensorprocessing_propriotuned_Vit/vit_base_256_005/proprioception_mlp.pth\n",
      "\n",
      "Training complete!\n",
      "Vision Transformer type: vit_b_16\n",
      "Latent space dimension: 256\n",
      "Output dimension (robot DOF): 6\n",
      "Use the VitSensorProcessing class to load and use this model for inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and print summary\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the VitSensorProcessing class to load and use this model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
