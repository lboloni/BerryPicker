{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Concatenated Image Vision Transformer (ViT)\n",
    "\n",
    "We creates a sensor processing model that concatenates\n",
    "multiple camera view images before processing them through a single Vision Transformer.\n",
    "The model is trained to:\n",
    "1. Create a meaningful 128-dimensional latent representation from concatenated camera views\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "from demonstration.demonstration_helper import BCDemonstration\n",
    "from sensorprocessing.sp_vit_concat_images import ConcatImageVitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview successfully loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The experiment/run we are going to run: the specified model will be created\n",
    "experiment = \"sensorprocessing_propriotuned_Vit_concat_multiview\"\n",
    "\n",
    "\n",
    "# Other possible configurations:\n",
    "\n",
    "#concat_proj\n",
    "# run = \"vit_base_concat_multiview\" # ViT Base\n",
    "run = \"vit_large_concat_multiview\"  # ViT Large\n",
    "# run = \"vit_huge_multiview\" # ViT Huge\n",
    "\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (XX, Y) is all the 2-view pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiview_demonstrations(task, proprioception_input_file, proprioception_target_file, num_views=2):\n",
    "    \"\"\"\n",
    "    Loads all the images of a task from multiple camera views, and processes it as two tensors\n",
    "    as input and target data for proprioception training.\n",
    "\n",
    "    Args:\n",
    "        task: Task name to load demonstrations from\n",
    "        proprioception_input_file: Path to save/load processed inputs\n",
    "        proprioception_target_file: Path to save/load processed targets\n",
    "        num_views: Number of camera views to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training and validation data splits\n",
    "    \"\"\"\n",
    "\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        print(f\"Loading cached data from {proprioception_input_file}\")\n",
    "        retval[\"view_inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "        # Lists to store multi-view images and targets\n",
    "        view_lists = {}  # Dictionary to organize views by camera\n",
    "        targetlist = []\n",
    "\n",
    "        print(f\"Loading demonstrations from {task_dir}\")\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing demonstration: {demo_dir.name}\")\n",
    "            # Create BCDemonstration with multi-camera support\n",
    "            bcd = BCDemonstration(\n",
    "                demo_dir,\n",
    "                sensorprocessor=None,\n",
    "                cameras=None  # This will detect all available cameras\n",
    "            )\n",
    "\n",
    "            # Initialize view lists if not already done\n",
    "            if not view_lists:\n",
    "                for camera in bcd.cameras:\n",
    "                    view_lists[camera] = []\n",
    "\n",
    "            # Process each timestep\n",
    "            for i in range(bcd.trim_from, bcd.trim_to):\n",
    "                # Get all images for this timestep\n",
    "                all_images = bcd.get_all_images(i)\n",
    "\n",
    "                # If we don't have all required views, skip this timestep\n",
    "                if len(all_images) < num_views:\n",
    "                    print(f\"  Skipping timestep {i} - only {len(all_images)}/{num_views} views available\")\n",
    "                    continue\n",
    "\n",
    "                # Collect images from each camera\n",
    "                for camera, (sensor_readings, _) in all_images.items():\n",
    "                    if camera in view_lists:\n",
    "                        view_lists[camera].append(sensor_readings[0])\n",
    "\n",
    "                # Get the robot action for this timestep\n",
    "                a = bcd.get_a(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        # Ensure we have the same number of frames for each view\n",
    "        min_frames = min(len(view_list) for view_list in view_lists.values())\n",
    "        if min_frames < len(targetlist):\n",
    "            print(f\"Truncating dataset to {min_frames} frames (from {len(targetlist)})\")\n",
    "            targetlist = targetlist[:min_frames]\n",
    "            for camera in view_lists:\n",
    "                view_lists[camera] = view_lists[camera][:min_frames]\n",
    "\n",
    "        # Stack tensors for each view\n",
    "        view_tensors = []\n",
    "        for camera in sorted(view_lists.keys())[:num_views]:  # Take only the required number of views\n",
    "            view_tensors.append(torch.stack(view_lists[camera]))\n",
    "\n",
    "        # Create multi-view input tensor list [num_views, num_samples, C, H, W]\n",
    "        retval[\"view_inputs\"] = view_tensors\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "\n",
    "        # Save processed data\n",
    "        torch.save(retval[\"view_inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "        print(f\"Saved {len(targetlist)} training examples with {num_views} views each\")\n",
    "\n",
    "    # Separate the training and validation data\n",
    "    length = len(retval[\"targets\"])\n",
    "    rows = torch.randperm(length)\n",
    "\n",
    "    # Shuffle targets\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    # Shuffle each view input using the same row indices\n",
    "    shuffled_view_inputs = []\n",
    "    for view_tensor in retval[\"view_inputs\"]:\n",
    "        shuffled_view_inputs.append(view_tensor[rows])\n",
    "\n",
    "    # Split into training (67%) and validation (33%) sets\n",
    "    training_size = int(length * 0.67)\n",
    "\n",
    "    # Training data\n",
    "    retval[\"view_inputs_training\"] = [view[:training_size] for view in shuffled_view_inputs]\n",
    "    retval[\"targets_training\"] = shuffled_targets[:training_size]\n",
    "\n",
    "    # Validation data\n",
    "    retval[\"view_inputs_validation\"] = [view[training_size:] for view in shuffled_view_inputs]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    print(f\"Created {training_size} training examples and {length - training_size} validation examples\")\n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview/train_inputs.pt\n",
      "Created 1169 training examples and 576 validation examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the training data\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_multiview_demonstrations(task, proprioception_input_file, proprioception_target_file)\n",
    "view_inputs_training = tr[\"view_inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "view_inputs_validation = tr[\"view_inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-view ViT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Concatenated Image ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Number of views: 2\n",
      "  Latent dimension: 128\n",
      "  Image size: 224x224\n",
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading Concatenated Image ViT encoder weights from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview/proprioception_mlp.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fs1/home/ssheikholeslami/BerryPicker/src/sensorprocessing/../sensorprocessing/sp_vit_concat_images.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.enc.load_state_dict(torch.load(modelfile, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully\n",
      "Parameters accessed successfully\n",
      "Total parameters: 304004998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the multi-view ViT model\n",
    "sp = ConcatImageVitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for multi-view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for multi-view images\n",
    "class MultiViewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, view_inputs, targets):\n",
    "        \"\"\"\n",
    "        Dataset for handling multiple camera views\n",
    "\n",
    "        Args:\n",
    "            view_inputs: List of tensors, one per view [view1, view2, ...]\n",
    "                Each view tensor has shape [num_samples, C, H, W]\n",
    "            targets: Tensor of target values [num_samples, output_dim]\n",
    "        \"\"\"\n",
    "        self.view_inputs = view_inputs  # List of tensors, one per view\n",
    "        self.targets = targets\n",
    "        self.num_samples = len(targets)\n",
    "\n",
    "        # Verify all views have the same number of samples\n",
    "        for view in view_inputs:\n",
    "            assert len(view) == self.num_samples, \"All views must have the same number of samples\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get corresponding sample from each view\n",
    "        views = [view[idx] for view in self.view_inputs]\n",
    "        target = self.targets[idx]\n",
    "        return views, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "train_dataset = MultiViewDataset(view_inputs_training, targets_training)\n",
    "test_dataset = MultiViewDataset(view_inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing with the encoder model\n",
    "def test_concat_model_direct(model, test_view_inputs, test_targets, device, n_samples=5):\n",
    "    \"\"\"Test the concatenated image ViT encoder model directly.\"\"\"\n",
    "    if n_samples > len(test_targets):\n",
    "        n_samples = len(test_targets)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_targets))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting concatenated image ViT model on random examples:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Get views and target\n",
    "            views = [view[idx].unsqueeze(0).to(device) for view in test_view_inputs]\n",
    "            target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "            # Get latent representation directly\n",
    "            latent = model.encode(views)\n",
    "\n",
    "            # Get prediction\n",
    "            prediction = model.proprioceptor(latent)\n",
    "            prediction = prediction.cpu().numpy().flatten()\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"Example {i+1}:\")\n",
    "            for j, view in enumerate(views):\n",
    "                print(f\"  View {j+1} shape: {view.shape}\")\n",
    "            print(f\"  Latent shape: {latent.shape}\")\n",
    "            print(f\"  Target position: {target}\")\n",
    "            print(f\"  Predicted position: {prediction}\")\n",
    "            print(f\"  Mean squared error: {np.mean((prediction - target) ** 2):.6f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_concat_vit_model(model, criterion, optimizer, modelfile,\n",
    "                                  device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                  log_interval=1):\n",
    "    \"\"\"\n",
    "    Trains and saves the concatenated image ViT proprioception model with checkpoint support\n",
    "\n",
    "    Args:\n",
    "        model: Concatenated image ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the final model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = modelfile.parent / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{epochs} with best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (batch_views, batch_y) in enumerate(train_loader):\n",
    "            # Move views and targets to device\n",
    "            batch_views = [view.to(device) for view in batch_views]\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_views)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print progress every few batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_views, batch_y in test_loader:\n",
    "                # Move views and targets to device\n",
    "                batch_views = [view.to(device) for view in batch_views]\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                predictions = model(batch_views)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save checkpoint for this epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Save the best model separately\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with loss {best_checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "    # Save to final model file only after completing all epochs\n",
    "    torch.save(model.state_dict(), modelfile)\n",
    "    print(f\"Final model saved to {modelfile}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing final model from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview/proprioception_mlp.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101379/3836558730.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelfile, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model validation loss: 0.0076\n",
      "\n",
      "Testing concatenated image ViT model on random examples:\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.36338103 0.46871686 0.8122891  0.5504956  0.26950523 0.20594741]\n",
      "  Predicted position: [0.3584069 0.5001888 0.7504191 0.5339994 0.2638954 0.2457543]\n",
      "  Mean squared error: 0.001122\n",
      "\n",
      "Example 2:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.91794205 0.4346888  0.26011056 0.13500535 0.49278548 0.8269405 ]\n",
      "  Predicted position: [0.7771102  0.40149823 0.29160368 0.26021338 0.49528894 0.7670573 ]\n",
      "  Mean squared error: 0.006866\n",
      "\n",
      "Example 3:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.11979929 0.64397836 0.08062661 0.37931362 0.10096735 0.35096252]\n",
      "  Predicted position: [0.16542187 0.5559786  0.16314036 0.39046472 0.16951188 0.3527826 ]\n",
      "  Mean squared error: 0.003577\n",
      "\n",
      "Example 4:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.81011546 0.88800025 0.5702392  0.57235307 0.9491138  0.40154794]\n",
      "  Predicted position: [0.69190836 0.8313831  0.5281374  0.55156124 0.79921013 0.33222508]\n",
      "  Mean squared error: 0.007777\n",
      "\n",
      "Example 5:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.8399459  0.18520209 0.31990382 0.6220711  0.8589263  0.0909282 ]\n",
      "  Predicted position: [0.78199756 0.25456613 0.36009264 0.5631983  0.7585904  0.18871921]\n",
      "  Mean squared error: 0.005480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {modelfile}\")\n",
    "        model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch_views, batch_y in test_loader:\n",
    "                # Move views and targets to device\n",
    "                batch_views = [view.to(device) for view in batch_views]\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                predictions = model(batch_views)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(test_loader)\n",
    "            print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Test model on some examples\n",
    "        test_concat_model_direct(model, view_inputs_validation, targets_validation, device)\n",
    "    else:\n",
    "        # Check for checkpoints to resume from, otherwise start fresh training\n",
    "        checkpoint_dir = modelfile.parent / \"checkpoints\"\n",
    "        checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Use the get_epoch_number function for reliable sorting\n",
    "        def get_epoch_number(checkpoint_file):\n",
    "            try:\n",
    "                filename = checkpoint_file.stem\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    return int(parts[1])\n",
    "                return 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "        if checkpoint_files:\n",
    "            # Sort by epoch number\n",
    "            checkpoint_files.sort(key=get_epoch_number)\n",
    "            latest_epoch = get_epoch_number(checkpoint_files[-1])\n",
    "            print(f\"Found checkpoints up to epoch {latest_epoch}. Will resume training from last checkpoint.\")\n",
    "        else:\n",
    "            print(f\"Starting new training for {epochs} epochs\")\n",
    "\n",
    "        # Train the model with checkpoint support\n",
    "        model = train_and_save_concat_vit_model(\n",
    "            model, criterion, optimizer, modelfile,\n",
    "            device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Concatenated Image ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Number of views: 2\n",
      "  Latent dimension: 128\n",
      "  Image size: 224x224\n",
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading Concatenated Image ViT encoder weights from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit_concat_multiview/vit_large_concat_multiview/proprioception_mlp.pth\n",
      "\n",
      "Testing concatenated image ViT model on random examples:\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.9088733  0.37754595 0.21844389 0.15448141 0.5594522  0.97095925]\n",
      "  Predicted position: [0.82214594 0.40359777 0.29848638 0.29593843 0.51397604 0.8868769 ]\n",
      "  Mean squared error: 0.007293\n",
      "\n",
      "Example 2:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.792582   0.4439673  0.25177723 0.5159715  0.8014022  0.6658608 ]\n",
      "  Predicted position: [0.70996475 0.41524732 0.2819103  0.46171963 0.70544904 0.6127261 ]\n",
      "  Mean squared error: 0.003922\n",
      "\n",
      "Example 3:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.95       0.31428573 0.51666665 0.46666667 0.46666667 0.9       ]\n",
      "  Predicted position: [0.8391441  0.32946396 0.42783517 0.48750883 0.47478303 0.8836463 ]\n",
      "  Mean squared error: 0.003530\n",
      "\n",
      "Example 4:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.10159913 0.18941109 0.08016337 0.91637987 0.69606143 0.46944034]\n",
      "  Predicted position: [0.1941795  0.2877822  0.25027198 0.9104355  0.7523319  0.5482502 ]\n",
      "  Mean squared error: 0.009433\n",
      "\n",
      "Example 5:\n",
      "  View 1 shape: torch.Size([1, 3, 256, 256])\n",
      "  View 2 shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: torch.Size([1, 128])\n",
      "  Target position: [0.46494654 0.6862727  0.31145412 0.5182245  0.62526083 0.9698937 ]\n",
      "  Predicted position: [0.49193725 0.6297251  0.3126949  0.5366671  0.5439118  0.92609906]\n",
      "  Mean squared error: 0.002134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the sensor processing module using the trained model\n",
    "sp = ConcatImageVitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "# Test it on a few validation examples\n",
    "\n",
    "test_concat_model_direct(sp.enc, view_inputs_validation, targets_validation, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_encoding(model, view_inputs_validation, device, n_samples=3):\n",
    "    \"\"\"Test the encoding functionality of the concatenated image ViT model.\"\"\"\n",
    "    if n_samples > len(view_inputs_validation[0]):\n",
    "        n_samples = len(view_inputs_validation[0])\n",
    "\n",
    "    # Get first few samples\n",
    "    indices = range(n_samples)\n",
    "\n",
    "    print(\"\\nTesting model encoding functionality:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Get expected dimensions from model configuration\n",
    "    latent_size = model.latent_size\n",
    "    output_size = model.output_size\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in indices:\n",
    "            # Get views\n",
    "            views = [view[i].unsqueeze(0).to(device) for view in view_inputs_validation]\n",
    "\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "\n",
    "            # Test image concatenation\n",
    "            concat_image = model.concatenate_images(views)\n",
    "            print(f\"  Concatenated image shape: {concat_image.shape}\")\n",
    "            # Assert concatenated image has same batch and channel size as input\n",
    "            assert concat_image.shape[0] == views[0].shape[0], \"Batch size mismatch in concatenated image\"\n",
    "            assert concat_image.shape[1] == views[0].shape[1], \"Channel size mismatch in concatenated image\"\n",
    "\n",
    "            # Test latent encoding\n",
    "            latent = model.encode(views)\n",
    "            print(f\"  Latent representation shape: {latent.shape}\")\n",
    "            # Assert latent has correct shape\n",
    "            assert latent.shape[1] == latent_size, f\"Expected latent size {latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "            # Test full forward pass\n",
    "            output = model.forward(views)\n",
    "            print(f\"  Model output shape: {output.shape}\")\n",
    "            # Assert output has correct shape\n",
    "            assert output.shape[1] == output_size, f\"Expected output size {output_size}, got {output.shape[1]}\"\n",
    "\n",
    "            # Test that the proprioceptor gives the expected output when fed the latent\n",
    "            proprio_output = model.proprioceptor(latent)\n",
    "            print(f\"  Proprioceptor output shape: {proprio_output.shape}\")\n",
    "            # Assert proprioceptor output has correct shape\n",
    "            assert proprio_output.shape[1] == output_size, f\"Expected proprioceptor output size {output_size}, got {proprio_output.shape[1]}\"\n",
    "\n",
    "            # Verify that direct proprioceptor output matches the forward pass\n",
    "            is_close = torch.allclose(output, proprio_output, rtol=1e-5, atol=1e-5)\n",
    "            print(f\"  Forward pass matches proprioceptor: {is_close}\")\n",
    "            assert is_close, \"Forward pass output does not match proprioceptor output\"\n",
    "\n",
    "            print(\"  All shape assertions passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model encoding functionality:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "  Concatenated image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent representation shape: torch.Size([1, 128])\n",
      "  Model output shape: torch.Size([1, 6])\n",
      "  Proprioceptor output shape: torch.Size([1, 6])\n",
      "  Forward pass matches proprioceptor: True\n",
      "  All shape assertions passed!\n",
      "\n",
      "Sample 2:\n",
      "  Concatenated image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent representation shape: torch.Size([1, 128])\n",
      "  Model output shape: torch.Size([1, 6])\n",
      "  Proprioceptor output shape: torch.Size([1, 6])\n",
      "  Forward pass matches proprioceptor: True\n",
      "  All shape assertions passed!\n",
      "\n",
      "Sample 3:\n",
      "  Concatenated image shape: torch.Size([1, 3, 224, 224])\n",
      "  Latent representation shape: torch.Size([1, 128])\n",
      "  Model output shape: torch.Size([1, 6])\n",
      "  Proprioceptor output shape: torch.Size([1, 6])\n",
      "  Forward pass matches proprioceptor: True\n",
      "  All shape assertions passed!\n"
     ]
    }
   ],
   "source": [
    "test_model_encoding(sp.enc, view_inputs_validation, device, n_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the process_file method with cache handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing process_file method with view caching:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing timestep 1:\n",
      "  Testing with all views available:\n",
      "Using 1 unique views (with 1 duplicated)\n",
      "    Processed view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 1\n",
      "    Processed view 2, latent shape: (128,)\n",
      "  Testing with one missing view:\n",
      "Using 1 unique views (with 1 duplicated)\n",
      "    Processed only view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 1\n",
      "    Processed view 2 with view 1 from cache, latent shape: (128,)\n",
      "\n",
      "Processing timestep 2:\n",
      "  Testing with all views available:\n",
      "Using 2 unique views (with 0 duplicated)\n",
      "    Processed view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 2\n",
      "    Processed view 2, latent shape: (128,)\n",
      "  Testing with one missing view:\n",
      "Using 1 unique views (with 1 duplicated)\n",
      "    Processed only view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 2\n",
      "    Processed view 2 with view 1 from cache, latent shape: (128,)\n",
      "\n",
      "Processing timestep 3:\n",
      "  Testing with all views available:\n",
      "Using 2 unique views (with 0 duplicated)\n",
      "    Processed view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 3\n",
      "    Processed view 2, latent shape: (128,)\n",
      "  Testing with one missing view:\n",
      "Using 1 unique views (with 1 duplicated)\n",
      "    Processed only view 1, latent shape: (128,)\n",
      "Using complete set of 2 views from timestep 3\n",
      "    Processed view 2 with view 1 from cache, latent shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "def test_process_file(sp, view_inputs_validation, n_samples=3):\n",
    "    \"\"\"Test the process_file method which handles view caching.\"\"\"\n",
    "    if n_samples > len(view_inputs_validation[0]):\n",
    "        n_samples = len(view_inputs_validation[0])\n",
    "\n",
    "    # Get first few samples\n",
    "    indices = range(n_samples)\n",
    "\n",
    "    print(\"\\nTesting process_file method with view caching:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Create temporary files to simulate image files\n",
    "    import tempfile\n",
    "    import os\n",
    "    from torchvision.utils import save_image\n",
    "\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        # Create temporary image files for testing\n",
    "        for idx in indices:\n",
    "            print(f\"\\nProcessing timestep {idx+1}:\")\n",
    "\n",
    "            # Create a file for each view at this timestep\n",
    "            file_paths = []\n",
    "            for view_idx, view_tensor in enumerate(view_inputs_validation):\n",
    "                # Create file name with timestep and camera ID\n",
    "                file_name = f\"{idx+1:05d}_camera{view_idx+1}.jpg\"\n",
    "                file_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "                # Save tensor as image\n",
    "                save_image(view_tensor[idx], file_path)\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "            # Test processing with all views available\n",
    "            print(\"  Testing with all views available:\")\n",
    "            for view_idx, file_path in enumerate(file_paths):\n",
    "                latent = sp.process_file(file_path, camera_id=f\"camera{view_idx+1}\")\n",
    "                print(f\"    Processed view {view_idx+1}, latent shape: {latent.shape}\")\n",
    "\n",
    "            # Test processing with missing views (if we have more than one view)\n",
    "            if len(file_paths) > 1:\n",
    "                print(\"  Testing with one missing view:\")\n",
    "                # Clear cache to simulate new scenario\n",
    "                if hasattr(sp, '_view_cache'):\n",
    "                    sp._view_cache = {}\n",
    "                    sp._timestep_cache = {}\n",
    "\n",
    "                # Process only one view and see if cache is updated\n",
    "                latent = sp.process_file(file_paths[0], camera_id=f\"camera1\")\n",
    "                print(f\"    Processed only view 1, latent shape: {latent.shape}\")\n",
    "\n",
    "                # Now process a different view and see if previous view is used from cache\n",
    "                latent = sp.process_file(file_paths[1], camera_id=f\"camera2\")\n",
    "                print(f\"    Processed view 2 with view 1 from cache, latent shape: {latent.shape}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary directory\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "test_process_file(sp, view_inputs_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Vision Transformer type: vit_l_16\n",
      "Number of views: 2\n",
      "Latent space dimension: 128\n",
      "Output dimension (robot DOF): 6\n",
      "Use the ConcatImageVitSensorProcessing class to load and use this model for inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Number of views: {exp.get('num_views', 2)}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the ConcatImageVitSensorProcessing class to load and use this model for inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
