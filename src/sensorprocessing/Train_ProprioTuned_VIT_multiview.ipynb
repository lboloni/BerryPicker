{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Multi-View Vision Transformer (ViT)\n",
    "We create a sensor processing model using multiple Vision Transformer (ViT) based visual encoders\n",
    "finetuned with proprioception.\n",
    "We start with pretrained ViT models, then train them to:\n",
    "1. Create a meaningful 128-dimensional latent representation from multiple camera views\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    " The sensor processing object associated with the trained model is in sensorprocessing/sp_vit_multiview.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The sensor processing object associated with the trained model is in\n",
    "# sensorprocessing/sp_vit_multiview.py\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from demonstration.demonstration import Demonstration\n",
    "from sensorprocessing.sp_vit_multiview import MultiViewVitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "import sensorprocessing.sp_helper as sp_helper\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PAPERMILL PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# If it is set to discard-old, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit_multiview\"\n",
    "run = \"vit_base_multiview\"\n",
    "#concat_proj\n",
    "# run = \"vit_base_multiview\"  # ViT Base\n",
    "# run = \"vit_large_multiview\" # ViT Large\n",
    "# run = \"vit_huge_multiview\" # ViT Huge\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge\n",
    "\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "# If not None, set an output path\n",
    "data_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Handle external paths (when called from Flow)\n",
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path).expanduser()\n",
    "    external_path.mkdir(parents=True, exist_ok=True)\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit_multiview\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path).expanduser()\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "# Load the experiment/run configuration\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def load_multiview_images_as_proprioception_training(exp, exp_robot):\n",
    "    \"\"\"Loads the training images specified in the exp/run for multiple views.\n",
    "\n",
    "    Processes them as tensors for multiview proprioception training.\n",
    "    Caches the processed results into the input and target file.\n",
    "    Remove those files to recalculate.\n",
    "\n",
    "    Args:\n",
    "        exp: Experiment configuration\n",
    "        exp_robot: Robot experiment for normalization\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with view_inputs (list of tensors) and targets\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    proprioception_input_path = pathlib.Path(exp.data_dir(), \"proprio_input_multiview.pth\")\n",
    "    proprioception_target_path = pathlib.Path(exp.data_dir(), \"proprio_target_multiview.pth\")\n",
    "\n",
    "    if proprioception_input_path.exists():\n",
    "        print(f\"Loading cached multiview data from {proprioception_input_path}\")\n",
    "        retval[\"view_inputs\"] = torch.load(proprioception_input_path, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_path, weights_only=True)\n",
    "    else:\n",
    "        view_lists = {}  # Dictionary to organize views by camera\n",
    "        targetlist = []\n",
    "        num_views = exp.get(\"num_views\", 2)\n",
    "        transform = sp_helper.get_transform_to_sp(exp)\n",
    "\n",
    "        print(f\"Loading multiview training data from demonstrations with {num_views} views...\")\n",
    "\n",
    "        for val in exp[\"training_data\"]:\n",
    "            run_name, demo_name, cameras = val  # cameras can be list or comma-separated string\n",
    "\n",
    "            # Handle cameras as either list or comma-separated string\n",
    "            if isinstance(cameras, str):\n",
    "                cameras = [c.strip() for c in cameras.split(\",\")]\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run_name)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "\n",
    "            # Initialize view lists for cameras\n",
    "            if not view_lists:\n",
    "                for camera in cameras[:num_views]:\n",
    "                    view_lists[camera] = []\n",
    "\n",
    "            for i in range(demo.metadata[\"maxsteps\"]):\n",
    "                # Get images from all cameras\n",
    "                frame_images = []\n",
    "                skip_frame = False\n",
    "\n",
    "                for camera in cameras[:num_views]:\n",
    "                    try:\n",
    "                        sensor_readings, _ = demo.get_image(\n",
    "                            i, device=device, transform=transform, camera=camera\n",
    "                        )\n",
    "                        frame_images.append(sensor_readings[0])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping frame {i} - missing camera {camera}: {e}\")\n",
    "                        skip_frame = True\n",
    "                        break\n",
    "\n",
    "                if skip_frame:\n",
    "                    continue\n",
    "\n",
    "                # Store images by camera\n",
    "                for camera, img in zip(cameras[:num_views], frame_images):\n",
    "                    view_lists[camera].append(img)\n",
    "\n",
    "                # Get robot position\n",
    "                rp = demo.get_action(i, \"rc-position-target\", exp_robot)\n",
    "                anorm = rp.to_normalized_vector(exp_robot)\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        # Ensure we have the same number of frames for each view\n",
    "        min_frames = min(len(view_list) for view_list in view_lists.values())\n",
    "        if min_frames < len(targetlist):\n",
    "            print(f\"Truncating dataset to {min_frames} frames (from {len(targetlist)})\")\n",
    "            targetlist = targetlist[:min_frames]\n",
    "            for camera in view_lists:\n",
    "                view_lists[camera] = view_lists[camera][:min_frames]\n",
    "\n",
    "        # Stack tensors for each view\n",
    "        view_tensors = []\n",
    "        for camera in sorted(view_lists.keys())[:num_views]:\n",
    "            view_tensors.append(torch.stack(view_lists[camera]))\n",
    "\n",
    "        retval[\"view_inputs\"] = view_tensors\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "\n",
    "        # Save processed data\n",
    "        torch.save(retval[\"view_inputs\"], proprioception_input_path)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_path)\n",
    "        print(f\"Saved {len(targetlist)} training examples with {num_views} views each\")\n",
    "\n",
    "    # Separate the training and validation data\n",
    "    length = len(retval[\"targets\"])\n",
    "    rows = torch.randperm(length)\n",
    "\n",
    "    # Shuffle targets\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    # Shuffle each view input using the same row indices\n",
    "    shuffled_view_inputs = []\n",
    "    for view_tensor in retval[\"view_inputs\"]:\n",
    "        shuffled_view_inputs.append(view_tensor[rows])\n",
    "\n",
    "    # Split into training (67%) and validation (33%) sets\n",
    "    training_size = int(length * 0.67)\n",
    "\n",
    "    # Training data\n",
    "    retval[\"view_inputs_training\"] = [view[:training_size] for view in shuffled_view_inputs]\n",
    "    retval[\"targets_training\"] = shuffled_targets[:training_size]\n",
    "\n",
    "    # Validation data\n",
    "    retval[\"view_inputs_validation\"] = [view[training_size:] for view in shuffled_view_inputs]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    print(f\"Created {training_size} training examples and {length - training_size} validation examples\")\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CUSTOM DATASET FOR MULTI-VIEW DATA\n",
    "# =============================================================================\n",
    "\n",
    "class MultiViewDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for multi-view image data.\"\"\"\n",
    "\n",
    "    def __init__(self, view_inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            view_inputs: List of tensors, one per view. Each tensor: [N, C, H, W]\n",
    "            targets: Tensor of targets: [N, output_dim]\n",
    "        \"\"\"\n",
    "        self.view_inputs = view_inputs\n",
    "        self.targets = targets\n",
    "        self.num_samples = len(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple: (list of view images, target)\"\"\"\n",
    "        views = [view[idx] for view in self.view_inputs]\n",
    "        target = self.targets[idx]\n",
    "        return views, target\n",
    "\n",
    "\n",
    "def multiview_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for multi-view data.\"\"\"\n",
    "    views_list = [item[0] for item in batch]\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "\n",
    "    # Transpose from [batch, views] to [views, batch]\n",
    "    num_views = len(views_list[0])\n",
    "    batched_views = []\n",
    "    for v in range(num_views):\n",
    "        view_batch = torch.stack([views_list[i][v] for i in range(len(views_list))])\n",
    "        batched_views.append(view_batch)\n",
    "\n",
    "    return batched_views, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR EXISTING MODEL AND LOAD DATA IF NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "\n",
    "if modelfile.exists():\n",
    "    print(\"*** Train-ProprioTuned-ViT-Multiview ***: NOT training; model already exists\")\n",
    "    # Load data anyway for testing\n",
    "    tr = load_multiview_images_as_proprioception_training(exp, exp_robot)\n",
    "else:\n",
    "    # Load data for training\n",
    "    tr = load_multiview_images_as_proprioception_training(exp, exp_robot)\n",
    "\n",
    "view_inputs_training = tr[\"view_inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "view_inputs_validation = tr[\"view_inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CREATE THE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Create the multi-view ViT model\n",
    "sp = MultiViewVitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {param_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# =============================================================================\n",
    "\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "\n",
    "train_dataset = MultiViewDataset(view_inputs_training, targets_training)\n",
    "test_dataset = MultiViewDataset(view_inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=multiview_collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=multiview_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_and_save_multiview_proprioception_model(\n",
    "    model, criterion, optimizer, modelfile,\n",
    "    device=\"cpu\", epochs=20, scheduler=None, log_interval=1\n",
    "):\n",
    "    \"\"\"Trains and saves the multiview ViT proprioception model.\n",
    "\n",
    "    Args:\n",
    "        model: Multi-view ViT model\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "\n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Checkpoint directory\n",
    "    checkpoint_dir = modelfile.parent / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Check for existing checkpoints\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_loss', float('inf'))\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_views, batch_y in train_loader:\n",
    "            # Move views and targets to device\n",
    "            batch_views = [view.to(device) for view in batch_views]\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_views)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        avg_train_loss = total_loss / max(batch_count, 1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_views, batch_y in test_loader:\n",
    "                batch_views = [view.to(device) for view in batch_views]\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                predictions = model(batch_views)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "\n",
    "        avg_val_loss = val_loss / max(val_batch_count, 1)\n",
    "\n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'best_loss': best_val_loss\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'best_loss': best_val_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"  New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Cleanup old checkpoints (keep last 3)\n",
    "        old_checkpoints = sorted(checkpoint_dir.glob(\"epoch_*.pth\"), key=get_epoch_number)\n",
    "        for old_ckpt in old_checkpoints[:-3]:\n",
    "            old_ckpt.unlink()\n",
    "\n",
    "    # Load best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} \"\n",
    "              f\"with loss {best_checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "    # Save to final model file\n",
    "    torch.save(model.state_dict(), modelfile)\n",
    "    print(f\"Final model saved to {modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TRAINING EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "epochs_to_train = exp.get(\"epochs\", 20)\n",
    "\n",
    "if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing final model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Evaluate loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_views, batch_y in test_loader:\n",
    "            batch_views = [view.to(device) for view in batch_views]\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_views)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"Training for {epochs_to_train} epochs\")\n",
    "    model = train_and_save_multiview_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs_to_train, scheduler=lr_scheduler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TESTING THE TRAINED MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Create sensor processing module using trained model\n",
    "sp = MultiViewVitSensorProcessing(exp, device)\n",
    "\n",
    "def test_multiview_sensor_processing(sp, test_view_inputs, test_targets, n_samples=5):\n",
    "    \"\"\"Test the multi-view sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_targets):\n",
    "        n_samples = len(test_targets)\n",
    "\n",
    "    indices = torch.randperm(len(test_targets))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting multi-view sensor processing on random examples:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        views = [view[idx].unsqueeze(0).to(device) for view in test_view_inputs]\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        latent = sp.process(views)\n",
    "\n",
    "        print(f\"Example {i+1}:\")\n",
    "        for j, view in enumerate(views):\n",
    "            print(f\"  View {j+1} shape: {view.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "test_multiview_sensor_processing(sp, view_inputs_validation, targets_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# VERIFY MODEL ENCODING AND FORWARD METHODS\n",
    "# =============================================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_views = [view[0].unsqueeze(0).to(device) for view in view_inputs_validation]\n",
    "\n",
    "    # Get latent representation\n",
    "    latent = model.encode(sample_views)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get robot position prediction\n",
    "    position = model.forward(sample_views)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Verify dimensions\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, \\\n",
    "        f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, \\\n",
    "        f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SAVE FINAL MODEL AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Number of views: {exp.get('num_views', 2)}\")\n",
    "print(f\"Fusion method: {exp.get('fusion_type', 'concat_proj')}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the MultiViewVitSensorProcessing class to load and use this model for inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
