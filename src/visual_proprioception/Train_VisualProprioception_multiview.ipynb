{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/ssheikholeslami/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/ssheikholeslami/SaharaBerryPickerData/settings-sahara.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training,load_concat_demonstrations_as_proprioception_training, get_visual_proprioception_sp, load_multiview_demonstrations_as_proprioception_training,load_concat_demonstrations_as_proprioception_training2\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/visual_proprioception/vit_base_128_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vit_base_128 successfully loaded\n",
      "{'batch_size': 8,\n",
      " 'data_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/experiment_data/visual_proprioception/vit_base_128'),\n",
      " 'encoding_size': 128,\n",
      " 'epochs': 1000,\n",
      " 'exp_run_sys_indep_file': PosixPath('/lustre/fs1/home/ssheikholeslami/BerryPicker/src/experiment_configs/visual_proprioception/vit_base_128.yaml'),\n",
      " 'freeze_backbone': False,\n",
      " 'freeze_feature_extractor': True,\n",
      " 'group_name': 'visual_proprioception',\n",
      " 'image_size': 224,\n",
      " 'latent_size': 128,\n",
      " 'learning_rate': 0.0001,\n",
      " 'loss': 'MSE',\n",
      " 'model_type': 'ViTProprioTunedRegression',\n",
      " 'name': 'vit-base-128',\n",
      " 'output_size': 6,\n",
      " 'projection_hidden_dim': 512,\n",
      " 'proprio_step_1': 64,\n",
      " 'proprio_step_2': 64,\n",
      " 'proprioception_input_file': 'train_inputs.pt',\n",
      " 'proprioception_mlp_model_file': 'proprioception_mlp.pth',\n",
      " 'proprioception_target_file': 'train_targets.pt',\n",
      " 'proprioception_test_input_file': 'test_inputs.pt',\n",
      " 'proprioception_test_target_file': 'test_targets.pt',\n",
      " 'proprioception_testing_task': 'proprio_regressor_validation',\n",
      " 'proprioception_training_task': 'proprio_regressor_training',\n",
      " 'regressor_hidden_size_1': 64,\n",
      " 'regressor_hidden_size_2': 64,\n",
      " 'resume': None,\n",
      " 'run_name': 'vit_base_128',\n",
      " 'sensor_processing': 'Vit',\n",
      " 'sp_experiment': 'sensorprocessing_propriotuned_Vit',\n",
      " 'sp_run': 'vit_base_128',\n",
      " 'use_6d_token': False,\n",
      " 'vit_model': 'vit_b_16',\n",
      " 'vit_output_dim': 768,\n",
      " 'vit_weights': 'DEFAULT'}\n",
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_propriotuned_Vit/vit_base_128_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_propriotuned_Vit/vit_base_128 successfully loaded\n",
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_b_16\n",
      "  Latent dimension: 128\n",
      "  Image size: 224x224\n",
      "Using vit_b_16 with output dimension 768\n",
      "Created projection network: 768 → 512 → 256 → 128\n",
      "Created latent representation: 768 → 512 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading ViT encoder weights from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_base_128/proprioception_mlp.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fs1/home/ssheikholeslami/BerryPicker/src/visual_proprioception/../sensorprocessing/sp_vit.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.enc.load_state_dict(torch.load(modelfile, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "\n",
    "# run = \"vp_aruco_128\"  #DONE\n",
    "# run = \"vp_convvae_128\" #DONE\n",
    "# run = \"vp_ptun_vgg19_128\" #DONE\n",
    "# run = \"vp_ptun_resnet50_128\" #DONE\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\" #DONE\n",
    "# run = \"vp_ptun_vgg19_256\" #DONE\n",
    "# run = \"vp_ptun_resnet50_256\" #DONE\n",
    "\n",
    "#vits\n",
    "run =\"vit_base_128\" #DONE\n",
    "# run =\"vit_base_256\" #DONE\n",
    "\n",
    "# run =\"vit_large_128\" #DONE\n",
    "# run =\"vit_large_256\" #DONE\n",
    "\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews  - NEW!         #\n",
    "##############################################\n",
    "\n",
    "############  latent space: 128  ############\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview_128\"  #DONE\n",
    "# run =\"vit_large_multiview_128\"  #DONE\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj_128\"  # ViT Base_indiv_proj_128  #DONE\n",
    "# run = \"vit_large_multiview_indiv_proj_128\" # ViT Large_indiv_proj_128  #DONE\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention_128\"  # ViT Base_attention  #DONE\n",
    "# run = \"vit_large_multiview_attention_128\" # ViT Large_attention  #DONE\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum_128\"  # ViT Base_weighted_sum  #DONE\n",
    "# run = \"vit_large_multiview_weighted_sum_128\" # ViT Large_weighted_sum  #DONE\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated_128\"  # ViT Base_gated  #DONE\n",
    "# run = \"vit_large_multiview_gated_128\" # ViT Large_gated  #DONE\n",
    "\n",
    "########## the latent space 256 ones #########\n",
    "\n",
    "\n",
    "# run =\"vit_base_multiview_256\"  #DONE\n",
    "# run =\"vit_large_multiview_256\"  #DONE\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj_256\"  # ViT Base_indiv_proj_256   #DONE\n",
    "# run = \"vit_large_multiview_indiv_proj_256\" # ViT Large_indiv_proj_256 #DONE\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention_256\"  # DONE\n",
    "# run = \"vit_large_multiview_attention_256\" # ViT Large_attention #DONE\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum_256\"  # ViT Base_weighted_sum   #DONE\n",
    "# run = \"vit_large_multiview_weighted_sum_256\" # ViT Large_weighted_sum  #DONE\n",
    "\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated_256\"  # ViT Base_gated  #DONE\n",
    "# run = \"vit_large_multiview_gated_256\" # ViT Large_gated  #DONE\n",
    "\n",
    "\n",
    "##############################################\n",
    "#          MultiViews Image Concat - NEW!    #\n",
    "##############################################\n",
    "# the latent space 128 ones\n",
    "# run = \"vit_base_concat_multiview_128\" # ViT Base  #DONE\n",
    "# run = \"vit_large_concat_multiview_128\"  # ViT Large  #DONE\n",
    "# run = \"vp_convvae_128_concat_multiview\"  #DONE\n",
    "\n",
    "# the latent space 256 ones\n",
    "\n",
    "# run = \"vit_base_concat_multiview_256\" # ViT Base  #DONE\n",
    "# run = \"vit_large_concat_multiview_256\"  # ViT Large  #DONE\n",
    "# run = \"vp_convvae_256_concat_multiview\" #DONE\n",
    "\n",
    "\n",
    "##############################################\n",
    "#          MultiViews CNN - NEW!             #\n",
    "##############################################\n",
    "\n",
    "# run = \"vp_ptun_vgg19_128_multiview\" #DONE\n",
    "# run = \"vp_ptun_resnet50_128_multiview\" #DONE\n",
    "# run = \"vp_ptun_vgg19_256_multiview\" #DONE\n",
    "# run = \"vp_ptun_resnet50_256_multiview\" #DONE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint(exp)\n",
    "\n",
    "sp = get_visual_proprioception_sp(exp, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_conv_vae_concat_fix.py\n",
    "\n",
    "Updated test script that handles both list and tensor formats for cached data.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from settings import Config\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "def test_conv_vae_concat_fix(experiment=\"sensorprocessing_conv_vae_concat_multiview\",\n",
    "                             run=\"proprio_128_concat_multiview\"):\n",
    "    \"\"\"\n",
    "    Test the fixed Conv-VAE concatenated multiview model.\n",
    "\n",
    "    Args:\n",
    "        experiment: Experiment name\n",
    "        run: Run name\n",
    "\n",
    "    Returns:\n",
    "        True if all tests pass, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Testing fix for {experiment}/{run}\")\n",
    "\n",
    "    # Load experiment configuration\n",
    "    exp = Config().get_experiment(experiment, run)\n",
    "    exp[\"debug\"] = True  # Enable debug output\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Import the fixed sensor processing class\n",
    "    from sensorprocessing.sp_conv_vae_concat_multiview import ConcatConvVaeSensorProcessing\n",
    "\n",
    "    print(\"\\n=== Test 1: Creating ConcatConvVaeSensorProcessing ===\")\n",
    "    try:\n",
    "        sp = ConcatConvVaeSensorProcessing(exp, device)\n",
    "        print(\"✓ Successfully created ConcatConvVaeSensorProcessing\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create ConcatConvVaeSensorProcessing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "    print(\"\\n=== Test 2: Processing dummy views ===\")\n",
    "    try:\n",
    "        # Create dummy views\n",
    "        num_views = exp.get(\"num_views\", 2)\n",
    "        dummy_views = [torch.randn(1, 3, 64, 64).to(device) for _ in range(num_views)]\n",
    "        print(f\"Created {num_views} dummy views with shape {dummy_views[0].shape}\")\n",
    "\n",
    "        # Process through encoder\n",
    "        latent = sp.encode(dummy_views)\n",
    "        print(f\"Encoded latent shape: {latent.shape}\")\n",
    "\n",
    "        # Verify latent size\n",
    "        expected_size = exp.get(\"latent_size\", 128)\n",
    "        if latent.size != expected_size:\n",
    "            print(f\"✗ Latent size mismatch: expected {expected_size}, got {latent.size}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"✓ Successfully encoded dummy views to latent size {latent.size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to process dummy views: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "    print(\"\\n=== Test 3: Testing with MLP model ===\")\n",
    "    try:\n",
    "        # Create the MLP model\n",
    "        mlp_model = VisProprio_SimpleMLPRegression(exp)\n",
    "        mlp_model.to(device)\n",
    "        print(f\"Created MLP with input_size={mlp_model.input_size}, output_size={mlp_model.output_size}\")\n",
    "\n",
    "        # Convert latent to tensor and reshape if needed\n",
    "        latent_tensor = torch.tensor(latent, dtype=torch.float32).to(device)\n",
    "        if len(latent_tensor.shape) == 1:\n",
    "            latent_tensor = latent_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        print(f\"Latent tensor shape for MLP: {latent_tensor.shape}\")\n",
    "\n",
    "        # Forward pass through MLP\n",
    "        output = mlp_model(latent_tensor)\n",
    "        print(f\"MLP output shape: {output.shape}\")\n",
    "\n",
    "        # Verify output size\n",
    "        expected_output_size = exp.get(\"output_size\", 6)\n",
    "        if output.shape[1] != expected_output_size:\n",
    "            print(f\"✗ Output size mismatch: expected {expected_output_size}, got {output.shape[1]}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"✓ Successfully passed latent through MLP to get output of size {output.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to test with MLP model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "    print(\"\\n=== Test 4: Testing data loading ===\")\n",
    "    try:\n",
    "        # Data paths\n",
    "        proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "        proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "        # Skip actual data loading to avoid long processing time during testing\n",
    "        if proprioception_input_file.exists() and proprioception_target_file.exists():\n",
    "            print(\"Cached data files exist, testing data loading...\")\n",
    "            try:\n",
    "                # Load the data - handle both list and tensor formats\n",
    "                raw_inputs = torch.load(proprioception_input_file, weights_only=True)\n",
    "                raw_targets = torch.load(proprioception_target_file, weights_only=True)\n",
    "\n",
    "                # Determine format and extract test samples\n",
    "                test_data = {}\n",
    "\n",
    "                # Handle list format (raw views)\n",
    "                if isinstance(raw_inputs, list):\n",
    "                    print(\"Data is in list format (raw views)\")\n",
    "                    # Take just a few samples for testing\n",
    "                    view_samples = [view[:5] for view in raw_inputs]\n",
    "\n",
    "                    # Process through encoder\n",
    "                    test_inputs = []\n",
    "                    for i in range(len(view_samples[0])):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in view_samples]\n",
    "                        latent = sp.encode(sample_views)\n",
    "                        test_inputs.append(torch.tensor(latent, dtype=torch.float32))\n",
    "\n",
    "                    test_data[\"inputs\"] = torch.stack(test_inputs).to(device)\n",
    "                    test_data[\"targets\"] = raw_targets[:5].to(device)\n",
    "\n",
    "                    print(f\"  Processed {len(test_inputs)} samples\")\n",
    "                    print(f\"  Each view shape: {[view.shape for view in view_samples]}\")\n",
    "                    print(f\"  Processed inputs shape: {test_data['inputs'].shape}\")\n",
    "                else:\n",
    "                    # Handle tensor format (already processed latents)\n",
    "                    print(\"Data is in tensor format (processed latents)\")\n",
    "                    test_data[\"inputs\"] = raw_inputs[:5].to(device)\n",
    "                    test_data[\"targets\"] = raw_targets[:5].to(device)\n",
    "\n",
    "                    print(f\"  Inputs shape: {test_data['inputs'].shape}\")\n",
    "\n",
    "                print(f\"  Targets shape: {test_data['targets'].shape}\")\n",
    "\n",
    "                # Test with MLP\n",
    "                output = mlp_model(test_data[\"inputs\"])\n",
    "                print(f\"  MLP output with loaded data: {output.shape}\")\n",
    "\n",
    "                print(\"✓ Successfully tested data loading and MLP forward pass\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load and test cached data: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return False\n",
    "        else:\n",
    "            print(\"No cached data found, skipping data loading test\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to import or test data loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "    print(\"\\n=== All tests passed! ===\")\n",
    "    print(\"The Conv-VAE concatenated multiview model fix is working correctly.\")\n",
    "    return True\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     success = test_conv_vae_concat_fix()\n",
    "#     sys.exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fix for sensorprocessing_conv_vae_concat_multiview/proprio_128_concat_multiview\n",
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_conv_vae_concat_multiview/proprio_128_concat_multiview_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_conv_vae_concat_multiview/proprio_128_concat_multiview successfully loaded\n",
      "Using device: cuda\n",
      "\n",
      "=== Test 1: Creating ConcatConvVaeSensorProcessing ===\n",
      "Initializing ConcatConvVaeSensorProcessing:\n",
      "  num_views: 2\n",
      "  stack_mode: width\n",
      "  latent_size: 128\n",
      "Warning: logging configuration file is not found in logger/logger_config.json.\n",
      "✓ Successfully created ConcatConvVaeSensorProcessing\n",
      "\n",
      "=== Test 2: Processing dummy views ===\n",
      "Created 2 dummy views with shape torch.Size([1, 3, 64, 64])\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "Encoded latent shape: (128,)\n",
      "✓ Successfully encoded dummy views to latent size 128\n",
      "\n",
      "=== Test 3: Testing with MLP model ===\n",
      "Created MLP with input_size=128, output_size=6\n",
      "Latent tensor shape for MLP: torch.Size([1, 128])\n",
      "MLP output shape: torch.Size([1, 6])\n",
      "✓ Successfully passed latent through MLP to get output of size 6\n",
      "\n",
      "=== Test 4: Testing data loading ===\n",
      "Cached data files exist, testing data loading...\n",
      "Data is in list format (raw views)\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "Concatenated shape: torch.Size([1, 3, 64, 128]), resizing to 64x64\n",
      "Final latent shape: (128,), size: 128\n",
      "  Processed 5 samples\n",
      "  Each view shape: [torch.Size([5, 3, 256, 256]), torch.Size([5, 3, 256, 256])]\n",
      "  Processed inputs shape: torch.Size([5, 128])\n",
      "  Targets shape: torch.Size([5, 6])\n",
      "  MLP output with loaded data: torch.Size([5, 6])\n",
      "✓ Successfully tested data loading and MLP forward pass\n",
      "\n",
      "=== All tests passed! ===\n",
      "The Conv-VAE concatenated multiview model fix is working correctly.\n"
     ]
    }
   ],
   "source": [
    "success = test_conv_vae_concat_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and cache the training data. \n",
    "* Iterate through the images and process them into latent encodings. \n",
    "* Iterate through the json files describing the robot position\n",
    "* Save the input and target values into files in the experiment directory. These will act as caches for later runs\n",
    "* Create the training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Using single-view approach\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "\n",
    "# Check if we're using a multi-view approach\n",
    "is_multiview = (\n",
    "    exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or\n",
    "    exp.get(\"sensor_processing\", \"\").startswith(\"Vit_concat\") or  # Add this line\n",
    "    \"concat\" in exp.get(\"sensor_processing\", \"\").lower() or       # Add this line\n",
    "    exp.get(\"num_views\", 1) > 1\n",
    ")\n",
    "# is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "# For specific CNN multi-view detection\n",
    "is_cnn_multiview = (\n",
    "    exp.get(\"sensor_processing\", \"\") == \"VGG19ProprioTunedSensorProcessing_multiview\" or\n",
    "    exp.get(\"sensor_processing\", \"\") == \"ResNetProprioTunedRegression_multiview\"\n",
    ")\n",
    "\n",
    "is_conv_vae_concat =  exp.get(\"sensor_processing\", \"\") == \"ConvVaeSensorProcessing_concat_multiview\"\n",
    "print(is_conv_vae_concat)\n",
    "print(is_multiview)\n",
    "if is_multiview:\n",
    "    print(f\"Using multi-view approach with {exp.get('num_views', 2)} views\")\n",
    "\n",
    "    # # Create appropriate dataset and dataloader based on the data format\n",
    "    # if is_conv_vae_concat:\n",
    "    #     # For the concat model approach with pre-processed latents\n",
    "    #     print(\"Using pre-processed latent approach for ConvVAE concat model\")\n",
    "\n",
    "    #     # Load data with your custom function\n",
    "    #     tr = load_concat_demonstrations_as_proprioception_training2(\n",
    "    #         task,\n",
    "    #         proprioception_input_file,\n",
    "    #         proprioception_target_file,\n",
    "    #         num_views=exp.get(\"num_views\", 2),\n",
    "    #         sp=sp)\n",
    "\n",
    "    #     # Create a dataset that wraps each input tensor in a list to match the multi-view format\n",
    "    #     class ViewListDataset(torch.utils.data.Dataset):\n",
    "    #         def __init__(self, inputs, targets, num_views=2):\n",
    "    #             self.inputs = inputs\n",
    "    #             self.targets = targets\n",
    "    #             self.num_views = num_views\n",
    "\n",
    "    #         def __len__(self):\n",
    "    #             return len(self.targets)\n",
    "\n",
    "    #         def __getitem__(self, idx):\n",
    "    #             # Create a list with num_views copies of the same tensor\n",
    "    #             views_list = [self.inputs[idx]] * self.num_views\n",
    "    #             return views_list, self.targets[idx]\n",
    "\n",
    "    #     is_preprocessed = tr.get(\"is_preprocessed\", False)\n",
    "\n",
    "    #     # Use ViewListDataset to ensure we get a list of tensors\n",
    "    #     train_dataset = ViewListDataset(tr[\"inputs_training\"], tr[\"targets_training\"], num_views=exp.get(\"num_views\", 2))\n",
    "    #     test_dataset = ViewListDataset(tr[\"inputs_validation\"], tr[\"targets_validation\"], num_views=exp.get(\"num_views\", 2))\n",
    "\n",
    "\n",
    "\n",
    "    # Use standard TensorDataset for ConvVAE since views are already concatenated\n",
    "        # train_dataset = TensorDataset(tr[\"inputs_training\"], tr[\"targets_training\"])\n",
    "        # test_dataset = TensorDataset(tr[\"inputs_validation\"], tr[\"targets_validation\"])\n",
    "\n",
    "\n",
    "\n",
    "    if is_cnn_multiview:\n",
    "        print(f\"Detected CNN-based multi-view model: {exp.get('sensor_processing')}\")\n",
    "    # Use the multiview loading function\n",
    "        tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "        task,\n",
    "        proprioception_input_file,\n",
    "        proprioception_target_file,\n",
    "        num_views=exp.get(\"num_views\", 2))\n",
    "    else:\n",
    "        tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "        task,\n",
    "        proprioception_input_file,\n",
    "        proprioception_target_file,\n",
    "        num_views=exp.get(\"num_views\", 2)\n",
    "\n",
    "    )\n",
    "\n",
    "    # Create a custom dataset for multi-view data\n",
    "    class MultiViewDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, view_inputs, targets):\n",
    "            self.view_inputs = view_inputs  # List of tensors, one per view\n",
    "            self.targets = targets\n",
    "            self.num_samples = len(targets)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Get corresponding sample from each view\n",
    "            views = [view[idx] for view in self.view_inputs]\n",
    "            target = self.targets[idx]\n",
    "            return views, target\n",
    "\n",
    "# Add the new class for processed latents\n",
    "    class ProcessedLatentDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, targets):\n",
    "            self.inputs = inputs  # These are already processed latents\n",
    "            self.targets = targets\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.targets)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Return inputs directly - these are already processed\n",
    "            # No need to pass through sp.encode again\n",
    "            return self.inputs[idx], self.targets[idx]\n",
    "    # Create DataLoaders for batching\n",
    "    batch_size = exp.get('batch_size', 32)\n",
    "    train_dataset = MultiViewDataset(tr[\"view_inputs_training\"], tr[\"targets_training\"])\n",
    "    test_dataset = MultiViewDataset(tr[\"view_inputs_validation\"], tr[\"targets_validation\"])\n",
    "\n",
    "else:\n",
    "    print(\"Using single-view approach\")\n",
    "\n",
    "    # Use the original loading function\n",
    "    tr = load_demonstrations_as_proprioception_training(\n",
    "        sp, task, proprioception_input_file, proprioception_target_file\n",
    "    )\n",
    "\n",
    "    inputs_training = tr[\"inputs_training\"]\n",
    "    targets_training = tr[\"targets_training\"]\n",
    "    inputs_validation = tr[\"inputs_validation\"]\n",
    "    targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "    # Create standard DataLoaders for single-view data\n",
    "    batch_size = exp.get('batch_size', 32)\n",
    "    train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "    test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to detect if a model has batch normalization layers\n",
    "def has_batch_norm(model):\n",
    "    \"\"\"Check if the model contains any BatchNorm layers\"\"\"\n",
    "    import torch.nn as nn\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp,is_preprocessed=False):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = (\n",
    "        exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or\n",
    "        exp.get(\"sensor_processing\", \"\").startswith(\"Vit_concat\") or\n",
    "        \"concat\" in exp.get(\"sensor_processing\", \"\").lower() or\n",
    "        exp.get(\"num_views\", 1) > 1\n",
    "    )\n",
    "\n",
    "    # Detect concatenated model approach specifically\n",
    "    is_concat_model =is_concat_model = \"concat\" in exp.get(\"sensor_processing\", \"\").lower()\n",
    "\n",
    "\n",
    "    is_cnn_multiview = (\n",
    "        exp.get(\"sensor_processing\", \"\") == \"VGG19ProprioTunedSensorProcessing_multiview\" or\n",
    "        exp.get(\"sensor_processing\", \"\") == \"ResNetProprioTunedRegression_multiview\"\n",
    "    )\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # We'll always use the standard MLP model regardless of approach\n",
    "    # This is model = VisProprio_SimpleMLPRegression(exp) defined outside this function\n",
    "    train_model = model\n",
    "\n",
    "    # Helper function to check if model has batch normalization\n",
    "    def has_batch_norm(model):\n",
    "        \"\"\"Check if the model contains any BatchNorm layers\"\"\"\n",
    "        import torch.nn as nn\n",
    "\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        train_model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        train_model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                try:\n",
    "                    if is_multiview:\n",
    "                        # if is_conv_vae_concat and is_preprocessed:\n",
    "                        #     print(\"we are here in concattttttttt convvvvv\")\n",
    "                        #     # ConvVAE concat model expects single tensor with concatenated views\n",
    "                        #     batch_X, batch_y = batch_data\n",
    "                        #     batch_X = batch_X.to(device)\n",
    "                        #     predictions = train_model(batch_X)\n",
    "                        #     continue\n",
    "                        if is_concat_model:\n",
    "                            print (\"not in is_concat_modelllllllllllllllllllll\")\n",
    "                            # For concat model, use sp.enc.encode to get latent features\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping evaluation batch with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            # Process each sample to get latent features\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                # Get views for this sample\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                                # Get latent using concat model's encoder (without proprioceptor)\n",
    "                                sample_features = sp.enc.encode(sample_views).cpu().numpy()\n",
    "\n",
    "                                # Convert to tensor and move to device\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            # Stack features\n",
    "                            batch_X = torch.stack(batch_features).squeeze(1).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "                        elif is_cnn_multiview:\n",
    "                            # Handle CNN-based multi-view processing similar to standard multi-view\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping evaluation batch with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                                # For CNN multi-view, use encode_views method of the model directly\n",
    "                                sample_features = sp.process(sample_views)\n",
    "                                # Convert numpy array to tensor and move to device\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            batch_X = torch.stack(batch_features).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "                        else:\n",
    "                            # Standard multi-view processing\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping evaluation batch with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                                sample_features = sp.process(sample_views)\n",
    "                                # Convert numpy array to tensor and move to device\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            batch_X = torch.stack(batch_features).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "                    else:\n",
    "                        batch_X, batch_y = batch_data\n",
    "\n",
    "                        # Check batch size for BatchNorm compatibility\n",
    "                        if batch_X.size(0) == 1 and has_batch_norm(train_model):\n",
    "                            print(f\"Warning: Skipping evaluation batch with size 1 (incompatible with BatchNorm)\")\n",
    "                            continue\n",
    "\n",
    "                        batch_X = batch_X.to(device)\n",
    "                        predictions = train_model(batch_X)\n",
    "\n",
    "                    # Make sure batch_y is on the same device\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    loss = criterion(predictions, batch_y)\n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return train_model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    train_model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        train_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        train_model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                # if isinstance(batch_data, tuple) and len(batch_data) == 2 and not isinstance(batch_data[0], list):\n",
    "                #     # This is a standard (tensor, target) tuple, no list of views\n",
    "                #     batch_X, batch_y = batch_data\n",
    "                #     batch_X = batch_X.to(device)\n",
    "                #     predictions = train_model(batch_X)\n",
    "                if is_multiview:\n",
    "                    # if is_conv_vae_concat and is_preprocessed:\n",
    "                    #         print(\"we are here in concattttttttt convvvvv\")\n",
    "                    #         # ConvVAE concat model expects single tensor with concatenated views\n",
    "                    #         batch_X, batch_y = batch_data\n",
    "                    #         batch_X = batch_X.to(device)\n",
    "                    #         predictions = train_model(batch_X)\n",
    "                    #         continue\n",
    "                    if is_concat_model:\n",
    "                        # For concat model, get latent from sp.enc.encode\n",
    "                        batch_views, batch_y = batch_data\n",
    "\n",
    "                        # Check batch size for BatchNorm compatibility\n",
    "                        if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                            print(f\"Warning: Skipping batch {batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                            continue\n",
    "\n",
    "                        # Process each sample to get latent features\n",
    "                        batch_size = batch_views[0].size(0)\n",
    "                        batch_features = []\n",
    "\n",
    "                        for i in range(batch_size):\n",
    "                            # Get views for this sample\n",
    "                            sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                            # Get latent using concat model's encoder (without proprioceptor)\n",
    "                            with torch.no_grad():  # Don't compute gradients through sp.enc\n",
    "                                # sample_features = sp.enc.encode(sample_views).cpu().numpy()\n",
    "                                sample_features = sp.enc.encode(sample_views)\n",
    "                            # Convert to tensor and move to device\n",
    "                            sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                            batch_features.append(sample_features_tensor)\n",
    "\n",
    "                        # Stack features\n",
    "                        batch_X = torch.stack(batch_features).squeeze(1).to(device)\n",
    "                        predictions = train_model(batch_X)\n",
    "\n",
    "\n",
    "                    elif is_cnn_multiview:\n",
    "                        # Handle CNN-based multi-view processing similar to standard multi-view\n",
    "                        batch_views, batch_y = batch_data\n",
    "\n",
    "                        # Check if we have a problematic batch size=1 situation\n",
    "                        if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                            print(f\"Warning: Skipping batch {batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                            continue\n",
    "\n",
    "                        # With CNN multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                        batch_size = batch_views[0].size(0)\n",
    "                        batch_features = []\n",
    "\n",
    "                        # Process each sample in the batch\n",
    "                        for i in range(batch_size):\n",
    "                            # Extract this sample's views\n",
    "                            sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                            # Process this sample through sp\n",
    "                            with torch.no_grad():  # Don't compute gradients through sp\n",
    "                                sample_features = sp.process(sample_views)\n",
    "\n",
    "                            # Convert numpy array to tensor and move to device\n",
    "                            sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                            batch_features.append(sample_features_tensor)\n",
    "\n",
    "                        # Stack all samples' features into a batch\n",
    "                        batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        predictions = train_model(batch_X)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        # Standard multi-view processing\n",
    "                        batch_views, batch_y = batch_data\n",
    "\n",
    "                        # Check if we have a problematic batch size=1 situation\n",
    "                        if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                            print(f\"Warning: Skipping batch {batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                            continue\n",
    "\n",
    "                        # With standard multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                        batch_size = batch_views[0].size(0)\n",
    "                        batch_features = []\n",
    "\n",
    "                        # Process each sample in the batch\n",
    "                        for i in range(batch_size):\n",
    "                            # Extract this sample's views\n",
    "                            sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                            # Process this sample through sp\n",
    "                            sample_features = sp.process(sample_views)\n",
    "\n",
    "                            # Convert numpy array to tensor and move to device\n",
    "                            sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                            batch_features.append(sample_features_tensor)\n",
    "\n",
    "                        # Stack all samples' features into a batch\n",
    "                        batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        predictions = train_model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "\n",
    "                    # Check if we have a problematic batch size=1 situation\n",
    "                    if batch_X.size(0) == 1 and has_batch_norm(train_model):\n",
    "                        print(f\"Warning: Skipping batch {batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                        continue\n",
    "\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = train_model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # Print the full stack trace for debugging\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': train_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        train_model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for eval_batch_idx, batch_data in enumerate(test_loader):\n",
    "                try:\n",
    "                    if is_multiview:\n",
    "                        # if is_conv_vae_concat and is_preprocessed:\n",
    "                        #     print(\"we are here in concattttttttt convvvvv\")\n",
    "                        #     # ConvVAE concat model expects single tensor with concatenated views\n",
    "                        #     batch_X, batch_y = batch_data\n",
    "                        #     batch_X = batch_X.to(device)\n",
    "                        #     predictions = train_model(batch_X)\n",
    "                        #     continue\n",
    "                        if is_concat_model:\n",
    "                            # For concat model, get latent from sp.enc.encode\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping eval batch {eval_batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            # Process each sample to get latent features\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                # Get views for this sample\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                                # Get latent using concat model's encoder (without proprioceptor)\n",
    "                                # sample_features = sp.enc.encode(sample_views).cpu().numpy()\n",
    "                                sample_features = sp.enc.encode(sample_views)\n",
    "                                # Convert to tensor and move to device\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            # Stack features\n",
    "                            batch_X = torch.stack(batch_features).squeeze(1).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "\n",
    "                        elif is_cnn_multiview:\n",
    "                            # Handle CNN-based multi-view processing\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping eval batch {eval_batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                                sample_features = sp.process(sample_views)\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            batch_X = torch.stack(batch_features).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "                        else:\n",
    "                            # Process the batch the same way as in training\n",
    "                            batch_views, batch_y = batch_data\n",
    "\n",
    "                            # Check batch size for BatchNorm compatibility\n",
    "                            if batch_views[0].size(0) == 1 and has_batch_norm(train_model):\n",
    "                                print(f\"Warning: Skipping eval batch {eval_batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                                continue\n",
    "\n",
    "                            batch_size = batch_views[0].size(0)\n",
    "                            batch_features = []\n",
    "\n",
    "                            for i in range(batch_size):\n",
    "                                sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                                sample_features = sp.process(sample_views)\n",
    "                                # Convert numpy array to tensor and move to device\n",
    "                                sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                                batch_features.append(sample_features_tensor)\n",
    "\n",
    "                            batch_X = torch.stack(batch_features).to(device)\n",
    "                            predictions = train_model(batch_X)\n",
    "                    else:\n",
    "                        batch_X, batch_y = batch_data\n",
    "\n",
    "                        # Check batch size for BatchNorm compatibility\n",
    "                        if batch_X.size(0) == 1 and has_batch_norm(train_model):\n",
    "                            print(f\"Warning: Skipping eval batch {eval_batch_idx} with size 1 (incompatible with BatchNorm)\")\n",
    "                            continue\n",
    "\n",
    "                        batch_X = batch_X.to(device)\n",
    "                        predictions = train_model(batch_X)\n",
    "\n",
    "                    # Make sure batch_y is on the same device\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    loss = criterion(predictions, batch_y)\n",
    "                    test_loss += loss.item()\n",
    "                    eval_batch_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation batch {eval_batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': train_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': train_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Update learning rate if using a scheduler\n",
    "        # if scheduler is not None:\n",
    "        #     scheduler.step(avg_test_loss)\n",
    "        #     print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        train_model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(train_model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing final model from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/visual_proprioception/vit_base_128/proprioception_mlp.pth\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Error in evaluation batch: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "Loaded model evaluation loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3196509/3136593053.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_model.load_state_dict(torch.load(final_modelfile, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisProprio_SimpleMLPRegression(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "train_and_save_proprioception_model(exp, is_preprocessed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Model Information ***\n",
      "✗ Single-view model detected\n",
      "  Model type: ViTEncoder\n",
      "  Latent size: 128\n",
      "\n",
      "Training complete! Model saved to:\n",
      "  /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/visual_proprioception/vit_base_128/proprioception_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Additional debug info - print model type information\n",
    "print(\"\\n*** Model Information ***\")\n",
    "if hasattr(sp, 'enc'):\n",
    "    # Check if model is multi-view\n",
    "    is_multiview = hasattr(sp.enc, 'feature_extractors') and isinstance(sp.enc.feature_extractors, nn.ModuleList)\n",
    "    if is_multiview:\n",
    "        num_views = len(sp.enc.feature_extractors)\n",
    "        view_type = \"CNN-based\" if any(['CNN' in str(type(sp.enc)), 'ResNet' in str(type(sp.enc)), 'VGG' in str(type(sp.enc))]) else \"ViT-based\"\n",
    "        print(f\"✓ Multi-view model detected: {view_type} with {num_views} views\")\n",
    "        print(f\"  Model type: {type(sp.enc).__name__}\")\n",
    "        print(f\"  Latent size: {sp.enc.latent_size}\")\n",
    "\n",
    "        # Print fusion method if applicable\n",
    "        if hasattr(sp.enc, 'fusion_type'):\n",
    "            print(f\"  Fusion method: {sp.enc.fusion_type}\")\n",
    "        else:\n",
    "            print(f\"  Fusion method: feature concatenation\")\n",
    "    else:\n",
    "        print(f\"✗ Single-view model detected\")\n",
    "        print(f\"  Model type: {type(sp.enc).__name__}\")\n",
    "        if hasattr(sp.enc, 'latent_size'):\n",
    "            print(f\"  Latent size: {sp.enc.latent_size}\")\n",
    "else:\n",
    "    print(\"Cannot determine model type - no 'enc' attribute found\")\n",
    "\n",
    "print(\"\\nTraining complete! Model saved to:\")\n",
    "print(f\"  {pathlib.Path(exp['data_dir'], exp['proprioception_mlp_model_file'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
