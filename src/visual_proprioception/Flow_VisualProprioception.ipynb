{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bea14d",
   "metadata": {},
   "source": [
    "# Visual proprioception flow\n",
    "\n",
    "Create the full flow for training models for visual proprioception. This notebook programmatically generates a set of exp/runs that cover all the necessary components for a visual proprioception system (sensor processing,  visual proprioception regressor and verification notebooks).\n",
    "\n",
    "Then, it writes the exp/runs into an external directory full separated from the github source, and creates an automation script that runs them. A separate directory for the results is also created. \n",
    "\n",
    "Finally, it runs the necessary notebooks to execute the whole flow using papermill.\n",
    "\n",
    "The results directory contain the output of this flow, both in terms of trained models, as well as results (in the verification exp/run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a83531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pprint\n",
    "import copy\n",
    "import socket\n",
    "import pathlib\n",
    "import yaml\n",
    "import tqdm\n",
    "import papermill\n",
    "# from automate import automate_exprun\n",
    "import visproprio_helper \n",
    "from demonstration.demonstration import list_demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173f38",
   "metadata": {},
   "source": [
    "# Setting up the separate directory\n",
    "Setting up a separate directory for generated exp/run config files and the results. This cell will create a new directory. \n",
    "\n",
    "__Note__\n",
    "Currently, as of 2025-09-03, we don't have a way to put the training data into this directory. Which means that once the code below creates the new directories, we need to manually copy the demonstrations as follows:\n",
    "* the demonstration descriptor (eg. {demonstration_run}.yaml) into {directory_name}/exprun/demonstration\n",
    "* the demonstration runs into {directory_name}/result/demonstration/{demonstration_run}/\n",
    "\n",
    "It would be useful to set up a special import mechanism for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997ac9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Hostname is glassy\n",
      "***Path for external experiments:\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\n",
      "***Path for external data:\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\result\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lotzi\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tc:\\Users\\lotzi\\Work\\_Config\\BerryPicker\\cfg\\settings.yaml\n",
      "***ExpRun**: Experiment config path changed to C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\n",
      "***ExpRun**: Experiment data path changed to C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\result\n",
      "***ExpRun**: Experiment demonstration copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\demonstration\n",
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\sensorprocessing_conv_vae\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\sensorprocessing_propriotuned_cnn\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\sensorprocessing_propriotuned_Vit\n",
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\sensorprocessing_aruco\n",
      "***ExpRun**: Experiment visual_proprioception copied to\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\exprun\\visual_proprioception\n"
     ]
    }
   ],
   "source": [
    "# the most likely changing things\n",
    "directory_name = \"VisualProprioception_flow_00\"\n",
    "demonstration_cam = \"dev0\"\n",
    "demonstration_run = \"touch-apple\"\n",
    "# determine these values based on experience\n",
    "# epochs_sp = 300\n",
    "# epochs_vp = 1000\n",
    "epochs_sp = 5\n",
    "epochs_vp = 10\n",
    "image_size = [256, 256] # for vgg... etc\n",
    "\n",
    "# Use exist_ok not to-re-run previously successfully run models\n",
    "creation_style = \"exist-ok\"\n",
    "# creation_style = \"discard-old\"\n",
    "\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"***Hostname is {hostname}\")\n",
    "if hostname == \"raven\":\n",
    "    rootdir = pathlib.Path(f\"~/WORK/_DataExternal/\").expanduser()\n",
    "elif hostname == \"szenes.local\":\n",
    "    rootdir = pathlib.Path(f\"~/Documents/Develop/Data/\").expanduser()\n",
    "elif hostname == \"glassy\":\n",
    "    rootdir = pathlib.Path(f\"~/Work/_DataExternal/\").expanduser()\n",
    "else:\n",
    "    rootdir = pathlib.Path(Config()[\"experiment_external\"])\n",
    "exprun_path, result_path = visproprio_helper.external_setup(directory_name, rootdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d27c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_conv_vae(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the conv-vae sensorprocessing with the right training data and parameters. Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    NOTE: a similar function is in Flow_BehaviorCloning.\n",
    "    \"\"\"\n",
    "    val = {}\n",
    "    val[\"latent_size\"] = params[\"latent_size\"]\n",
    "    val[\"epochs\"] = params[\"epochs\"]\n",
    "    val[\"save_period\"] = 5\n",
    "    val[\"training_data\"] = params[\"training_data\"]\n",
    "    val[\"validation_data\"] = params[\"validation_data\"]\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_experiment_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_Conv-VAE\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_Conv_VAE.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_propriotuned_cnn(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned CNN with the right training data and parameters. \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_experiment_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_CNN\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_CNN.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476839eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_train(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    \n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_experiment_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Train_{run_name}\"\n",
    "    v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99c9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_verify(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    \n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_experiment_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Verify_{run_name}\"\n",
    "    v[\"notebook\"] = \"TODO Verify.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64e6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_compare(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.  \n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation. \n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"name\"] = exp_name\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_experiment_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file \n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Compare_{run_name}\"\n",
    "    v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cd090",
   "metadata": {},
   "source": [
    "### Generate the exp/runs to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020b8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: demonstration/touch-apple successfully loaded\n",
      "***The demos considered\n",
      "['2025_08_07__15_13_18',\n",
      " '2025_08_07__15_17_27',\n",
      " '2025_08_07__15_18_27',\n",
      " '2025_08_07__15_19_25',\n",
      " '2025_08_07__15_20_47',\n",
      " '2025_08_07__15_22_29',\n",
      " '2025_08_07__15_23_23',\n",
      " '2025_08_07__15_24_25',\n",
      " '2025_08_07__15_25_18',\n",
      " '2025_08_07__15_26_10',\n",
      " '2025_08_07__15_28_02']\n",
      "sensorprocessing_conv_vae sp_conv_vae_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_128_0001 128\n",
      "sensorprocessing_conv_vae sp_conv_vae_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_256_0001 256\n"
     ]
    }
   ],
   "source": [
    "expruns = []\n",
    "# overall values\n",
    "latent_sizes = [128, 256] # the possible latent sizes we consider\n",
    "cnntypes = [\"vgg19\", \"resnet50\"] # the CNN architectures we consider\n",
    "\n",
    "#\n",
    "# Configuring the training and validation data, based \n",
    "# on all the demonstrations of a particular type\n",
    "#\n",
    "experiment = \"demonstration\"\n",
    "exp = Config().get_experiment(experiment, demonstration_run)\n",
    "demos = list_demos(exp)\n",
    "print(\"***The demos considered\")\n",
    "pprint.pprint(demos)\n",
    "# create a data structure from all the data directories \n",
    "# we considered\n",
    "data = []\n",
    "for demo in demos:\n",
    "    data.append([demonstration_run, demo, demonstration_cam])\n",
    "# the training data used to train/validate the sensor processor\n",
    "sp_training_data = data[0:2]\n",
    "sp_validation_data = data[2:4]\n",
    "# the training data used to train/validate the visual proprioception regressor\n",
    "vp_training_data = data[4:6]\n",
    "vp_validation_data = data[6:]\n",
    "\n",
    "\n",
    "# *******************************************\n",
    "# generate the sensorprocessing models\n",
    "# *******************************************\n",
    "sps = [] # the list of the sensorprocessing models (exp/run)\n",
    "for latent_size in latent_sizes:\n",
    "\n",
    "    # generate the vae exprun\n",
    "    exp_name = \"sensorprocessing_conv_vae\"\n",
    "    run_name = f\"sp_conv_vae_{latent_size}_0001\"\n",
    "    params = {}\n",
    "    params[\"latent_size\"] = latent_size\n",
    "    params[\"epochs\"] = epochs_sp\n",
    "    params[\"training_data\"] = sp_training_data\n",
    "    params[\"validation_data\"] = sp_validation_data\n",
    "    exprun = generate_sp_conv_vae(\n",
    "        exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "    exprun[\"latent_size\"] = latent_size\n",
    "    sps.append(exprun)\n",
    "    expruns.append(exprun)\n",
    "\n",
    "    # generate the propriotuned expruns\n",
    "    for cnntype in cnntypes:\n",
    "        exp_name = \"sensorprocessing_propriotuned_cnn\"\n",
    "        run_name = f\"sp_{cnntype}_{latent_size}_0001\"\n",
    "        params = {}\n",
    "        params[\"image_size\"] = image_size\n",
    "        params[\"latent_size\"] = latent_size\n",
    "        params[\"epochs\"] = epochs_sp\n",
    "        params[\"training_data\"] = sp_training_data\n",
    "        params[\"validation_data\"] = sp_validation_data\n",
    "        if cnntype == \"vgg19\":\n",
    "            params[\"class\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"VGG19ProprioTunedRegression\"\n",
    "        elif cnntype == \"resnet50\":\n",
    "            params[\"class\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"ResNetProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = True\n",
    "            params[\"reductor_step_1\"] = 512\n",
    "            params[\"proprio_step_1\"] = 64\n",
    "            params[\"proprio_step_2\"] = 16\n",
    "        else:\n",
    "            raise Exception(f\"Unknown cnntype {cnntype}\")\n",
    "        params[\"loss\"] = \"MSELoss\" # alternative L1Loss\n",
    "        params[\"learning_rate\"] = 0.001\n",
    "        # alternative\n",
    "        exprun = generate_sp_propriotuned_cnn(\n",
    "            exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "        exprun[\"latent_size\"] = latent_size\n",
    "        exprun[\"cnntype\"] = cnntype\n",
    "        sps.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "    # FIXME: add here the ViT models\n",
    "\n",
    "# *******************************************\n",
    "# generate the proprioception models\n",
    "# *******************************************\n",
    "vpruns = []\n",
    "vpruns_latent = {128:[], 256:[]}\n",
    "for spexp, sprun,latent_size in [(a[\"experiment\"],a[\"run\"],a[\"latent_size\"]) for a in sps]:\n",
    "    print(spexp, sprun, latent_size)\n",
    "    # *** generate the vp train expruns ***\n",
    "    exp_name = \"visual_proprioception\"\n",
    "    run_name = \"vp_\" + sprun[3:]\n",
    "    vpruns.append(run_name)\n",
    "    vpruns_latent[latent_size].append(run_name)\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"output_size\"] = 6\n",
    "    params[\"encoding_size\"] = latent_size\n",
    "    params[\"training_data\"] = vp_training_data\n",
    "    params[\"validation_data\"] = vp_validation_data\n",
    "\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"loss\"] = \"MSE\"\n",
    "    params[\"epochs\"] = epochs_vp\n",
    "    params[\"batch_size\"] = 64\n",
    "    # FIXME this is hackish, should not do it this way\n",
    "    if \"vae\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ConvVaeSensorProcessing\"\n",
    "    elif \"resnet\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "    elif \"vgg19\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected sprun {sprun}\")\n",
    "\n",
    "    params[\"sp_experiment\"] = spexp\n",
    "    params[\"sp_run\"] = sprun\n",
    "\n",
    "    exprun = generate_vp_train(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "    # *** generate the vp verify expruns FIXME: not implemented yet ***\n",
    "    params_verify = {}\n",
    "    \n",
    "    expruns.append(exprun)\n",
    "# *******************************************\n",
    "# generate the comparisons: all, for latents 128 and 256\n",
    "# *******************************************\n",
    "exp_name = \"visual_proprioception\"\n",
    "# all\n",
    "run_name = \"vp_comp_flow_all\"\n",
    "params = {}\n",
    "params[\"name\"] = run_name\n",
    "params[\"tocompare\"] = vpruns\n",
    "exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "expruns.append(exprun)\n",
    "# by latent\n",
    "for latent_size in [128, 256]:\n",
    "    run_name = f\"vp_comp_flow_{latent_size}\"\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"tocompare\"] = vpruns_latent[latent_size]\n",
    "    exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "    expruns.append(exprun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef389ef",
   "metadata": {},
   "source": [
    "### Run the flow\n",
    "\n",
    "Run the flow, that is, run a series of notebooks with papermill. In order to follow the execution inside these notebooks, one needs to open the output notebook, which is in the output_filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21aab023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Starting automated running of the flow.\n",
      " The path for the output notebooks is\n",
      "C:\\Users\\lotzi\\Work\\_DataExternal\\VisualProprioception_flow_00\\result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_128_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lotzi\\Work\\_VirtualEnvs\\BerryPicker\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Executing: 100%|██████████| 11/11 [00:03<00:00,  3.35cell/s]\n",
      "  7%|▋         | 1/15 [00:03<00:46,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Propriotuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_128_0001\n",
      "--> Train_Propriotuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 12/12 [00:02<00:00,  4.30cell/s]\n",
      " 13%|█▎        | 2/15 [00:06<00:39,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Propriotuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_128_0001\n",
      "--> Train_Propriotuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 12/12 [00:02<00:00,  4.48cell/s]\n",
      " 20%|██        | 3/15 [00:08<00:34,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_256_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 11/11 [00:03<00:00,  3.48cell/s]\n",
      " 27%|██▋       | 4/15 [00:11<00:32,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Propriotuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_256_0001\n",
      "--> Train_Propriotuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 12/12 [00:02<00:00,  4.57cell/s]\n",
      " 33%|███▎      | 5/15 [00:14<00:28,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Propriotuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_256_0001\n",
      "--> Train_Propriotuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 12/12 [00:02<00:00,  4.40cell/s]\n",
      " 40%|████      | 6/15 [00:17<00:25,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:03<00:00,  4.23cell/s]\n",
      " 47%|████▋     | 7/15 [00:20<00:24,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:04<00:00,  3.64cell/s]\n",
      " 53%|█████▎    | 8/15 [00:25<00:23,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:03<00:00,  4.07cell/s]\n",
      " 60%|██████    | 9/15 [00:28<00:20,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:03<00:00,  4.25cell/s]\n",
      " 67%|██████▋   | 10/15 [00:32<00:17,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:04<00:00,  3.74cell/s]\n",
      " 73%|███████▎  | 11/15 [00:36<00:14,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 15/15 [00:03<00:00,  3.97cell/s]\n",
      " 80%|████████  | 12/15 [00:40<00:11,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_all\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_all_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 18/18 [00:08<00:00,  2.07cell/s]\n",
      " 87%|████████▋ | 13/15 [00:48<00:10,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_128\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_128_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 18/18 [00:07<00:00,  2.54cell/s]\n",
      " 93%|█████████▎| 14/15 [00:55<00:05,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_comp_flow_256\n",
      "--> Compare_VisualProprioception_visual_proprioception_vp_comp_flow_256_output.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing: 100%|██████████| 18/18 [00:07<00:00,  2.53cell/s]\n",
      "100%|██████████| 15/15 [01:02<00:00,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"***Starting automated running of the flow.\\n The path for the output notebooks is\\n{result_path}\")\n",
    "\n",
    "for exprun in tqdm.tqdm(expruns):\n",
    "    print(f\"***Automating {exprun['notebook']} :\\n {exprun['experiment']}/{exprun['run']}\")\n",
    "    notebook_path = pathlib.Path(\"..\", exprun[\"notebook\"])\n",
    "    output_filename = f\"{notebook_path.stem}_{exprun['experiment']}_{exprun['run']}_output{notebook_path.suffix}\"\n",
    "    print(f\"--> {output_filename}\")\n",
    "    # parameters that we are passing on to the notebook\n",
    "    params = {}\n",
    "    params[\"experiment\"] = exprun[\"experiment\"]\n",
    "    params[\"run\"] = exprun[\"run\"]\n",
    "    params[\"external_path\"] = exprun[\"external_path\"]\n",
    "    params[\"data_path\"] = exprun[\"data_path\"]    \n",
    "    output_path = pathlib.Path(result_path, output_filename)\n",
    "    try:\n",
    "        papermill.execute_notebook(\n",
    "            notebook_path,\n",
    "            output_path.absolute(),\n",
    "            cwd=notebook_path.parent,\n",
    "            parameters=params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"There was an exception {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8455f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
