{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bea14d",
   "metadata": {},
   "source": [
    "# Visual proprioception flow\n",
    "\n",
    "Create the full flow for training models for visual proprioception. This notebook programmatically generates a set of exp/runs that cover all the necessary components for a visual proprioception system (sensor processing,  visual proprioception regressor and verification notebooks).\n",
    "\n",
    "Then, it writes the exp/runs into an external directory full separated from the github source, and creates an automation script that runs them. A separate directory for the results is also created. \n",
    "\n",
    "Finally, it runs the necessary notebooks to execute the whole flow using papermill.\n",
    "\n",
    "The results directory contain the output of this flow, both in terms of trained models, as well as results (in the verification exp/run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a83531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\pathlib\\__init__.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lotzi\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tc:\\Users\\lotzi\\Work\\_Config\\BerryPicker\\cfg\\settings.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import copy\n",
    "import pprint\n",
    "import pathlib\n",
    "import yaml\n",
    "import tqdm\n",
    "import papermill\n",
    "import visproprio_helper\n",
    "from demonstration.demonstration import list_demos\n",
    "from demonstration.demopack import import_demopack, group_chooser_sp_vp_standard, group_chooser_sp_vp_multiview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173f38",
   "metadata": {},
   "source": [
    "# Setting up the separate directory\n",
    "Setting up a separate directory for generated exp/run config files and the results. This cell will create a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997ac9a7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "flow_name = \"VisualProprioception_flow_multiview_70\"\n",
    "demopack_name = \"random-both-cameras-video\"\n",
    "\n",
    "# Camera configuration\n",
    "# For single-view models:\n",
    "demonstration_cam = \"dev2\"\n",
    "# For multi-view models:\n",
    "multiview_cameras = [\"dev2\", \"dev3\"]\n",
    "num_views = 2\n",
    "\n",
    "# Model selection flags\n",
    "do_VAE = True\n",
    "do_VGG = True\n",
    "do_RESNET = True\n",
    "do_VIT = True\n",
    "do_VIT_MULTIVIEW = True  # Enable multiview ViT models\n",
    "\n",
    "# Training parameters\n",
    "epochs_sp = 10\n",
    "epochs_vp = 10\n",
    "image_size = [256, 256]  # for VGG, ResNet etc.\n",
    "vit_image_size = [224, 224]  # ViT models require 224x224\n",
    "\n",
    "# Latent sizes to explore\n",
    "latent_sizes = [128, 256]\n",
    "\n",
    "# CNN and ViT architectures\n",
    "cnntypes = [\"vgg19\", \"resnet50\"]\n",
    "vit_types = [\"vit_base\"]  # Can add \"vit_large\"\n",
    "\n",
    "# Fusion methods for multiview models\n",
    "# fusion_types = [\"concat_proj\", \"indiv_proj\", \"attention\", \"weighted_sum\", \"gated\"]\n",
    "fusion_types = [\"attention\"]  # Use this for faster testing\n",
    "\n",
    "# Creation style: \"exist-ok\" to skip existing, \"discard-old\" to recreate\n",
    "creation_style = \"exist-ok\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8ba6e",
   "metadata": {},
   "source": [
    "### SETUP EXTERNAL DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8448e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demopacks base: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Demopacks\n",
      "Base exists: True\n",
      "Demopack path: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Demopacks\\random-both-cameras-video\n",
      "Demopack exists: True\n",
      "***Path for external experiments:\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\n",
      "***Path for external data:\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\result\n",
      "***ExpRun**: Experiment config path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\n",
      "***ExpRun**: Experiment data path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\result\n",
      "***ExpRun**: Experiment robot_al5d copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\robot_al5d\n",
      "***ExpRun**: Experiment demonstration copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\demonstration\n",
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_conv_vae\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_propriotuned_cnn\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_propriotuned_Vit\n",
      "***ExpRun**: Experiment sensorprocessing_aruco copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_aruco\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_Vit_multiview copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_propriotuned_Vit_multiview\n",
      "***ExpRun**: Experiment sensorprocessing_propriotuned_cnn_multiview copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_propriotuned_cnn_multiview\n",
      "***ExpRun**: Experiment sensorprocessing_conv_vae_concat_multiview copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\sensorprocessing_conv_vae_concat_multiview\n",
      "***ExpRun**: Experiment visual_proprioception copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\exprun\\visual_proprioception\n",
      "*** import_demopack: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Demopacks\\random-both-cameras-video, target directory c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\result\\demonstration\\random-both-cameras-video already exists, not copying\n",
      "***ExpRun**: Configuration for exp/run: demonstration/random-both-cameras-video successfully loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SETUP EXTERNAL DIRECTORY\n",
    "# =============================================================================\n",
    "\n",
    "demopacks_base = pathlib.Path(Config()[\"demopacks_path\"]).expanduser()\n",
    "demopack_path = pathlib.Path(demopacks_base, demopack_name)\n",
    "print(f\"Demopacks base: {demopacks_base}\")\n",
    "print(f\"Base exists: {demopacks_base.exists()}\")\n",
    "print(f\"Demopack path: {demopack_path}\")\n",
    "print(f\"Demopack exists: {demopack_path.exists()}\")\n",
    "\n",
    "exprun_path, result_path = visproprio_helper.external_setup(\n",
    "    flow_name,\n",
    "    pathlib.Path(Config()[\"flows_path\"]).expanduser()\n",
    ")\n",
    "\n",
    "# Import demopack using appropriate group chooser\n",
    "demopack_path = pathlib.Path(Config()[\"demopacks_path\"], demopack_name).expanduser()\n",
    "selection = import_demopack(demopack_path, group_chooser_sp_vp_standard)\n",
    "\n",
    "# Configure demonstration experiment\n",
    "experiment = \"demonstration\"\n",
    "exp = Config().get_experiment(experiment, demopack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b7e91",
   "metadata": {},
   "source": [
    "### TRAINING DATA CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83eff4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection: {'sp_training': ['sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003'], 'sp_validation': ['sp_validation_00000', 'sp_validation_00001'], 'sp_testing': ['sp_testing_00000', 'sp_testing_00001'], 'vp_training': ['vp_training_00000', 'vp_training_00001', 'vp_training_00002', 'vp_training_00003'], 'vp_validation': ['vp_validation_00000', 'vp_validation_00001'], 'vp_testing': ['vp_testing_00000', 'vp_testing_00001']}\n",
      "SP training demos: ['sp_testing_00000', 'sp_testing_00001', 'sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003', 'sp_validation_00000', 'sp_validation_00001']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TRAINING DATA CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Single-view training data: [run, demo_name, camera]\n",
    "sp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_training\"]]\n",
    "sp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_validation\"]]\n",
    "vp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_training\"]]\n",
    "vp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_validation\"]]\n",
    "\n",
    "\n",
    "# Multi-view training data: [run, demo_name, camera_string] - cameras stored separately\n",
    "# For multiview, we use a comma-separated string that gets parsed later\n",
    "# sp_training_data_multiview = [[demopack_name, demo, multiview_cameras] for demo in selection[\"sp_training\"]]\n",
    "cameras_str = \",\".join(multiview_cameras)\n",
    "sp_training_data_multiview = [[demopack_name, demo, cameras_str] for demo in selection[\"sp_training\"]]\n",
    "sp_validation_data_multiview = [[demopack_name, demo, cameras_str] for demo in selection[\"sp_validation\"]]\n",
    "vp_training_data_multiview = [[demopack_name, demo, cameras_str] for demo in selection[\"vp_training\"]]\n",
    "vp_validation_data_multiview = [[demopack_name, demo, cameras_str] for demo in selection[\"vp_validation\"]]\n",
    "\n",
    "print(f\"Selection: {selection}\")\n",
    "demos = list_demos(exp)\n",
    "print(f\"SP training demos: {list_demos(exp, 'sp')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_training': ['sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003'], 'sp_validation': ['sp_validation_00000', 'sp_validation_00001'], 'sp_testing': ['sp_testing_00000', 'sp_testing_00001'], 'vp_training': ['vp_training_00000', 'vp_training_00001', 'vp_training_00002', 'vp_training_00003'], 'vp_validation': ['vp_validation_00000', 'vp_validation_00001'], 'vp_testing': ['vp_testing_00000', 'vp_testing_00001']}\n"
     ]
    }
   ],
   "source": [
    "print(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444076a6",
   "metadata": {},
   "source": [
    "### TRAINING DATA CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608fa32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection: {'sp_training': ['sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003'], 'sp_validation': ['sp_validation_00000', 'sp_validation_00001'], 'sp_testing': ['sp_testing_00000', 'sp_testing_00001'], 'vp_training': ['vp_training_00000', 'vp_training_00001', 'vp_training_00002', 'vp_training_00003'], 'vp_validation': ['vp_validation_00000', 'vp_validation_00001'], 'vp_testing': ['vp_testing_00000', 'vp_testing_00001']}\n",
      "SP training demos: ['sp_testing_00000', 'sp_testing_00001', 'sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003', 'sp_validation_00000', 'sp_validation_00001']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TRAINING DATA CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Single-view training data: [run, demo_name, camera]\n",
    "sp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_training\"]]\n",
    "sp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"sp_validation\"]]\n",
    "vp_training_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_training\"]]\n",
    "vp_validation_data = [[demopack_name, demo, demonstration_cam] for demo in selection[\"vp_validation\"]]\n",
    "\n",
    "# Multi-view training data: [run, demo_name, [camera_list]]\n",
    "sp_training_data_multiview = [[demopack_name, demo, multiview_cameras] for demo in selection[\"sp_training\"]]\n",
    "sp_validation_data_multiview = [[demopack_name, demo, multiview_cameras] for demo in selection[\"sp_validation\"]]\n",
    "vp_training_data_multiview = [[demopack_name, demo, multiview_cameras] for demo in selection[\"vp_training\"]]\n",
    "vp_validation_data_multiview = [[demopack_name, demo, multiview_cameras] for demo in selection[\"vp_validation\"]]\n",
    "\n",
    "print(f\"Selection: {selection}\")\n",
    "demos = list_demos(exp)\n",
    "print(f\"SP training demos: {list_demos(exp, 'sp')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2ada4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sp_testing_00000', 'sp_testing_00001', 'sp_training_00000', 'sp_training_00001', 'sp_training_00002', 'sp_training_00003', 'sp_validation_00000', 'sp_validation_00001']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sp_training_00000',\n",
       " 'sp_training_00001',\n",
       " 'sp_training_00002',\n",
       " 'sp_training_00003']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demos = list_demos(exp)\n",
    "# print(demos)\n",
    "print(list_demos(exp, \"sp\"))\n",
    "[s for s in demos if s.startswith(\"sp_training\" + \"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e3b4a",
   "metadata": {},
   "source": [
    "### GENERATOR FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0958627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GENERATOR FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_sp_conv_vae(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the conv-vae sensorprocessing.\"\"\"\n",
    "    val = {}\n",
    "    val[\"latent_size\"] = params[\"latent_size\"]\n",
    "    val[\"epochs\"] = params[\"epochs\"]\n",
    "    val[\"save_period\"] = 5\n",
    "    val[\"training_data\"] = params[\"training_data\"]\n",
    "    val[\"validation_data\"] = params[\"validation_data\"]\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_Conv-VAE\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_Conv_VAE.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_sp_propriotuned_cnn(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned CNN.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_CNN\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_CNN.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def generate_sp_propriotuned_cnn_multiview(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned multiview CNN.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "    val[\"num_views\"] = params.get(\"num_views\", 2)\n",
    "    val[\"fusion_type\"] = params.get(\"fusion_type\", \"attention\")\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_CNN_Multiview\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_CNN_multiview.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_sp_propriotuned_vit(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned ViT.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "\n",
    "    # Robot configuration\n",
    "    val[\"robot_exp\"] = \"robot_al5d\"\n",
    "    val[\"robot_run\"] = \"position_controller_00\"\n",
    "    val[\"proprioception_mlp_model_file\"] = \"proprioception_mlp.pth\"\n",
    "\n",
    "    # ViT architecture parameters based on model type\n",
    "    if params[\"vit_model\"] == \"vit_base\":\n",
    "        val[\"vit_model\"] = \"vit_b_16\"\n",
    "        val[\"vit_output_dim\"] = 768\n",
    "        val[\"projection_hidden_dim\"] = 512\n",
    "    elif params[\"vit_model\"] == \"vit_large\":\n",
    "        val[\"vit_model\"] = \"vit_l_16\"\n",
    "        val[\"vit_output_dim\"] = 1024\n",
    "        val[\"projection_hidden_dim\"] = 768\n",
    "\n",
    "    val[\"vit_weights\"] = \"DEFAULT\"\n",
    "    val[\"proprio_step_1\"] = 64\n",
    "    val[\"proprio_step_2\"] = 64\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_ViT\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_VIT.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_sp_propriotuned_vit_multiview(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned multiview ViT.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "\n",
    "    # Multiview-specific\n",
    "    val[\"num_views\"] = params.get(\"num_views\", 2)\n",
    "    val[\"fusion_type\"] = params.get(\"fusion_type\", \"attention\")\n",
    "    val[\"cameras\"] = params.get(\"cameras\", multiview_cameras)\n",
    "\n",
    "    # Robot configuration\n",
    "    val[\"robot_exp\"] = \"robot_al5d\"\n",
    "    val[\"robot_run\"] = \"position_controller_00\"\n",
    "    val[\"proprioception_mlp_model_file\"] = \"proprioception_mlp.pth\"\n",
    "\n",
    "    # ViT architecture parameters based on model type\n",
    "    if params[\"vit_model\"] == \"vit_base\":\n",
    "        val[\"vit_model\"] = \"vit_b_16\"\n",
    "        val[\"vit_output_dim\"] = 768\n",
    "        val[\"projection_hidden_dim\"] = 512\n",
    "    elif params[\"vit_model\"] == \"vit_large\":\n",
    "        val[\"vit_model\"] = \"vit_l_16\"\n",
    "        val[\"vit_output_dim\"] = 1024\n",
    "        val[\"projection_hidden_dim\"] = 768\n",
    "\n",
    "    val[\"vit_weights\"] = \"DEFAULT\"\n",
    "    val[\"proprio_step_1\"] = 64\n",
    "    val[\"proprio_step_2\"] = 64\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_ViT_Multiview\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_VIT_multiview.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_vp_train(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training visual proprioception regressor.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Determine which notebook to use based on model type\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Train_{run_name}\"\n",
    "\n",
    "    is_multiview = (\n",
    "        \"multiview\" in params.get(\"sensor_processing\", \"\").lower() or\n",
    "        params.get(\"num_views\", 1) > 1\n",
    "    )\n",
    "\n",
    "    if is_multiview:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception_multiview.ipynb\"\n",
    "    else:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception.ipynb\"\n",
    "\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_vp_verify(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Verify_{run_name}\"\n",
    "    v[\"notebook\"] = \"TODO Verify.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_vp_compare(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the comparison of visual proprioception models.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"name\"] = exp_name\n",
    "\n",
    "    # Save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # Determine if any multiview models are in the comparison\n",
    "    has_multiview = any(\"multiview\" in r.lower() for r in params.get(\"tocompare\", []))\n",
    "\n",
    "    # Generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Compare_{run_name}\"\n",
    "    if has_multiview or do_VIT_MULTIVIEW:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb\"\n",
    "    else:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9b7ca",
   "metadata": {},
   "source": [
    "### GENERATE EXP/RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e5877ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GENERATE EXP/RUNS\n",
    "# =============================================================================\n",
    "\n",
    "expruns = []\n",
    "sps = []  # list of sensor processing models (exp/run)\n",
    "sps_singleview = []\n",
    "sps_multiview = []\n",
    "vpruns = []\n",
    "vpruns_singleview = []\n",
    "vpruns_multiview = []\n",
    "vpruns_latent = {128: [], 256: []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3319220",
   "metadata": {},
   "source": [
    "### GENERATE EXP/RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8def94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating VP for: sensorprocessing_conv_vae/sp_conv_vae_128_0001 (multiview=False, latent=128)\n",
      "Generating VP for: sensorprocessing_propriotuned_Vit/sp_vit_base_128_0001 (multiview=False, latent=128)\n",
      "Generating VP for: sensorprocessing_propriotuned_Vit_multiview/sp_vit_base_multiview_attention_128_0001 (multiview=True, latent=128)\n",
      "Generating VP for: sensorprocessing_propriotuned_cnn/sp_vgg19_128_0001 (multiview=False, latent=128)\n",
      "Generating VP for: sensorprocessing_propriotuned_cnn/sp_resnet50_128_0001 (multiview=False, latent=128)\n",
      "Generating VP for: sensorprocessing_conv_vae/sp_conv_vae_256_0001 (multiview=False, latent=256)\n",
      "Generating VP for: sensorprocessing_propriotuned_Vit/sp_vit_base_256_0001 (multiview=False, latent=256)\n",
      "Generating VP for: sensorprocessing_propriotuned_Vit_multiview/sp_vit_base_multiview_attention_256_0001 (multiview=True, latent=256)\n",
      "Generating VP for: sensorprocessing_propriotuned_cnn/sp_vgg19_256_0001 (multiview=False, latent=256)\n",
      "Generating VP for: sensorprocessing_propriotuned_cnn/sp_resnet50_256_0001 (multiview=False, latent=256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GENERATE EXP/RUNS\n",
    "# =============================================================================\n",
    "\n",
    "expruns = []\n",
    "sps = []  # list of sensor processing models (exp/run)\n",
    "sps_singleview = []\n",
    "sps_multiview = []\n",
    "vpruns = []\n",
    "vpruns_singleview = []\n",
    "vpruns_multiview = []\n",
    "vpruns_latent = {128: [], 256: []}\n",
    "\n",
    "# -----------------------------------------\n",
    "# Generate SENSOR PROCESSING models\n",
    "# -----------------------------------------\n",
    "for latent_size in latent_sizes:\n",
    "\n",
    "    # --- Conv-VAE (single-view only) ---\n",
    "    if do_VAE:\n",
    "        exp_name = \"sensorprocessing_conv_vae\"\n",
    "        run_name = f\"sp_conv_vae_{latent_size}_0001\"\n",
    "        params = {}\n",
    "        params[\"latent_size\"] = latent_size\n",
    "        params[\"epochs\"] = epochs_sp\n",
    "        params[\"training_data\"] = sp_training_data\n",
    "        params[\"validation_data\"] = sp_validation_data\n",
    "        exprun = generate_sp_conv_vae(\n",
    "            exprun_path=exprun_path, result_path=result_path,\n",
    "            params=params, exp_name=exp_name, run_name=run_name\n",
    "        )\n",
    "        exprun[\"latent_size\"] = latent_size\n",
    "        sps.append(exprun)\n",
    "        sps_singleview.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "    # --- Single-view ViT models ---\n",
    "    if do_VIT:\n",
    "        for vit_type in vit_types:\n",
    "            exp_name = \"sensorprocessing_propriotuned_Vit\"\n",
    "            run_name = f\"sp_{vit_type}_{latent_size}_0001\"\n",
    "            params = {}\n",
    "            params[\"image_size\"] = vit_image_size\n",
    "            params[\"latent_size\"] = latent_size\n",
    "            params[\"epochs\"] = epochs_sp\n",
    "            params[\"training_data\"] = sp_training_data\n",
    "            params[\"validation_data\"] = sp_validation_data\n",
    "            params[\"vit_model\"] = vit_type\n",
    "            params[\"class\"] = \"Vit\"\n",
    "            params[\"model\"] = \"VitProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = False\n",
    "            params[\"loss\"] = \"MSELoss\"\n",
    "            params[\"learning_rate\"] = 0.0001\n",
    "\n",
    "            exprun = generate_sp_propriotuned_vit(\n",
    "                exprun_path=exprun_path, result_path=result_path,\n",
    "                params=params, exp_name=exp_name, run_name=run_name\n",
    "            )\n",
    "            exprun[\"latent_size\"] = latent_size\n",
    "            exprun[\"vittype\"] = vit_type\n",
    "            sps.append(exprun)\n",
    "            sps_singleview.append(exprun)\n",
    "            expruns.append(exprun)\n",
    "\n",
    "    # --- Multi-view ViT models ---\n",
    "    if do_VIT_MULTIVIEW:\n",
    "        for vit_type in vit_types:\n",
    "            for fusion_type in fusion_types:\n",
    "                exp_name = \"sensorprocessing_propriotuned_Vit_multiview\"\n",
    "                run_name = f\"sp_{vit_type}_multiview_{fusion_type}_{latent_size}_0001\"\n",
    "                params = {}\n",
    "                params[\"image_size\"] = vit_image_size\n",
    "                params[\"latent_size\"] = latent_size\n",
    "                params[\"epochs\"] = epochs_sp\n",
    "                params[\"num_views\"] = num_views\n",
    "                params[\"cameras\"] = multiview_cameras\n",
    "                params[\"fusion_type\"] = fusion_type\n",
    "                params[\"training_data\"] = sp_training_data_multiview\n",
    "                params[\"validation_data\"] = sp_validation_data_multiview\n",
    "                params[\"vit_model\"] = vit_type\n",
    "                params[\"class\"] = \"Vit_multiview\"\n",
    "                params[\"model\"] = \"VitProprioTunedRegression_multiview\"\n",
    "                params[\"freeze_feature_extractor\"] = False\n",
    "                params[\"loss\"] = \"MSELoss\"\n",
    "                params[\"learning_rate\"] = 0.0001\n",
    "\n",
    "                exprun = generate_sp_propriotuned_vit_multiview(\n",
    "                    exprun_path=exprun_path, result_path=result_path,\n",
    "                    params=params, exp_name=exp_name, run_name=run_name\n",
    "                )\n",
    "                exprun[\"latent_size\"] = latent_size\n",
    "                exprun[\"vittype\"] = vit_type\n",
    "                exprun[\"fusion\"] = fusion_type\n",
    "                exprun[\"is_multiview\"] = True\n",
    "                sps.append(exprun)\n",
    "                sps_multiview.append(exprun)\n",
    "                expruns.append(exprun)\n",
    "\n",
    "    # --- Single-view CNN models (VGG, ResNet) ---\n",
    "    for cnntype in cnntypes:\n",
    "        exp_name = \"sensorprocessing_propriotuned_cnn\"\n",
    "        run_name = f\"sp_{cnntype}_{latent_size}_0001\"\n",
    "        params = {}\n",
    "        params[\"image_size\"] = image_size\n",
    "        params[\"latent_size\"] = latent_size\n",
    "        params[\"epochs\"] = epochs_sp\n",
    "        params[\"training_data\"] = sp_training_data\n",
    "        params[\"validation_data\"] = sp_validation_data\n",
    "\n",
    "        if cnntype == \"vgg19\":\n",
    "            if not do_VGG:\n",
    "                continue\n",
    "            params[\"class\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"VGG19ProprioTunedRegression\"\n",
    "        elif cnntype == \"resnet50\":\n",
    "            if not do_RESNET:\n",
    "                continue\n",
    "            params[\"class\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"ResNetProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = True\n",
    "            params[\"reductor_step_1\"] = 512\n",
    "            params[\"proprio_step_1\"] = 64\n",
    "            params[\"proprio_step_2\"] = 16\n",
    "        else:\n",
    "            raise Exception(f\"Unknown cnntype {cnntype}\")\n",
    "\n",
    "        params[\"loss\"] = \"MSELoss\"\n",
    "        params[\"learning_rate\"] = 0.001\n",
    "\n",
    "        exprun = generate_sp_propriotuned_cnn(\n",
    "            exprun_path=exprun_path, result_path=result_path,\n",
    "            params=params, exp_name=exp_name, run_name=run_name\n",
    "        )\n",
    "        exprun[\"latent_size\"] = latent_size\n",
    "        exprun[\"cnntype\"] = cnntype\n",
    "        sps.append(exprun)\n",
    "        sps_singleview.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Generate VISUAL PROPRIOCEPTION models\n",
    "# -----------------------------------------\n",
    "for spexp in sps:\n",
    "    spexp_name = spexp[\"experiment\"]\n",
    "    sprun = spexp[\"run\"]\n",
    "    latent_size = spexp[\"latent_size\"]\n",
    "    is_multiview = spexp.get(\"is_multiview\", False) or \"multiview\" in sprun.lower()\n",
    "\n",
    "    print(f\"Generating VP for: {spexp_name}/{sprun} (multiview={is_multiview}, latent={latent_size})\")\n",
    "\n",
    "    # Generate the VP train exp/run\n",
    "    exp_name = \"visual_proprioception\"\n",
    "    run_name = \"vp_\" + sprun[3:]  # Remove \"sp_\" prefix\n",
    "    vpruns.append(run_name)\n",
    "    vpruns_latent[latent_size].append(run_name)\n",
    "\n",
    "    if is_multiview:\n",
    "        vpruns_multiview.append(run_name)\n",
    "    else:\n",
    "        vpruns_singleview.append(run_name)\n",
    "\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"output_size\"] = 6\n",
    "    params[\"encoding_size\"] = latent_size\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"regressor_hidden_size_2\"] = 64\n",
    "    params[\"loss\"] = \"MSE\"\n",
    "    params[\"epochs\"] = epochs_vp\n",
    "    params[\"batch_size\"] = 64\n",
    "\n",
    "    # Use appropriate training data\n",
    "    if is_multiview:\n",
    "        params[\"training_data\"] = vp_training_data_multiview\n",
    "        params[\"validation_data\"] = vp_validation_data_multiview\n",
    "        params[\"num_views\"] = num_views\n",
    "        params[\"cameras\"] = multiview_cameras\n",
    "    else:\n",
    "        params[\"training_data\"] = vp_training_data\n",
    "        params[\"validation_data\"] = vp_validation_data\n",
    "\n",
    "    # Determine sensor processing class\n",
    "    if \"vae\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ConvVaeSensorProcessing\"\n",
    "    elif \"resnet\" in sprun.lower():\n",
    "        if is_multiview:\n",
    "            params[\"sensor_processing\"] = \"ResNetProprioTunedSensorProcessing_multiview\"\n",
    "            params[\"num_views\"] = len(multiview_cameras)  # e.g. 2\n",
    "        else:\n",
    "            params[\"sensor_processing\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "    elif \"vgg19\" in sprun.lower():\n",
    "        if is_multiview:\n",
    "            params[\"sensor_processing\"] = \"VGG19ProprioTunedSensorProcessing_multiview\"\n",
    "            params[\"num_views\"] = len(multiview_cameras)  # e.g. 2\n",
    "        else:\n",
    "            params[\"sensor_processing\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "    elif \"vit\" in sprun.lower():\n",
    "        if is_multiview:\n",
    "            params[\"sensor_processing\"] = \"MultiViewVitSensorProcessing\"\n",
    "            params[\"num_views\"] = len(multiview_cameras)  # e.g. 2\n",
    "        else:\n",
    "            params[\"sensor_processing\"] = \"VitSensorProcessing\"\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected sprun {sprun}\")\n",
    "\n",
    "    params[\"sp_experiment\"] = spexp_name\n",
    "    params[\"sp_run\"] = sprun\n",
    "\n",
    "    exprun = generate_vp_train(\n",
    "        exprun_path=exprun_path, result_path=result_path,\n",
    "        params=params, exp_name=exp_name, run_name=run_name\n",
    "    )\n",
    "    expruns.append(exprun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d27c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_conv_vae(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the conv-vae sensorprocessing with the right training data and parameters. Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    NOTE: a similar function is in Flow_BehaviorCloning.\n",
    "    \"\"\"\n",
    "    val = {}\n",
    "    val[\"latent_size\"] = params[\"latent_size\"]\n",
    "    val[\"epochs\"] = params[\"epochs\"]\n",
    "    val[\"save_period\"] = 5\n",
    "    val[\"training_data\"] = params[\"training_data\"]\n",
    "    val[\"validation_data\"] = params[\"validation_data\"]\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_Conv-VAE\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_Conv_VAE.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d97895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_propriotuned_cnn(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned CNN with the right training data and parameters.\n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_CNN\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_CNN.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241edda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_propriotuned_vit(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned ViT with the right training data and parameters.\n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "\n",
    "    # val[\"robot_exp\"] = \"robot_al5d\"\n",
    "    # val[\"robot_run\"] = \"robot_al5d\"\n",
    "    # val[\"proprioception_mlp_model_file\"] = \"proprioception_mlp.pth\"\n",
    "\n",
    "    # ViT-specific additions\n",
    "    # val[\"robot_exp\"] = \"robot_al5d\"\n",
    "    # val[\"robot_run\"] = \"robot_al5d\"\n",
    "    # val[\"proprioception_mlp_model_file\"] = \"proprioception_mlp.pth\"\n",
    "\n",
    "    # ViT architecture parameters based on model type\n",
    "    if params[\"vit_model\"] == \"vit_base\":\n",
    "        val[\"vit_model\"] = \"vit_b_16\"\n",
    "        val[\"vit_output_dim\"] = 768\n",
    "        val[\"projection_hidden_dim\"] = 512\n",
    "    elif params[\"vit_model\"] == \"vit_large\":\n",
    "        val[\"vit_model\"] = \"vit_l_16\"\n",
    "        val[\"vit_output_dim\"] = 1024\n",
    "        val[\"projection_hidden_dim\"] = 768\n",
    "\n",
    "    val[\"vit_weights\"] = \"DEFAULT\"\n",
    "    val[\"proprio_step_1\"] = 64\n",
    "    val[\"proprio_step_2\"] = 64\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_ViT\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_VIT.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee22fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_propriotuned_vit_multiview(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training of the propriotuned multiview ViT.\"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"output_size\"] = 6\n",
    "    val[\"batch_size\"] = 32\n",
    "\n",
    "    # Multiview-specific\n",
    "    val[\"num_views\"] = params.get(\"num_views\", 2)  # âœ… Default to 2\n",
    "\n",
    "    # ViT-specific additions\n",
    "    val[\"robot_exp\"] = \"robot_al5d\"\n",
    "    val[\"robot_run\"] = \"position_controller_00\"\n",
    "    val[\"proprioception_mlp_model_file\"] = \"proprioception_mlp.pth\"\n",
    "\n",
    "    # ViT architecture parameters based on model type\n",
    "    if params[\"vit_model\"] == \"vit_base\":\n",
    "        val[\"vit_model\"] = \"vit_b_16\"\n",
    "        val[\"vit_output_dim\"] = 768\n",
    "        val[\"projection_hidden_dim\"] = 512\n",
    "    elif params[\"vit_model\"] == \"vit_large\":\n",
    "        val[\"vit_model\"] = \"vit_l_16\"\n",
    "        val[\"vit_output_dim\"] = 1024\n",
    "        val[\"projection_hidden_dim\"] = 768\n",
    "\n",
    "    val[\"vit_weights\"] = \"DEFAULT\"\n",
    "    val[\"proprio_step_1\"] = 64\n",
    "    val[\"proprio_step_2\"] = 64\n",
    "\n",
    "    # Fusion method - THIS WAS THE BUG!\n",
    "    val[\"fusion_type\"] = params.get(\"fusion_type\", \"concat_proj\")  # CORRECT\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "\n",
    "    # generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = \"Train_SP_ViT_Multiview\"\n",
    "    v[\"notebook\"] = \"sensorprocessing/Train_ProprioTuned_VIT_multiview.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476839eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_train(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the training visual proprioception regressor.\n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Train_{run_name}\"\n",
    "    if \"multiview\" in params.get(\"sensor_processing\", \"\").lower():\n",
    "        v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception_multiview.ipynb\"\n",
    "    else:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Train_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99c9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_verify(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.\n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Verify_{run_name}\"\n",
    "    v[\"notebook\"] = \"TODO Verify.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f64e6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vp_compare(exprun_path, result_path, params, exp_name, run_name):\n",
    "    \"\"\"Generate the experiment for the verification of the visual proprioception regressor.\n",
    "    Returns a dictionary with the experiment, runname as well as an entry that will be used for the automation.\n",
    "    \"\"\"\n",
    "    val = copy.copy(params)\n",
    "    val[\"name\"] = exp_name\n",
    "\n",
    "    # save the generated exprun spec\n",
    "    path = pathlib.Path(Config().get_exprun_path(), exp_name, run_name + \".yaml\")\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(val, f)\n",
    "    # now, generate the entry in the automation file\n",
    "    v = {}\n",
    "    v[\"name\"] = f\"Compare_{run_name}\"\n",
    "    if do_VIT_MULTIVIEW:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb\"\n",
    "    else:\n",
    "        v[\"notebook\"] = \"visual_proprioception/Compare_VisualProprioception.ipynb\"\n",
    "    v[\"experiment\"] = exp_name\n",
    "    v[\"run\"] = run_name\n",
    "    v[\"external_path\"] = exprun_path.as_posix()\n",
    "    v[\"data_path\"] = result_path.as_posix()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cd090",
   "metadata": {},
   "source": [
    "### Generate the exp/runs to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020b8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensorprocessing_conv_vae sp_conv_vae_128_0001 128\n",
      "sensorprocessing_propriotuned_Vit sp_vit_base_128_0001 128\n",
      "sensorprocessing_propriotuned_Vit_multiview sp_vit_base_multiview_attention_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_128_0001 128\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_128_0001 128\n",
      "sensorprocessing_conv_vae sp_conv_vae_256_0001 256\n",
      "sensorprocessing_propriotuned_Vit sp_vit_base_256_0001 256\n",
      "sensorprocessing_propriotuned_Vit_multiview sp_vit_base_multiview_attention_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_vgg19_256_0001 256\n",
      "sensorprocessing_propriotuned_cnn sp_resnet50_256_0001 256\n"
     ]
    }
   ],
   "source": [
    "expruns = []\n",
    "# overall values\n",
    "latent_sizes = [128, 256] # the possible latent sizes we consider (Its pretty flexible now we can change this to any numbers)\n",
    "cnntypes = [\"vgg19\", \"resnet50\"] # the CNN architectures we consider\n",
    "vit_types = [\"vit_base\"] #FIXME using thsi for testing VIT Types, we can add vit_large and vit-huge here as well\n",
    "# vit_types = [\"vit_base\", \"vit_large\"] #VIT Types, we can add vit-huge here as well\n",
    "# fusion_methods = [\"concat_proj\", \"indiv_proj\", \"attention\", \"weighted_sum\", \"gated\"]\n",
    "fusion_types = [\"attention\"]  #FIXME using this for testing I should remeber to uncomment the top pne\n",
    "\n",
    "\n",
    "# *******************************************\n",
    "# generate the sensorprocessing models\n",
    "# *******************************************\n",
    "sps = [] # the list of the sensorprocessing models (exp/run)\n",
    "for latent_size in latent_sizes:\n",
    "\n",
    "    # generate the vae exprun\n",
    "    exp_name = \"sensorprocessing_conv_vae\"\n",
    "    run_name = f\"sp_conv_vae_{latent_size}_0001\"\n",
    "    params = {}\n",
    "    params[\"latent_size\"] = latent_size\n",
    "    params[\"epochs\"] = epochs_sp\n",
    "    params[\"training_data\"] = sp_training_data\n",
    "    params[\"validation_data\"] = sp_validation_data\n",
    "    exprun = generate_sp_conv_vae(\n",
    "        exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "    exprun[\"latent_size\"] = latent_size\n",
    "    if do_VAE:\n",
    "        sps.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "    if do_VIT:\n",
    "        for vit_type in vit_types:\n",
    "            exp_name = \"sensorprocessing_propriotuned_Vit\"\n",
    "            run_name = f\"sp_{vit_type}_{latent_size}_0001\"\n",
    "            params = {}\n",
    "            params[\"image_size\"] = [224, 224]  # ViT needs 224x224!\n",
    "            params[\"latent_size\"] = latent_size\n",
    "            params[\"epochs\"] = epochs_sp\n",
    "            params[\"training_data\"] = sp_training_data\n",
    "            params[\"validation_data\"] = sp_validation_data\n",
    "            params[\"vit_model\"] = vit_type  # Pass as \"vit_base\" or \"vit_large\"\n",
    "            params[\"class\"] = \"Vit\"\n",
    "            params[\"model\"] = \"VitProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = False\n",
    "            params[\"loss\"] = \"MSELoss\"\n",
    "            params[\"learning_rate\"] = 0.0001\n",
    "\n",
    "            exprun = generate_sp_propriotuned_vit(\n",
    "                exprun_path=exprun_path, result_path=result_path, params=params, exp_name=exp_name, run_name=run_name)\n",
    "            exprun[\"latent_size\"] = latent_size\n",
    "            exprun[\"vittype\"] = vit_type\n",
    "            sps.append(exprun)\n",
    "            expruns.append(exprun)\n",
    "\n",
    "    # Generate multiview ViT models\n",
    "    if do_VIT_MULTIVIEW:\n",
    "        for vit_type in vit_types:\n",
    "            for fusion_type in fusion_types:\n",
    "                exp_name = \"sensorprocessing_propriotuned_Vit_multiview\"\n",
    "                run_name = f\"sp_{vit_type}_multiview_{fusion_type}_{latent_size}_0001\"\n",
    "                params = {}\n",
    "                params[\"image_size\"] = [224, 224]\n",
    "                params[\"latent_size\"] = latent_size\n",
    "                params[\"epochs\"] = epochs_sp\n",
    "                params[\"num_views\"] = num_views\n",
    "                params[\"fusion_type\"] = fusion_type\n",
    "\n",
    "                # Create multiview training data with camera list\n",
    "                params[\"training_data\"] = [\n",
    "                    [demopack_name, demo, multiview_cameras]\n",
    "                    for demo in selection[\"sp_training\"]\n",
    "                ]\n",
    "                params[\"validation_data\"] = [\n",
    "                    [demopack_name, demo, multiview_cameras]\n",
    "                    for demo in selection[\"sp_validation\"]\n",
    "                ]\n",
    "\n",
    "                params[\"vit_model\"] = vit_type\n",
    "                params[\"class\"] = \"Vit_multiview\"\n",
    "                params[\"model\"] = \"VitProprioTunedRegression\"\n",
    "                params[\"freeze_feature_extractor\"] = False\n",
    "                params[\"loss\"] = \"MSELoss\"\n",
    "                params[\"learning_rate\"] = 0.0001\n",
    "\n",
    "                exprun = generate_sp_propriotuned_vit_multiview(\n",
    "                    exprun_path=exprun_path, result_path=result_path,\n",
    "                    params=params, exp_name=exp_name, run_name=run_name\n",
    "                )\n",
    "                exprun[\"latent_size\"] = latent_size\n",
    "                exprun[\"vittype\"] = vit_type\n",
    "                exprun[\"fusion\"] = fusion_type\n",
    "                sps.append(exprun)\n",
    "                expruns.append(exprun)\n",
    "    # generate the propriotuned expruns\n",
    "    for cnntype in cnntypes:\n",
    "        exp_name = \"sensorprocessing_propriotuned_cnn\"\n",
    "        run_name = f\"sp_{cnntype}_{latent_size}_0001\"\n",
    "        params = {}\n",
    "        params[\"image_size\"] = image_size\n",
    "        params[\"latent_size\"] = latent_size\n",
    "        params[\"epochs\"] = epochs_sp\n",
    "        params[\"training_data\"] = sp_training_data\n",
    "        params[\"validation_data\"] = sp_validation_data\n",
    "        if cnntype == \"vgg19\":\n",
    "            if not do_VGG:\n",
    "                continue\n",
    "            params[\"class\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"VGG19ProprioTunedRegression\"\n",
    "        elif cnntype == \"resnet50\":\n",
    "            if not do_RESNET:\n",
    "                continue\n",
    "            params[\"class\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "            params[\"model\"] = \"ResNetProprioTunedRegression\"\n",
    "            params[\"freeze_feature_extractor\"] = True\n",
    "            params[\"reductor_step_1\"] = 512\n",
    "            params[\"proprio_step_1\"] = 64\n",
    "            params[\"proprio_step_2\"] = 16\n",
    "        else:\n",
    "            raise Exception(f\"Unknown cnntype {cnntype}\")\n",
    "        params[\"loss\"] = \"MSELoss\" # alternative L1Loss\n",
    "        params[\"learning_rate\"] = 0.001\n",
    "        # alternative\n",
    "        exprun = generate_sp_propriotuned_cnn(\n",
    "            exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name = run_name)\n",
    "        exprun[\"latent_size\"] = latent_size\n",
    "        exprun[\"cnntype\"] = cnntype\n",
    "        sps.append(exprun)\n",
    "        expruns.append(exprun)\n",
    "\n",
    "    # FIXME: add here the ViT models\n",
    "\n",
    "# *******************************************\n",
    "# generate the proprioception models\n",
    "# *******************************************\n",
    "vpruns = []\n",
    "vpruns_latent = {128:[], 256:[]}\n",
    "for spexp, sprun,latent_size in [(a[\"experiment\"],a[\"run\"],a[\"latent_size\"]) for a in sps]:\n",
    "    print(spexp, sprun, latent_size)\n",
    "    # *** generate the vp train expruns ***\n",
    "    exp_name = \"visual_proprioception\"\n",
    "    run_name = \"vp_\" + sprun[3:]\n",
    "    vpruns.append(run_name)\n",
    "    vpruns_latent[latent_size].append(run_name)\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"output_size\"] = 6\n",
    "    params[\"encoding_size\"] = latent_size\n",
    "    # NEW: Use multiview data for multiview runs\n",
    "    if \"multiview\" in sprun.lower():\n",
    "        params[\"training_data\"] = vp_training_data_multiview\n",
    "        params[\"validation_data\"] = vp_validation_data_multiview\n",
    "    else:\n",
    "        params[\"training_data\"] = vp_training_data\n",
    "        params[\"validation_data\"] = vp_validation_data\n",
    "\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"regressor_hidden_size_1\"] = 64\n",
    "    params[\"loss\"] = \"MSE\"\n",
    "    params[\"epochs\"] = epochs_vp\n",
    "    params[\"batch_size\"] = 64\n",
    "    # FIXME this is hackish, should not do it this way\n",
    "    if \"vae\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ConvVaeSensorProcessing\"\n",
    "    elif \"resnet\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"ResNetProprioTunedSensorProcessing\"\n",
    "    elif \"vgg19\" in sprun.lower():\n",
    "        params[\"sensor_processing\"] = \"VGG19ProprioTunedSensorProcessing\"\n",
    "    elif \"vit\" in sprun.lower():\n",
    "        if \"multiview\" in sprun.lower():\n",
    "            params[\"sensor_processing\"] = \"MultiViewVitSensorProcessing\"\n",
    "            params[\"num_views\"] = len(multiview_cameras)  # e.g. 2\n",
    "            # Added for VIT\n",
    "        else:\n",
    "            params[\"sensor_processing\"] = \"VitSensorProcessing\"\n",
    "    # elif \"vit\" in sprun.lower():  # Added for VIT\n",
    "    #     params[\"sensor_processing\"] = \"VitSensorProcessing\"\n",
    "    # elif \"multiview\" in sprun.lower():  # Aded for multi-VIT\n",
    "    #     params[\"sensor_processing\"] = \"MultiViewVitSensorProcessing\"\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected sprun {sprun}\")\n",
    "\n",
    "    params[\"sp_experiment\"] = spexp\n",
    "    params[\"sp_run\"] = sprun\n",
    "\n",
    "    exprun = generate_vp_train(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "    # *** generate the vp verify expruns FIXME: not implemented yet ***\n",
    "    params_verify = {}\n",
    "\n",
    "    expruns.append(exprun)\n",
    "# FIXME: Lotzi: this seems to be duplicated by the next cell???\n",
    "# *******************************************\n",
    "# generate the comparisons: all, for latents 128 and 256\n",
    "# *******************************************\n",
    "# exp_name = \"visual_proprioception\"\n",
    "# # all\n",
    "# run_name = \"vp_comp_flow_all\"\n",
    "# params = {}\n",
    "# params[\"name\"] = run_name\n",
    "# params[\"tocompare\"] = vpruns\n",
    "# exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "# expruns.append(exprun)\n",
    "# # by latent\n",
    "# for latent_size in [128, 256]:\n",
    "#     run_name = f\"vp_comp_flow_{latent_size}\"\n",
    "#     params = {}\n",
    "#     params[\"name\"] = run_name\n",
    "#     params[\"tocompare\"] = vpruns_latent[latent_size]\n",
    "#     exprun = generate_vp_compare(exprun_path = exprun_path, result_path = result_path, params = params, exp_name = exp_name, run_name=run_name)\n",
    "#     expruns.append(exprun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7183f4",
   "metadata": {},
   "source": [
    "### Generate COMPARISON experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# Generate COMPARISON experiments\n",
    "# -----------------------------------------\n",
    "exp_name = \"visual_proprioception\"\n",
    "\n",
    "# Compare ALL models\n",
    "# run_name = \"vp_comp_flow_all\"\n",
    "# I think this created a problem, because the same name was checked in\n",
    "run_name = \"flow_vp_comp_all\"\n",
    "params = {}\n",
    "params[\"name\"] = run_name\n",
    "params[\"tocompare\"] = vpruns\n",
    "params[\"robot_exp\"] = \"robot_al5d\"\n",
    "params[\"robot_run\"] = \"position_controller_00\"\n",
    "exprun = generate_vp_compare(\n",
    "    exprun_path=exprun_path, result_path=result_path,\n",
    "    params=params, exp_name=exp_name, run_name=run_name\n",
    ")\n",
    "expruns.append(exprun)\n",
    "\n",
    "\n",
    "\n",
    "# Compare by latent size\n",
    "for latent_size in latent_sizes:\n",
    "    run_name = f\"vp_comp_flow_{latent_size}\"\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"tocompare\"] = vpruns_latent[latent_size]\n",
    "    params[\"robot_exp\"] = \"robot_al5d\"\n",
    "    params[\"robot_run\"] = \"position_controller_00\"\n",
    "    exprun = generate_vp_compare(\n",
    "        exprun_path=exprun_path, result_path=result_path,\n",
    "        params=params, exp_name=exp_name, run_name=run_name\n",
    "    )\n",
    "    expruns.append(exprun)\n",
    "\n",
    "# Compare single-view only\n",
    "if vpruns_singleview:\n",
    "    run_name = \"vp_comp_flow_singleview\"\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"tocompare\"] = vpruns_singleview\n",
    "    params[\"robot_exp\"] = \"robot_al5d\"\n",
    "    params[\"robot_run\"] = \"position_controller_00\"\n",
    "    exprun = generate_vp_compare(\n",
    "        exprun_path=exprun_path, result_path=result_path,\n",
    "        params=params, exp_name=exp_name, run_name=run_name\n",
    "    )\n",
    "    expruns.append(exprun)\n",
    "\n",
    "# Compare multi-view only\n",
    "if vpruns_multiview:\n",
    "    run_name = \"vp_comp_flow_multiview\"\n",
    "    params = {}\n",
    "    params[\"name\"] = run_name\n",
    "    params[\"tocompare\"] = vpruns_multiview\n",
    "    params[\"robot_exp\"] = \"robot_al5d\"\n",
    "    params[\"robot_run\"] = \"position_controller_00\"\n",
    "    exprun = generate_vp_compare(\n",
    "        exprun_path=exprun_path, result_path=result_path,\n",
    "        params=params, exp_name=exp_name, run_name=run_name\n",
    "    )\n",
    "    expruns.append(exprun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795e0c0",
   "metadata": {},
   "source": [
    "### RUN THE FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e83c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Starting automated running of the flow.\n",
      " The path for the output notebooks is\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\VisualProprioception_flow_multiview_70\\result\n",
      "Total experiments to run: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_128_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20d0d66ce39495c8177ebe399dca29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/11 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1/25 [00:03<01:19,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_VIT.ipynb :\n",
      " sensorprocessing_propriotuned_Vit/sp_vit_base_128_0001\n",
      "--> Train_ProprioTuned_VIT_sensorprocessing_propriotuned_Vit_sp_vit_base_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8461a8f4a54e4bb698d62cb0f2fc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/23 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 2/25 [00:08<01:36,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an exception \n",
      "---------------------------------------------------------------------------\n",
      "Exception encountered at \"In [14]\":\n",
      "---------------------------------------------------------------------------\n",
      "NameError                                 Traceback (most recent call last)\n",
      "Cell In[14], line 32\n",
      "     29         print()\n",
      "     31 # Test the sensor processing\n",
      "---> 32 test_sensor_processing(sp, inputs_validation, targets_validation)\n",
      "\n",
      "NameError: name 'inputs_validation' is not defined\n",
      "\n",
      "***Automating sensorprocessing/Train_ProprioTuned_VIT_multiview.ipynb :\n",
      " sensorprocessing_propriotuned_Vit_multiview/sp_vit_base_multiview_attention_128_0001\n",
      "--> Train_ProprioTuned_VIT_multiview_sensorprocessing_propriotuned_Vit_multiview_sp_vit_base_multiview_attention_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eca0f0a4dac4dbb8f9556f70dafd97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 3/25 [00:15<02:01,  5.54s/it]WARNING:papermill:Passed unknown parameter: experiment\n",
      "WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_128_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a5e0edaf2848e8834ad34296063bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/13 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 4/25 [00:18<01:33,  4.45s/it]WARNING:papermill:Passed unknown parameter: experiment\n",
      "WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_128_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20f8477ff4e4228939ace89d620bf7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/13 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 5/25 [00:20<01:16,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_Conv_VAE.ipynb :\n",
      " sensorprocessing_conv_vae/sp_conv_vae_256_0001\n",
      "--> Train_Conv_VAE_sensorprocessing_conv_vae_sp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33008cd5a95e49d7b3b8e270c78c0966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/11 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 6/25 [00:23<01:08,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_VIT.ipynb :\n",
      " sensorprocessing_propriotuned_Vit/sp_vit_base_256_0001\n",
      "--> Train_ProprioTuned_VIT_sensorprocessing_propriotuned_Vit_sp_vit_base_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53952051ec3047a88cef45f60f1d69b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/23 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:28<01:09,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an exception \n",
      "---------------------------------------------------------------------------\n",
      "Exception encountered at \"In [14]\":\n",
      "---------------------------------------------------------------------------\n",
      "NameError                                 Traceback (most recent call last)\n",
      "Cell In[14], line 32\n",
      "     29         print()\n",
      "     31 # Test the sensor processing\n",
      "---> 32 test_sensor_processing(sp, inputs_validation, targets_validation)\n",
      "\n",
      "NameError: name 'inputs_validation' is not defined\n",
      "\n",
      "***Automating sensorprocessing/Train_ProprioTuned_VIT_multiview.ipynb :\n",
      " sensorprocessing_propriotuned_Vit_multiview/sp_vit_base_multiview_attention_256_0001\n",
      "--> Train_ProprioTuned_VIT_multiview_sensorprocessing_propriotuned_Vit_multiview_sp_vit_base_multiview_attention_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9517584c37e44128438e36a3cd13830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:35<01:23,  4.90s/it]WARNING:papermill:Passed unknown parameter: experiment\n",
      "WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_vgg19_256_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64541a51bf574a28bb265b49124eadf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/13 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:38<01:07,  4.22s/it]WARNING:papermill:Passed unknown parameter: experiment\n",
      "WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating sensorprocessing/Train_ProprioTuned_CNN.ipynb :\n",
      " sensorprocessing_propriotuned_cnn/sp_resnet50_256_0001\n",
      "--> Train_ProprioTuned_CNN_sensorprocessing_propriotuned_cnn_sp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b57928e99e04c73966a0b50adef25cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/13 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:40<00:56,  3.75s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450b5c90a51742a8970159813b524c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:44<00:51,  3.71s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vit_base_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vit_base_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe5e56349ef457b82222760c94542f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:48<00:49,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception_multiview.ipynb :\n",
      " visual_proprioception/vp_vit_base_multiview_attention_128_0001\n",
      "--> Train_VisualProprioception_multiview_visual_proprioception_vp_vit_base_multiview_attention_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051060e39e7482eaae633ee8ae0a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:53<00:48,  4.05s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50de44dd3bd141ddaa30cb36b73b8bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:57<00:44,  4.07s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_128_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_128_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25541c37f34240bab8267b424ea28799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [01:01<00:39,  3.99s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_conv_vae_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_conv_vae_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe548739ccd4a3396d98fe7d42171c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [01:04<00:34,  3.88s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vit_base_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vit_base_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0be73d84fa04a538c12666b0c41b0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [01:08<00:31,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception_multiview.ipynb :\n",
      " visual_proprioception/vp_vit_base_multiview_attention_256_0001\n",
      "--> Train_VisualProprioception_multiview_visual_proprioception_vp_vit_base_multiview_attention_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b037d2984d8249dd993fbdd0c1e88d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [01:13<00:28,  4.13s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_vgg19_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_vgg19_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474f6af03b91423c87b02867d26d3453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [01:17<00:24,  4.12s/it]WARNING:papermill:Passed unknown parameter: run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Train_VisualProprioception.ipynb :\n",
      " visual_proprioception/vp_resnet50_256_0001\n",
      "--> Train_VisualProprioception_visual_proprioception_vp_resnet50_256_0001_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c896a6311c4c69b283acaa94ad1799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/17 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [01:21<00:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb :\n",
      " visual_proprioception/vp_comp_flow_all\n",
      "--> Compare_VisualProprioception_multiview_and_singleview_visual_proprioception_vp_comp_flow_all_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6db06dd0e384ace9de0da49bd714ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/14 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [01:24<00:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an exception \n",
      "---------------------------------------------------------------------------\n",
      "Exception encountered at \"In [8]\":\n",
      "---------------------------------------------------------------------------\n",
      "FileNotFoundError                         Traceback (most recent call last)\n",
      "Cell In[8], line 18\n",
      "     15 spexps.append(spexp)\n",
      "     17 # Load and ensure sensor processor is on the right device\n",
      "---> 18 sp = sp_factory.create_sp(spexp, device)\n",
      "     19 if hasattr(sp, 'enc') and hasattr(sp.enc, 'to'):\n",
      "     20     sp.enc = force_to_device(sp.enc, device)\n",
      "\n",
      "File c:\\Users\\lotzi\\Work\\_Code\\BerryPicker\\src\\visual_proprioception\\..\\sensorprocessing\\sp_factory.py:83, in create_sp(spexp, device)\n",
      "     78 # =========================================================================\n",
      "     79 # CONV-VAE MODELS\n",
      "     80 # =========================================================================\n",
      "     82 if sp_class == \"ConvVaeSensorProcessing\":\n",
      "---> 83     return sp_conv_vae.ConvVaeSensorProcessing(spexp, device)\n",
      "     85 if sp_class == \"ConvVaeSensorProcessing_concat_multiview\":\n",
      "     86     return sp_conv_vae_concat_multiview.ConcatConvVaeSensorProcessing(spexp, device)\n",
      "\n",
      "File c:\\Users\\lotzi\\Work\\_Code\\BerryPicker\\src\\visual_proprioception\\..\\sensorprocessing\\sp_conv_vae.py:63, in ConvVaeSensorProcessing.__init__(self, exp, device)\n",
      "     60 self.resume_model_pthfile = Path(exp.data_dir(), \"model.pth\")\n",
      "     61 # self.conv_vae_jsonfile = conv_vae_jsonfile\n",
      "     62 # self.resume_model_pthfile = resume_model_pthfile\n",
      "---> 63 self.vae_config = get_conv_vae_config(\n",
      "     64     self.conv_vae_jsonfile, \n",
      "     65     self.resume_model_pthfile, \n",
      "     66     inference_only=True)\n",
      "     67 # build model architecture\n",
      "     68 self.model = self.vae_config.init_obj('arch', module_arch)\n",
      "\n",
      "File c:\\Users\\lotzi\\Work\\_Code\\BerryPicker\\src\\visual_proprioception\\..\\sensorprocessing\\conv_vae.py:217, in get_conv_vae_config(jsonfile, resume_model, inference_only)\n",
      "    215 savedargv = sys.argv\n",
      "    216 sys.argv = value\n",
      "--> 217 config = ConfigParser.from_args(args)\n",
      "    218 sys.argv = savedargv\n",
      "    219 # print(json.dumps(config.config, indent=4))\n",
      "    220 #\n",
      "    221 # THIS was an attempt to fix some kind of weird bug where an empty \n",
      "    222 # directory was created... it is not needed on 2024.11.17???\n",
      "    223 # if it is inference only, remove the superfluously created directories.\n",
      "    224 #\n",
      "\n",
      "File c:\\Users\\lotzi\\Work\\_Code\\Conv-VAE-PyTorch\\parse_config.py:71, in ConfigParser.from_args(cls, args, options)\n",
      "     68     resume = None\n",
      "     69     cfg_fname = Path(args.config)\n",
      "---> 71 config = read_json(cfg_fname)\n",
      "     72 if args.config and resume:\n",
      "     73     # update new config for fine-tuning\n",
      "     74     config.update(read_json(args.config))\n",
      "\n",
      "File c:\\Users\\lotzi\\Work\\_Code\\Conv-VAE-PyTorch\\utils\\util.py:16, in read_json(fname)\n",
      "     14 def read_json(fname):\n",
      "     15     fname = Path(fname)\n",
      "---> 16     with fname.open('rt') as handle:\n",
      "     17         return json.load(handle, object_hook=OrderedDict)\n",
      "\n",
      "File C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\pathlib\\_local.py:537, in Path.open(self, mode, buffering, encoding, errors, newline)\n",
      "    535 if \"b\" not in mode:\n",
      "    536     encoding = io.text_encoding(encoding)\n",
      "--> 537 return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\Users\\\\lotzi\\\\Work\\\\_Data\\\\BerryPicker-Flows\\\\VisualProprioception_flow_multiview_70\\\\result\\\\sensorprocessing_conv_vae\\\\sp_vae_128\\\\config.json'\n",
      "\n",
      "***Automating visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb :\n",
      " visual_proprioception/vp_comp_flow_128\n",
      "--> Compare_VisualProprioception_multiview_and_singleview_visual_proprioception_vp_comp_flow_128_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cc23433e404ccf8fbf15923e3dc68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/14 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [01:32<00:14,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb :\n",
      " visual_proprioception/vp_comp_flow_256\n",
      "--> Compare_VisualProprioception_multiview_and_singleview_visual_proprioception_vp_comp_flow_256_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f164a36ae52341378951efb54f28733c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/14 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [01:39<00:11,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb :\n",
      " visual_proprioception/vp_comp_flow_singleview\n",
      "--> Compare_VisualProprioception_multiview_and_singleview_visual_proprioception_vp_comp_flow_singleview_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b0756220c94edb99aa78dfe53c3d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/14 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [01:47<00:06,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Automating visual_proprioception/Compare_VisualProprioception_multiview_and_singleview.ipynb :\n",
      " visual_proprioception/vp_comp_flow_multiview\n",
      "--> Compare_VisualProprioception_multiview_and_singleview_visual_proprioception_vp_comp_flow_multiview_output.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0242637cb1043f4b87976a77b3b6910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/14 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:54<00:00,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# RUN THE FLOW\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"***Starting automated running of the flow.\\n The path for the output notebooks is\\n{result_path}\")\n",
    "print(f\"Total experiments to run: {len(expruns)}\")\n",
    "\n",
    "for exprun in tqdm.tqdm(expruns):\n",
    "    print(f\"***Automating {exprun['notebook']} :\\n {exprun['experiment']}/{exprun['run']}\")\n",
    "    notebook_path = pathlib.Path(\"..\", exprun[\"notebook\"])\n",
    "    output_filename = f\"{notebook_path.stem}_{exprun['experiment']}_{exprun['run']}_output{notebook_path.suffix}\"\n",
    "    print(f\"--> {output_filename}\")\n",
    "\n",
    "    # Parameters to pass to the notebook\n",
    "    params = {}\n",
    "    params[\"experiment\"] = exprun[\"experiment\"]\n",
    "    params[\"run\"] = exprun[\"run\"]\n",
    "    params[\"external_path\"] = exprun[\"external_path\"]\n",
    "    params[\"data_path\"] = exprun[\"data_path\"]\n",
    "    output_path = pathlib.Path(result_path, output_filename)\n",
    "\n",
    "    try:\n",
    "        papermill.execute_notebook(\n",
    "            notebook_path,\n",
    "            output_path.absolute(),\n",
    "            cwd=notebook_path.parent,\n",
    "            parameters=params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"There was an exception {e}\")\n",
    "\n",
    "print(\"Flow completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
