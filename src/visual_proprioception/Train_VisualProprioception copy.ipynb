{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/pathlib.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/sa641631/.config/BerryPicker/mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# from demonstration.demonstration import Demonstration\n",
    "\n",
    "\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, load_multiview_demonstrations_as_proprioception_training\n",
    "\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Code for deterministic run, from Robi Konievic\n",
    "#\n",
    "superpower=777\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(superpower)\n",
    "import random\n",
    "random.seed(superpower)\n",
    "import numpy as np\n",
    "np.random.seed(superpower)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(superpower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. \n",
    "Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values\n",
    "# *** This cell should be tagged as parameters\n",
    "# *** If papermill is used, some of the values will be overwritten\n",
    "\n",
    "# If it is set to discard-old, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# If not None, set the epochs to something different than the exp\n",
    "epochs = None\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "# Dr. Boloni's path\n",
    "#external_path = pathlib.Path(Config()[\"experiment_external\"])\n",
    "# Sahara's path\n",
    "# external_path = pathlib.Path(\"/home/sa641631/SaharaBerryPickerData/experiment_data\")\n",
    "\n",
    "##############################################\n",
    "#                 SingleView                 #\n",
    "##############################################\n",
    "\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "#vits\n",
    "run =\"vit_base_256_002\"\n",
    "# run =\"vit_large\"\n",
    "# run =\"vit_huge\"\n",
    "\n",
    "##############################################\n",
    "#                 MultiViews                 #\n",
    "##############################################\n",
    "\n",
    "#concat_proj\n",
    "\n",
    "# run =\"vit_base_multiview\"\n",
    "# run =\"vit_large_multiview\"\n",
    "# run =vit_huge_multiview\n",
    "\n",
    "\n",
    "##  indiv_proj\n",
    "# run = \"vit_base_multiview_indiv_proj\"  # ViT Base_indiv_proj\n",
    "# run = \"vit_large_multiview_indiv_proj\" # ViT Large_indiv_proj\n",
    "# run = \"vit_huge_multiview_indiv_proj\" # ViT Huge_indiv_proj\n",
    "\n",
    "##  attention\n",
    "# run = \"vit_base_multiview_attention\"  # ViT Base_attention\n",
    "# run = \"vit_large_multiview_attention\" # ViT Large_attention\n",
    "# run = \"vit_huge_multiview_attention\" # ViT Huge_attention\n",
    "\n",
    "\n",
    "##  weighted_sum\n",
    "# run = \"vit_base_multiview_weighted_sum\"  # ViT Base_weighted_sum\n",
    "# run = \"vit_large_multiview_weighted_sum\" # ViT Large_weighted_sum\n",
    "# run = \"vit_huge_multiview_weighted_sum\" # ViT Huge_weighted_sum\n",
    "\n",
    "##  gated\n",
    "# run = \"vit_base_multiview_gated\"  # ViT Base_gated\n",
    "# run = \"vit_large_multiview_gated\" # ViT Large_gated\n",
    "# run = \"vit_huge_multiview_gated\" # ViT Huge_gated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Missing experiment file\n /lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/experiment_configs/visual_proprioception/vit_base_002.yaml",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m data_path.exists()\n\u001b[32m     15\u001b[39m     Config().set_results_path(data_path)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m exp = \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreation_style\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreation_style\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m pprint(exp)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create the sp object described in the experiment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/visual_proprioception/../exp_run_config.py:302\u001b[39m, in \u001b[36mConfig.get_experiment\u001b[39m\u001b[34m(self, experiment_name, run_name, subrun_name, creation_style)\u001b[39m\n\u001b[32m    300\u001b[39m experiment_run_indep = pathlib.Path(\u001b[38;5;28mself\u001b[39m.experiment_path, experiment_name, run_name + \u001b[33m\"\u001b[39m\u001b[33m.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_run_indep.exists():\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing experiment file\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_run_indep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m experiment_run_indep.open(\u001b[33m\"\u001b[39m\u001b[33mrt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m    304\u001b[39m     indep_config = yaml.safe_load(handle)\n",
      "\u001b[31mException\u001b[39m: Missing experiment file\n /lustre/fs1/home/sa641631/WORK/BerryPicker/src/BerryPicker/src/experiment_configs/visual_proprioception/vit_base_002.yaml"
     ]
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_aruco\")\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "    Config().copy_experiment(\"visual_proprioception\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n",
    "exp_robot = Config().get_experiment(exp[\"robot_exp\"], exp[\"robot_run\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "model.to(device)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spexp image_size: [224, 224], type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"spexp image_size: {spexp['image_size']}, type: {type(spexp['image_size'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and validation data which maps latent encodings into robot position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 224]\n",
      "***ExpRun**: Configuration for exp/run: demonstration/random-both-cameras-video successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***load_demonstrations_as_proprioception_training*** \n",
      "\tSuccessfully recalculated the proprioception training and saved it to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/train_inputs.pt etc\n",
      "***Timer*** load-demos-as-proprioception-training started\n",
      "[224, 224]\n",
      "***ExpRun**: Configuration for exp/run: demonstration/random-both-cameras-video successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: demonstration/random-both-cameras-video successfully loaded\n",
      "***load_demonstrations_as_proprioception_training*** \n",
      "\tSuccessfully recalculated the proprioception training and saved it to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/test_inputs.pt etc\n",
      "***Timer*** load-demos-as-proprioception-training finished in 17.590554 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use the original loading function\n",
    "\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"training_data\", proprioception_input_file, proprioception_target_file, device=device\n",
    ")\n",
    "\n",
    "inputs_training = tr[\"inputs\"]\n",
    "targets_training = tr[\"targets\"]\n",
    "\n",
    "proprioception_test_input_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_input_file\"])\n",
    "proprioception_test_target_file = pathlib.Path(\n",
    "    exp.data_dir(), exp[\"proprioception_test_target_file\"])\n",
    "\n",
    "\n",
    "exp.start_timer(\"load-demos-as-proprioception-training\")\n",
    "tr_test = load_demonstrations_as_proprioception_training(\n",
    "    sp, exp, spexp, exp_robot, \"validation_data\", proprioception_test_input_file, proprioception_test_target_file, device=device\n",
    ")\n",
    "exp.end_timer(\"load-demos-as-proprioception-training\")\n",
    "\n",
    "inputs_validation = tr_test[\"inputs\"]\n",
    "targets_validation = tr_test[\"targets\"]\n",
    "\n",
    "# Create standard DataLoaders for single-view data\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "# batch_size = exp['batch_size']\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model, handling both single and multi-view inputs\n",
    "    with checkpoint support for resuming interrupted training\n",
    "    \"\"\"\n",
    "    final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "    checkpoint_dir = pathlib.Path(exp[\"data_dir\"], \"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Maximum number of checkpoints to keep (excluding the best model)\n",
    "    max_checkpoints = 2\n",
    "\n",
    "    # Check if we're using a multi-view approach\n",
    "    is_multiview = exp.get(\"sensor_processing\", \"\").endswith(\"_multiview\") or exp.get(\"num_views\", 1) > 1\n",
    "    num_views = exp.get(\"num_views\", 2)\n",
    "\n",
    "    # First check for existing final model\n",
    "    if final_modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "        print(f\"Loading existing final model from {final_modelfile}\")\n",
    "        model.load_state_dict(torch.load(final_modelfile, map_location=device))\n",
    "\n",
    "        # Evaluate the loaded model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch for evaluation\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(batch_count, 1)\n",
    "            print(f\"Loaded model evaluation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Function to extract epoch number from checkpoint file\n",
    "    def get_epoch_number(checkpoint_file):\n",
    "        try:\n",
    "            # Use a more robust approach to extract epoch number\n",
    "            # Format: epoch_XXXX.pth where XXXX is the epoch number\n",
    "            filename = checkpoint_file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return int(parts[1])  # Get the number after \"epoch_\"\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # Function to clean up old checkpoints\n",
    "    def cleanup_old_checkpoints():\n",
    "        # Get all epoch checkpoint files\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "\n",
    "        # Sort by actual epoch number, not just filename\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        if len(checkpoint_files) > max_checkpoints:\n",
    "            files_to_delete = checkpoint_files[:-max_checkpoints]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                    print(f\"Deleted old checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path.name}: {e}\")\n",
    "\n",
    "    # Make sure model is on the correct device\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "\n",
    "    # Set training parameters\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"epoch_*.pth\"))\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number for more reliable ordering\n",
    "        checkpoint_files.sort(key=get_epoch_number)\n",
    "\n",
    "        # Get the most recent checkpoint\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        epoch_num = get_epoch_number(latest_checkpoint)\n",
    "\n",
    "        print(f\"Found checkpoint from epoch {epoch_num}. Resuming training...\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}/{num_epochs} with best loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Starting new training for {num_epochs} epochs\")\n",
    "\n",
    "    # Start or resume training\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training loop handles both single and multi-view cases\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            try:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # With multi-view, batch_views is a list of tensors, each with shape [batch_size, C, H, W]\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch_size):\n",
    "                        # Extract this sample's views\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "\n",
    "                        # Process this sample through sp\n",
    "                        sample_features = sp.process(sample_views)\n",
    "\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    # Stack all samples' features into a batch\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    # Move to device\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # Standard single-view processing\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print progress every few batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                # Save emergency checkpoint in case of error - use formatted epoch and batch numbers\n",
    "                save_path = checkpoint_dir / f\"emergency_epoch_{epoch:06d}_batch_{batch_idx:06d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss / max(batch_count, 1),\n",
    "                    'best_loss': best_loss\n",
    "                }, save_path)\n",
    "                print(f\"Emergency checkpoint saved to {save_path}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        eval_batch_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if is_multiview:\n",
    "                    batch_views, batch_y = batch_data\n",
    "\n",
    "                    # Process the batch the same way as in training\n",
    "                    batch_size = batch_views[0].size(0)\n",
    "                    batch_features = []\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        sample_views = [view[i].unsqueeze(0).to(device) for view in batch_views]\n",
    "                        sample_features = sp.process(sample_views)\n",
    "                        # Convert numpy array to tensor and move to device\n",
    "                        sample_features_tensor = torch.tensor(sample_features, dtype=torch.float32).to(device)\n",
    "                        batch_features.append(sample_features_tensor)\n",
    "\n",
    "                    batch_X = torch.stack(batch_features).to(device)\n",
    "                    predictions = model(batch_X)\n",
    "                else:\n",
    "                    batch_X, batch_y = batch_data\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    predictions = model(batch_X)\n",
    "\n",
    "                # Make sure batch_y is on the same device\n",
    "                batch_y = batch_y.to(device)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                eval_batch_count += 1\n",
    "\n",
    "        avg_test_loss = test_loss / max(eval_batch_count, 1)\n",
    "        print(f'Validation Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint after each epoch - using formatted epoch numbers for reliable sorting\n",
    "        checkpoint_path = checkpoint_dir / f\"epoch_{epoch:06d}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'best_loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clean up old checkpoints to save space\n",
    "        cleanup_old_checkpoints()\n",
    "\n",
    "        # Update best model if improved\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'best_loss': best_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Training completed successfully\n",
    "    print(f\"Training complete. Best test loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load the best model for final save\n",
    "    best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "    if best_model_path.exists():\n",
    "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']+1} with test loss {best_checkpoint['test_loss']:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), final_modelfile)\n",
    "    print(f\"Final model saved to {final_modelfile}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Timer*** train started\n",
      "Model moved to cuda\n",
      "Starting new training for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:04<00:08,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/49, Loss: 0.3278\n",
      "  Batch 20/49, Loss: 0.1766\n",
      "  Batch 30/49, Loss: 0.1382\n",
      "  Batch 40/49, Loss: 0.0902\n",
      "Epoch [1/3], Loss: 0.1563\n",
      "Validation Loss: 0.0845\n",
      "Checkpoint saved to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/checkpoints/epoch_000000.pth\n",
      "New best model saved with test loss: 0.0845\n",
      "Starting epoch 2/3\n",
      "  Batch 10/49, Loss: 0.0482\n",
      "  Batch 20/49, Loss: 0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:04<00:01,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30/49, Loss: 0.0484\n",
      "  Batch 40/49, Loss: 0.0566\n",
      "Epoch [2/3], Loss: 0.0563\n",
      "Validation Loss: 0.0783\n",
      "Checkpoint saved to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/checkpoints/epoch_000001.pth\n",
      "New best model saved with test loss: 0.0783\n",
      "Starting epoch 3/3\n",
      "  Batch 10/49, Loss: 0.0537\n",
      "  Batch 20/49, Loss: 0.0515\n",
      "  Batch 30/49, Loss: 0.0547\n",
      "  Batch 40/49, Loss: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Loss: 0.0525\n",
      "Validation Loss: 0.0797\n",
      "Checkpoint saved to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/checkpoints/epoch_000002.pth\n",
      "Deleted old checkpoint: epoch_000000.pth\n",
      "Training complete. Best test loss: 0.0783\n",
      "Loaded best model from epoch 2 with test loss 0.0783\n",
      "Final model saved to /home/sa641631/WORK/BerryPicker/data/visual_proprioception/vit_base/proprioception_mlp.pth\n",
      "***Timer*** train finished in 4.610264 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "exp.start_timer(\"train\")\n",
    "train_and_save_proprioception_model(exp)\n",
    "exp.end_timer(\"train\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-11-17 22:09:29.752681'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp[\"timer-load-demos-as-proprioception-training-end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
