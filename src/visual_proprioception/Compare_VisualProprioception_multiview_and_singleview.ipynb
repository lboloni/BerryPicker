{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models for visual proprioception\n",
    "\n",
    "Compares regression models for visual proprioception, by running them on specific test data, and creating comparison graphs that put all of them onto the graphs. \n",
    "\n",
    "Each configuration is specified by a run of type visual_proprioception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/ssheikholeslami/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/ssheikholeslami/SaharaBerryPickerData/settings-sahara.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from visual_proprioception.visproprio_helper import (\n",
    "    load_demonstrations_as_proprioception_training,\n",
    "    load_multiview_demonstrations_as_proprioception_training,\n",
    "    get_visual_proprioception_sp\n",
    ")\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "import sensorprocessing.sp_factory as sp_factory\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETERMINISTIC RUN SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "superpower = 777\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(superpower)\n",
    "import random\n",
    "random.seed(superpower)\n",
    "np.random.seed(superpower)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(superpower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PAPERMILL PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "experiment = \"visual_proprioception\"\n",
    "run = \"vp_comp_flow_all\"\n",
    "\n",
    "external_path = None\n",
    "data_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path).expanduser()\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_aruco\")\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_Vit_multiview\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn\")\n",
    "    Config().copy_experiment(\"sensorprocessing_propriotuned_cnn_multiview\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path).expanduser()\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def force_to_device(model, target_device):\n",
    "    \"\"\"Make sure model and all its parameters are on the target device.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    model = model.to(target_device)\n",
    "    for param in model.parameters():\n",
    "        if param.device != target_device:\n",
    "            param.data = param.data.to(target_device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def ensure_tensor_on_device(x, target_device):\n",
    "    \"\"\"Make sure x is a tensor and is on the correct device.\"\"\"\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float()\n",
    "        else:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "    return x.to(target_device)\n",
    "\n",
    "\n",
    "def is_model_multiview(exp):\n",
    "    \"\"\"Check if an experiment uses a multiview model.\"\"\"\n",
    "    sp_name = exp.get(\"sensor_processing\", \"\")\n",
    "    return (\n",
    "        sp_name.endswith(\"_multiview\") or\n",
    "        sp_name.lower().startswith(\"multiview\") or\n",
    "        \"multiview\" in sp_name.lower() or\n",
    "        exp.get(\"num_views\", 1) > 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LOAD COMPARISON EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "runs = exp[\"tocompare\"]\n",
    "\n",
    "exp_robot = Config().get_experiment(\n",
    "    exp.get(\"robot_exp\", \"robot_al5d\"),\n",
    "    exp.get(\"robot_run\", \"position_controller_00\")\n",
    ")\n",
    "\n",
    "print(f\"Comparing {len(runs)} models:\")\n",
    "for r in runs:\n",
    "    print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LOAD ALL MODELS AND SENSOR PROCESSORS\n",
    "# =============================================================================\n",
    "\n",
    "exps = []\n",
    "sps = []\n",
    "spexps = []\n",
    "models = []\n",
    "\n",
    "for subrun in runs:\n",
    "    subexp = Config().get_experiment(experiment, subrun)\n",
    "    exps.append(subexp)\n",
    "\n",
    "    spexp = Config().get_experiment(subexp[\"sp_experiment\"], subexp[\"sp_run\"])\n",
    "    spexps.append(spexp)\n",
    "\n",
    "    # Load and ensure sensor processor is on the right device\n",
    "    sp = sp_factory.create_sp(spexp, device)\n",
    "    if hasattr(sp, 'enc') and hasattr(sp.enc, 'to'):\n",
    "        sp.enc = force_to_device(sp.enc, device)\n",
    "    if hasattr(sp, 'model') and hasattr(sp.model, 'to'):\n",
    "        sp.model = force_to_device(sp.model, device)\n",
    "    sps.append(sp)\n",
    "\n",
    "    # Load regression model\n",
    "    model = VisProprio_SimpleMLPRegression(subexp)\n",
    "    modelfile = pathlib.Path(subexp[\"data_dir\"], subexp[\"proprioception_mlp_model_file\"])\n",
    "\n",
    "    # Handle different checkpoint formats\n",
    "    checkpoint = torch.load(modelfile, map_location=device)\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "\n",
    "    model = force_to_device(model, device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "    print(f\"Loaded {subexp.get('name', subrun)}: multiview={is_model_multiview(subexp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# EVALUATE EACH MODEL ON ITS TEST DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Evaluating Each Model ===\")\n",
    "ypreds = []\n",
    "all_targets = []\n",
    "all_t = []\n",
    "\n",
    "for i, (subexp, sp, spexp, model) in enumerate(zip(exps, sps, spexps, models)):\n",
    "    model_name = subexp.get('name', f'model_{i}')\n",
    "    print(f\"\\nProcessing model {i+1}/{len(models)}: {model_name}\")\n",
    "\n",
    "    # Get test data paths\n",
    "    proprioception_input_file = pathlib.Path(\n",
    "        subexp.data_dir(), subexp.get(\"proprioception_test_input_file\", \"proprioception_test_input.pth\")\n",
    "    )\n",
    "    proprioception_target_file = pathlib.Path(\n",
    "        subexp.data_dir(), subexp.get(\"proprioception_test_target_file\", \"proprioception_test_target.pth\")\n",
    "    )\n",
    "\n",
    "    # Determine if multiview\n",
    "    is_multiview = is_model_multiview(subexp)\n",
    "\n",
    "    if is_multiview:\n",
    "        print(f\"  Using multi-view evaluation with {subexp.get('num_views', 2)} views\")\n",
    "\n",
    "        tr = load_multiview_demonstrations_as_proprioception_training(\n",
    "            sp, subexp, spexp, exp_robot,\n",
    "            \"validation_data\",\n",
    "            proprioception_input_file,\n",
    "            proprioception_target_file,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        inputs = tr[\"inputs\"]\n",
    "        targets = tr[\"targets\"]\n",
    "\n",
    "        # Run predictions\n",
    "        ypred = []\n",
    "        y = []\n",
    "        t = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx in range(len(targets)):\n",
    "                x = inputs[idx].to(device)\n",
    "                if len(x.shape) == 1:\n",
    "                    x = x.unsqueeze(0)\n",
    "\n",
    "                predictions = model(x)\n",
    "\n",
    "                t.append(idx)\n",
    "                y.append(targets[idx].cpu().numpy())\n",
    "                ypred.append(predictions[0].cpu().numpy())\n",
    "\n",
    "    else:\n",
    "        print(f\"  Using single-view evaluation\")\n",
    "\n",
    "        tr = load_demonstrations_as_proprioception_training(\n",
    "            sp, subexp, spexp, exp_robot,\n",
    "            \"validation_data\",\n",
    "            proprioception_input_file,\n",
    "            proprioception_target_file,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        inputs = tr[\"inputs\"]\n",
    "        targets = tr[\"targets\"]\n",
    "\n",
    "        # Run predictions\n",
    "        ypred = []\n",
    "        y = []\n",
    "        t = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx in range(len(targets)):\n",
    "                x = inputs[idx].to(device)\n",
    "                if len(x.shape) == 1:\n",
    "                    x = x.unsqueeze(0)\n",
    "\n",
    "                predictions = model(x)\n",
    "\n",
    "                t.append(idx)\n",
    "                y.append(targets[idx].cpu().numpy())\n",
    "                ypred.append(predictions[0].cpu().numpy())\n",
    "\n",
    "    ypred = np.vstack(ypred)\n",
    "    y = np.vstack(y)\n",
    "    t = np.array(t)\n",
    "\n",
    "    ypreds.append(ypred)\n",
    "    all_targets.append(y)\n",
    "    all_t.append(t)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = np.mean(np.sum((ypred - y) ** 2, axis=1))\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "\n",
    "# Use first model's targets and time for plotting\n",
    "y = all_targets[0]\n",
    "t = all_t[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# PLOTTING\n",
    "# =============================================================================\n",
    "\n",
    "titles = [\"height\", \"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "output_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Time comparison plot (3x2 layout with bottom legend) ---\n",
    "fig, axs = plt.subplots(3, 2, figsize=(4.6, 6))\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "\n",
    "    for ypred, subexp in zip(ypreds, exps):\n",
    "        ax.plot(t, ypred[:, i], label=subexp.get(\"name\", \"model\"), linewidth=1)\n",
    "    ax.plot(t, y[:, i], label=\"ground truth\", linewidth=2, color=\"black\")\n",
    "\n",
    "    if i == 4:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, ncol=len(exps) + 1,\n",
    "                   bbox_to_anchor=(0.5, 0), loc=\"upper center\", fontsize=8)\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "graphfilename = output_dir / \"comparison23.pdf\"\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = output_dir / \"comparison23.jpg\"\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "print(f\"Saved: {graphfilename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MSE comparison plot (3x2 layout) ---\n",
    "fig, axs = plt.subplots(3, 2, figsize=(4.6, 6))\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    ax.set_ylim(0, 0.5)\n",
    "\n",
    "    bars = []\n",
    "    names = []\n",
    "\n",
    "    for ypred, subexp in zip(ypreds, exps):\n",
    "        error = math.sqrt(np.mean((y[:, i] - ypred[:, i]) ** 2))\n",
    "        br = ax.bar(subexp.get(\"name\", \"model\"), error, label=subexp.get(\"name\", \"model\"))\n",
    "        bars.append(br)\n",
    "        names.append(subexp.get(\"name\", \"model\"))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    if i == 4:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, ncol=2,\n",
    "                   bbox_to_anchor=(0.5, 0), loc=\"upper center\", fontsize=8)\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "graphfilename = output_dir / \"msecomparison23.pdf\"\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = output_dir / \"msecomparison23.jpg\"\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "print(f\"Saved: {graphfilename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GENERATE CSV FILE\n",
    "# =============================================================================\n",
    "\n",
    "csv_path = output_dir / \"msecomparison_values.csv\"\n",
    "\n",
    "# Clear existing file\n",
    "if csv_path.exists():\n",
    "    csv_path.unlink()\n",
    "\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Header\n",
    "    header = [\"Metric\"] + [subexp.get(\"name\", f\"model_{j}\") for j, subexp in enumerate(exps)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write MSE for each DOF\n",
    "    for i, title in enumerate(titles):\n",
    "        errors = []\n",
    "        for ypred in ypreds:\n",
    "            error = math.sqrt(np.mean((y[:, i] - ypred[:, i]) ** 2))\n",
    "            errors.append(error)\n",
    "        writer.writerow([title] + errors)\n",
    "\n",
    "    # Write overall MSE\n",
    "    overall_errors = []\n",
    "    for ypred in ypreds:\n",
    "        overall_mse = np.mean(np.sum((ypred - y) ** 2, axis=1))\n",
    "        overall_errors.append(math.sqrt(overall_mse))\n",
    "    writer.writerow([\"Overall RMSE\"] + overall_errors)\n",
    "\n",
    "print(f\"Saved: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SUMMARY TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<40} {'Type':<15} {'Overall RMSE':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for j, (subexp, ypred) in enumerate(zip(exps, ypreds)):\n",
    "    model_name = subexp.get(\"name\", f\"model_{j}\")[:40]\n",
    "    model_type = \"Multiview\" if is_model_multiview(subexp) else \"Single-view\"\n",
    "    overall_rmse = math.sqrt(np.mean(np.sum((ypred - y) ** 2, axis=1)))\n",
    "    print(f\"{model_name:<40} {model_type:<15} {overall_rmse:>15.6f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find best model\n",
    "best_idx = np.argmin([math.sqrt(np.mean(np.sum((ypred - y) ** 2, axis=1))) for ypred in ypreds])\n",
    "best_name = exps[best_idx].get(\"name\", f\"model_{best_idx}\")\n",
    "print(f\"\\nBest model: {best_name}\")\n",
    "\n",
    "print(\"\\nComparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
