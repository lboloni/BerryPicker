{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67fb5ab",
   "metadata": {},
   "source": [
    "# Exploring the demonstration models\n",
    "\n",
    "This notebook shows several standard ways to interact with the demonstrations stored into the exp/run directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import pathlib\n",
    "from demonstration import Demonstration, list_demos, select_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"demonstration\"\n",
    "run = \"freeform\"\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492dcc9",
   "metadata": {},
   "source": [
    "### Read out all the demonstrations from a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out all the demonstrations from a run\n",
    "#demos = [item.name for item in exp.data_dir().iterdir() if item.is_dir()]\n",
    "demos = list_demos(exp)\n",
    "print(demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = select_demo(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919b650",
   "metadata": {},
   "source": [
    "###  Read out all the pictures in the demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Demonstration(exp, demo)\n",
    "print(demo)\n",
    "print(f\"Cameras found: {demo.cameras}\")\n",
    "print(f\"There are {demo.maxsteps} steps in this demonstration\")\n",
    "print(f\"This demonstration was recorded by the following cameras: {demo.cameras}\")\n",
    "\n",
    "# pick and show a random image from a random camera\n",
    "cam = random.choice(demo.cameras)\n",
    "imgno = random.randint(0, demo.maxsteps)\n",
    "sensor_readings, image = demo.get_image(imgno, camera=cam)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c2aeb",
   "metadata": {},
   "source": [
    "# Move all the figures to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_video_per_camera(demo: Demonstration, cam, delete_img_files = False):\n",
    "    \"\"\"Move the content of a specific camera into video\"\"\"\n",
    "    if cam in demo.exp[\"cameras\"]:\n",
    "        params = demo.exp[\"cameras\"][cam]\n",
    "    else:\n",
    "        params = demo.exp[\"cameras\"][\"all\"]\n",
    "    video_path = demo.get_video_path()\n",
    "    image_paths = []\n",
    "    # Initialize video writer\n",
    "    if not video_path.exists():\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(video_path, fourcc, params[\"fps\"], (params[\"width\"], params[\"height\"]))\n",
    "        for i in range(demo.maxsteps):\n",
    "            img_path = demo.get_image_path(i, camera=cam)\n",
    "            image_paths.append(img_path)\n",
    "            frame = cv2.imread(str(img_path))\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "    # if specified, delete the image files\n",
    "    if delete_img_files:\n",
    "        for img_path in image_paths:\n",
    "            img_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c510c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_video(demo: Demonstration, delete_img_files = False):\n",
    "    \"\"\"Moves to the video the content all all cameras\"\"\"\n",
    "    for cam in demo.cameras:\n",
    "        move_to_video_per_camera(demo, cam, delete_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c46c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensorprocessing.sp_helper import load_capture_to_tensor\n",
    "\n",
    "def get_image_from_video(demo: Demonstration, i, camera=None, cache=False):\n",
    "    \"\"\"Extracts an image from the video. \n",
    "    FIXME: this function opens the video file, seeks and closes it, so it should be very inefficient, we should store the open one in the demonstration instead\n",
    "    If cache is False, the function closes the open file\n",
    "    \"\"\"\n",
    "    if camera is None:\n",
    "        camera = demo.cameras[0]    \n",
    "    if camera not in demo.videocap:    \n",
    "        video_path = demo.get_video_path(camera)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        demo.videocap[camera] = cap\n",
    "    else:\n",
    "        cap = demo.videocap[camera]\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # CV2 reads by default in BGR... \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # cv2.imwrite(output_image, frame)    \n",
    "        image_to_process, image_to_show = load_capture_to_tensor(frame, transform=None)\n",
    "    else:\n",
    "        print(f\"Could not read frame {i}\")\n",
    "    if not cache:\n",
    "        demo.videocap[camera].release()\n",
    "        demo.videocap[camera] = None\n",
    "    return image_to_process, image_to_show        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b495b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgno = random.randint(0, demo.maxsteps)\n",
    "vid_tensor, vid_image = get_image_from_video(demo, imgno)\n",
    "\n",
    "img_tensor, img_image = demo.get_image(imgno, camera=cam)\n",
    "\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(1,3)\n",
    "ax1.imshow(image)\n",
    "ax3.imshow(vid_image)\n",
    "ax2.imshow(img_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9adcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_tensor - img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168a375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
