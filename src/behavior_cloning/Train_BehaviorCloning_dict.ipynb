{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train behavior cloning\n",
    "\n",
    "Train a behavior cloning based robot controller. \n",
    "* Code for loading and pre-processing the training data, typically from a set of demonstrations as specified in an exp/run\n",
    "* Train the controller. \n",
    "* The trained controllers should be saved into the exp/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/pathlib.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\t/home/al5d/.config/BerryPicker/mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\t~/WORK/BerryPicker/cfg/settings.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## NOTEPAD CHANGES OVERVIEW\n",
    "# I changed the notebook to calculate validation loss when training bc if \"validation\" is ture\n",
    "# I adjusted the data creation to include validation data\n",
    "# Config file example: experiment_configs/behavior_cloning/bc_lstm_resid_00_validation.yaml\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import torch\n",
    "import math\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from bc_trainingdata import create_trainingdata_bc_dict\n",
    "from bc_factory import create_bc_model\n",
    "from bc_LSTM_MDN import mdn_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp/run initialization\n",
    "Create the exp/run-s that describe the parameters of the training. Some of the code here is structured in such a way as to make the notebook automatizable with papermill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values \n",
    "# *** This cell should be tagged as parameters     \n",
    "# *** If papermill is used, some of the values will be overwritten \n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "experiment = \"behavior_cloning\"\n",
    "# run = \"bc_mlp_00\"\n",
    "# run = \"bc_lstm_00\"\n",
    "# run = \"bc_lstm_resid_00\"\n",
    "run = \"bc_lstm_resid_00_validation\"\n",
    "# run = \"bc_lstm_mdn_00\"\n",
    "# exp = Config().get_experiment(experiment, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Configuration for exp/run: behavior_cloning/bc_lstm_resid_00_validation successfully loaded\n",
      "Experiment:\n",
      "    batch_size: 64\n",
      "    control_size: 6\n",
      "    controller: bc_LSTM_Residual\n",
      "    controller_file: controller.pth\n",
      "    data_dir: /home/al5d/WORK/BerryPicker/data/behavior_cloning/bc_lstm_resid_00_validation\n",
      "    epochs: 300\n",
      "    exp_robot: robot_al5d\n",
      "    exp_run_sys_indep_file: /home/al5d/WORK/BerryPicker/src/BerryPicker/src/experiment_configs/behavior_cloning/bc_lstm_resid_00_validation.yaml\n",
      "    exp_sp: sensorprocessing_conv_vae\n",
      "    experiment_name: behavior_cloning\n",
      "    hidden_size: 32\n",
      "    loss: MSELoss\n",
      "    name: bc_lstm_resid_00_validation\n",
      "    optimizer: Adam\n",
      "    optimizer_lr: 0.001\n",
      "    run_name: bc_lstm_resid_00_validation\n",
      "    run_robot: position_controller_00\n",
      "    run_sp: sp_vae_128_300epochs_validation\n",
      "    sequence_length: 10\n",
      "    shuffle: false\n",
      "    subrun_name: null\n",
      "    time_started: '2025-09-20 14:07:21.143551'\n",
      "    training_data:\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_15_53'\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_16_57'\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_19_12'\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_21_28'\n",
      "      - dev2\n",
      "    validation: true\n",
      "    validation_data:\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_23_19'\n",
      "      - dev2\n",
      "    - - random-both-cameras-video\n",
      "      - '2025_03_08__14_24_52'\n",
      "      - dev2\n",
      "\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_conv_vae/sp_vae_128_300epochs_validation successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_experiment_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "    Config().copy_experiment(\"behavior_cloning\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_experiment_data(data_path)\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "pprint.pprint(exp)\n",
    "exp_sp = Config().get_experiment(exp[\"exp_sp\"], exp[\"run_sp\"])\n",
    "exp_robot = Config().get_experiment(exp[\"exp_robot\"], exp[\"run_robot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an RNN model\n",
    "Functions for training an RNN type model. These models assume that the input is a sequence $[z_{t-k},...z_{t}]$ while the output is the next action $a_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bc_rnn(model, validation_loss, data, device):\n",
    "    \"\"\"Calculates the average validation error for the behavior cloning model using an RNN with the specific criterion function. Uses the z_validation an a_validation fields in \"data\". The inputs and the targets a list of individual input and target. \n",
    "    CHECK: I think that the target is supposed to be the last output of the RNN when the whole input string had been passed through it. \n",
    "    The model is reset before each of the strings (i.e. state is not transferred)\n",
    "    model: an LSTM or similar model that can consume a sequence of inputs\n",
    "    criterion: any function that calculates the distance between the targets\n",
    "    \"\"\"\n",
    "    num_sequences = data[\"z_validation\"].shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            input_seq = data[\"z_validation\"][i].to(device)\n",
    "            target = data[\"a_validation\"][i].to(device)\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "            if not model.stochastic:\n",
    "                outputs = model(input_seq)\n",
    "                loss = validation_loss(outputs, target)\n",
    "            else: # for MDN, the output is sampling\n",
    "                outputs = model.forward_and_sample(input_seq)\n",
    "            loss = validation_loss(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES\n",
    "# Changed function to accept a validation parameter to use validation loss when saving the model with early stopping\n",
    "\n",
    "def train_bc_rnn(model, optimizer, criterion, data, num_epochs, batch_size=32, controller_path=None, validation=False):\n",
    "    \"\"\"Train a behavior cloning model using a sequence model (eg. an RNN)\n",
    "    Uses a writer for TensorBoard _and_ tqdm\n",
    "    Model with batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Variables to track validation progress\n",
    "    best_val = math.inf\n",
    "    best_model = None\n",
    "    best_model_epoch = None\n",
    "    early_stop_epoch = 10\n",
    "\n",
    "    exp.start_timer(\"train\")\n",
    "    num_sequences = data[\"z_train\"].shape[0]\n",
    "    num_batches = num_sequences // batch_size\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):        \n",
    "        model.train()\n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            # Prepare input and target\n",
    "            input_seq = data[\"z_train\"][i * batch_size: (i+1)* batch_size].to(device) # Shape: [batch_size, sequence_length, latent_size]\n",
    "            target = data[\"a_train\"][i * batch_size: (i+1)* batch_size].to(device) # Shape: [batch_size, latent_size]\n",
    "            # Forward pass\n",
    "            output = model(input_seq) # Shape: [batch_size, output_size]\n",
    "            # Check for MDN, that is different\n",
    "            if not model.stochastic:\n",
    "                loss = criterion(output, target)\n",
    "            else: \n",
    "                loss = mdn_loss(target, *output)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "\n",
    "        #if writer is not None:\n",
    "        #    writer.add_scalar(\"TrainingLoss\", avg_training_loss, epoch)\n",
    "        #    writer.add_scalar(\"ValidationLoss\", avg_validation_loss, epoch)\n",
    "        #    writer.flush()\n",
    "        if (epoch+1) % 5 == 0: # was 0\n",
    "            avg_validation_loss = validate_bc_rnn(model, criterion, data, device)\n",
    "\n",
    "            # Added condition to track model with best validation loss if validation flag is true\n",
    "            if validation:\n",
    "                if avg_validation_loss < best_val:\n",
    "                    best_val = avg_validation_loss\n",
    "                    best_model = model.state_dict()\n",
    "                    best_model_epoch = epoch\n",
    "                if best_val < avg_validation_loss and epoch > best_model_epoch + early_stop_epoch:\n",
    "                    print(\"Early stop.\")\n",
    "                    break\n",
    "            else:\n",
    "                best_model = model.state_dict()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n",
    "    print(\"Training complete.\")\n",
    "    exp.end_timer(\"train\")\n",
    "\n",
    "    # Save model in this function now for clarity\n",
    "    torch.save(best_model, controller_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES\n",
    "# Checks for validation in config file and saves model based on validation score\n",
    "# Splits data into training and validation if config \"training\" is True\n",
    "# Performs shuffling on the training data unless config \"shuffle\" is False\n",
    "#   FUNCTION: train_bc_rnn()\n",
    "#   FUNCTION: create_trainingdata_bc()\n",
    "\n",
    "controller_path = pathlib.Path(exp.data_dir(), exp[\"controller_file\"])\n",
    "\n",
    "# Checks if validation and shuffle parameters exists\n",
    "if 'validation' in exp:\n",
    "    validation = exp['validation']\n",
    "else:\n",
    "    validation = False\n",
    "\n",
    "if controller_path.exists():\n",
    "    print(\"***Train_BehaviorCloning: Controller exists. Re-run with creation-style=discard-old to recompute.\")\n",
    "else:\n",
    "    print(\"***Train_BehaviorCloning: Proceeding to train the controller.\")\n",
    "    model, validation_loss, optimizer = create_bc_model(exp, exp_sp, device)\n",
    "    print(model)\n",
    "\n",
    "    data = create_trainingdata_bc_dict(exp, exp_sp, exp_robot, device=\"cpu\", validation=validation)\n",
    "    # Training Loop\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    batch_size = exp[\"batch_size\"]\n",
    "\n",
    "    # Create a SummaryWriter instance\n",
    "    # where does the logdir go???\n",
    "    # writer = SummaryWriter(logdir=\"/home/lboloni/runs/example\")\n",
    "    train_bc_rnn(\n",
    "            model, optimizer, validation_loss, data=data,\n",
    "            num_epochs=num_epochs, batch_size=batch_size, controller_path=controller_path, validation=validation)\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model \n",
    "\n",
    "Creates and trains a behavior cloning model specified by the exp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berrypickervenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
