{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train behavior cloning\n",
    "\n",
    "Train a behavior cloning based robot controller. \n",
    "* Code for loading and pre-processing the training data, typically from a set of demonstrations as specified in an exp/run\n",
    "* Train the behavior cloning controller. This notebook should be able to run different kind of controllers such as MLP, LSTM, LSTM+MDN, Transformer etc. \n",
    "__To be done as of June 1, 2025__\n",
    "* The trained controllers should be saved into the exp/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tG:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "from sensorprocessing.sp_helper import get_transform_to_sp\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from demo_to_trainingdata import create_RNN_training_sequence_xy\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "from bc_LSTM import LSTMXYPredictor, LSTMResidualController\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"behavior_cloning\"\n",
    "run = \"bc_mlp_00\"\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation data\n",
    "Create training and validation data from all the demonstrations of a certain task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: logging configuration file is not found in logger\\logger_config.json.\n",
      "Cameras found: ['dev2']\n",
      "There are 744 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 744,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_18_47'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 744}\n",
      "(743, 128)\n",
      "(743, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 968 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 968,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_23_22'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 968}\n",
      "(967, 128)\n",
      "(967, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 753 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 753,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_31_40'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 753}\n",
      "(752, 128)\n",
      "(752, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 508 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 508,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_33_14'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 508}\n",
      "(507, 128)\n",
      "(507, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 469 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 469,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_34_31'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 469}\n",
      "(468, 128)\n",
      "(468, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 787 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 787,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x0000018CFDB7BF50>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_36_02'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 787}\n",
      "(786, 128)\n",
      "(786, 6)\n"
     ]
    }
   ],
   "source": [
    "def create_bc_training_and_validation(exp, sp):\n",
    "    \"\"\"Creates training data for training and validation with the demonstrations specified in the exp/run. Caches the results into the input and target files specified in the exp/run. Remove those files to recalculate.\"\"\"\n",
    "    retval = {}\n",
    "    input_path = pathlib.Path(exp.data_dir(), \"training_input.pth\")\n",
    "    target_path = pathlib.Path(exp.data_dir(), \"training_target.pth\")\n",
    "\n",
    "    demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "    task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "    if input_path.exists():\n",
    "        retval[\"inputs\"] = torch.load(input_path, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(target_path, weights_only=True)\n",
    "    else:\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "        transform = get_transform_to_sp(exp)\n",
    "        for val in exp[\"training_data\"]:\n",
    "            run, demo_name, camera = val\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "            # read the a and z \n",
    "            for i in range(demo.metadata[\"maxsteps\"]):\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = demo.get_action(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()        \n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "            inputs, targets = create_RNN_training_sequence_xy(inputlist, targetlist, sequence_length=10)\n",
    "            inputlist.append(inputs)\n",
    "            targetlist.append(targets)\n",
    "        inputs = torch.cat(inputlist)\n",
    "        targets = torch.cat(targetlist)\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    rows = torch.randperm(inputs.size(0)) \n",
    "    shuffled_inputs = inputs[rows]\n",
    "    shuffled_targets = targets[rows]\n",
    "\n",
    "    training_size = int( inputs.size(0) * 0.67 )\n",
    "    inputs_training = shuffled_inputs[1:training_size]\n",
    "    targets_training = shuffled_targets[1:training_size]\n",
    "\n",
    "    inputs_validation = shuffled_inputs[training_size:]\n",
    "    targets_validation = shuffled_targets[training_size:] \n",
    "    return inputs_training, targets_training, inputs_validation, targets_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_behavior_cloning(model, criterion, inputs_validation, targets_validation):\n",
    "    \"\"\"Calculates the validation error for the behavior cloning model using pairs of input strings (of the specific length) and single output target string. \n",
    "    The model is reset before each of the strings (i.e. state is not transferred)\n",
    "    model: an LSTM or similar model that can consume a sequence of inputs    \n",
    "    \"\"\"\n",
    "    num_sequences = inputs_validation.shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            # Forward pass\n",
    "            input_seq = inputs_validation[i]\n",
    "            target = targets_validation[i]\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, target)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_behavior_cloning(model, optimizer, criterion, inputs_training, targets_training, inputs_validation, targets_validation, num_epochs, writer = None):\n",
    "    \"\"\"Train a behavior cloning model of the LSTM class.\"\"\"\n",
    "    num_sequences = inputs_training.shape[0]\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_sequences):\n",
    "            # Prepare input and target\n",
    "            input_seq = inputs_training[i]\n",
    "            target = targets_training[i]\n",
    "\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "        avg_validation_loss = validate_behavior_cloning(model, criterion, inputs_validation=inputs_validation, targets_validation=targets_validation)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"TrainingLoss\", avg_training_loss, epoch)\n",
    "            writer.add_scalar(\"ValidationLoss\", avg_validation_loss, epoch)\n",
    "            writer.flush()\n",
    "        if (epoch+1) % 2 == 0: # was 0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTMXYPredictor model \n",
    "\n",
    "Trains the single layer LSTM model LSTMXYPredictor. This is a baseline LSTM model. \n",
    "\n",
    "Training notes:\n",
    "* On the proprioception experiments, this reaches the performance:\n",
    "    Epoch [20/100], Training Loss: 0.0079 Validation Loss: 0.0080\n",
    "* No further improvement is observed from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:15<12:47,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Training Loss: 0.0117 Validation Loss: 0.0115 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:31<12:38,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Training Loss: 0.0108 Validation Loss: 0.0105 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:47<12:25,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Training Loss: 0.0103 Validation Loss: 0.0101 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:04<12:27,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Training Loss: 0.0099 Validation Loss: 0.0097 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:20<12:12,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Training Loss: 0.0096 Validation Loss: 0.0093 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [01:37<12:06,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Training Loss: 0.0093 Validation Loss: 0.0089 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [01:54<12:17,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Training Loss: 0.0089 Validation Loss: 0.0084 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [02:11<11:58,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Training Loss: 0.0085 Validation Loss: 0.0079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [02:30<12:02,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Training Loss: 0.0082 Validation Loss: 0.0081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:47<11:41,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Training Loss: 0.0079 Validation Loss: 0.0080 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [03:05<11:27,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Training Loss: 0.0077 Validation Loss: 0.0076 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [03:22<11:02,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Training Loss: 0.0075 Validation Loss: 0.0075 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [03:40<11:02,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Training Loss: 0.0074 Validation Loss: 0.0074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [03:58<10:55,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Training Loss: 0.0078 Validation Loss: 0.0075 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [04:15<10:11,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Training Loss: 0.0070 Validation Loss: 0.0073 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [04:33<09:52,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Training Loss: 0.0068 Validation Loss: 0.0077 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [04:51<09:47,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Training Loss: 0.0067 Validation Loss: 0.0073 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [05:10<09:45,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Training Loss: 0.0067 Validation Loss: 0.0073 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [05:29<09:47,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Training Loss: 0.0065 Validation Loss: 0.0071 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [05:50<09:54,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Training Loss: 0.0066 Validation Loss: 0.0074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [06:08<09:12,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Training Loss: 0.0062 Validation Loss: 0.0072 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [06:27<08:56,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 0.0061 Validation Loss: 0.0082 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [06:47<08:47,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Training Loss: 0.0059 Validation Loss: 0.0074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [07:10<09:09, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Training Loss: 0.0056 Validation Loss: 0.0076 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [07:34<09:31, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Training Loss: 0.0055 Validation Loss: 0.0081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [08:01<10:01, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Training Loss: 0.0054 Validation Loss: 0.0082 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [08:29<10:07, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Training Loss: 0.0056 Validation Loss: 0.0079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [08:51<08:49, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Training Loss: 0.0053 Validation Loss: 0.0088 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [09:12<07:57, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Training Loss: 0.0053 Validation Loss: 0.0085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [09:32<07:03, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Training Loss: 0.0053 Validation Loss: 0.0079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [09:53<06:36, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Training Loss: 0.0051 Validation Loss: 0.0081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [10:14<06:24, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Training Loss: 0.0052 Validation Loss: 0.0101 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [10:35<05:58, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Training Loss: 0.0049 Validation Loss: 0.0087 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [10:59<06:02, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Training Loss: 0.0048 Validation Loss: 0.0084 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [11:21<05:35, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Training Loss: 0.0048 Validation Loss: 0.0083 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [11:42<05:00, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Training Loss: 0.0045 Validation Loss: 0.0090 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [12:05<04:49, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Training Loss: 0.0048 Validation Loss: 0.0074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [12:28<04:28, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Training Loss: 0.0043 Validation Loss: 0.0078 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [12:49<04:00, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Training Loss: 0.0047 Validation Loss: 0.0088 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [13:11<03:37, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Training Loss: 0.0045 Validation Loss: 0.0081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [13:31<03:09, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Training Loss: 0.0046 Validation Loss: 0.0087 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [13:52<02:48, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Training Loss: 0.0042 Validation Loss: 0.0079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [14:15<02:35, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Training Loss: 0.0040 Validation Loss: 0.0077 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [14:38<02:15, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Training Loss: 0.0041 Validation Loss: 0.0085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [14:59<01:48, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Training Loss: 0.0039 Validation Loss: 0.0080 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [15:21<01:26, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Training Loss: 0.0041 Validation Loss: 0.0081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [15:45<01:09, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Training Loss: 0.0039 Validation Loss: 0.0073 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [16:09<00:46, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Training Loss: 0.0039 Validation Loss: 0.0078 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [16:29<00:21, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Training Loss: 0.0042 Validation Loss: 0.0077 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:50<00:00, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Training Loss: 0.0035 Validation Loss: 0.0077 \n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "hidden_size = 32  # \n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "\n",
    "task = \"proprioception-uncluttered\"\n",
    "inputs_training, targets_training, inputs_validation, targets_validation = create_bc_training_and_validation(task)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# where does the logdir go???\n",
    "writer = SummaryWriter(logdir=\"/home/lboloni/runs/example\")\n",
    "train_behavior_cloning(\n",
    "    model, optimizer, criterion,\n",
    "    inputs_training=inputs_training, \n",
    "    targets_training=targets_training, \n",
    "    inputs_validation=inputs_validation,\n",
    "    targets_validation=targets_validation,\n",
    "    num_epochs=num_epochs, writer=writer)\n",
    "print(\"Training complete.\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = pathlib.Path(exp.data_dir(), exp[\"controller_file\"])\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
