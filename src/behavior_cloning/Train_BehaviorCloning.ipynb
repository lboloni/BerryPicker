{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train behavior cloning\n",
    "\n",
    "Train a behavior cloning based robot controller. \n",
    "* Code for loading and pre-processing the training data, typically from a set of demonstrations as specified in an exp/run\n",
    "* Train the behavior cloning controller. This notebook should be able to run different kind of controllers such as MLP, LSTM, LSTM+MDN, Transformer etc. \n",
    "__To be done as of June 1, 2025__\n",
    "* The trained controllers should be saved into the exp/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tG:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pprint\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "from sensorprocessing.sp_helper import get_transform_to_sp\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "from bc_LSTM import LSTMXYPredictor, LSTMResidualController\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\behavior_cloning\\lstm_00_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: behavior_cloning/lstm_00 successfully loaded\n",
      "Experiment:\n",
      "    control_size: 6\n",
      "    controller: LSTMXYPredictor\n",
      "    controller_file: controller.pth\n",
      "    controller_hidden_size: 32\n",
      "    controller_num_layers: 2\n",
      "    data_dir: c:\\Users\\lboloni\\Documents\\Code\\_TempData\\BerryPicker-experiments\\behavior_cloning\\lstm_00\n",
      "    epochs: 10\n",
      "    exp_run_sys_indep_file: C:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\experiment_configs\\behavior_cloning\\lstm_00.yaml\n",
      "    experiment_name: behavior_cloning\n",
      "    run_name: lstm_00\n",
      "    sequence_lenght: 10\n",
      "    sp_experiment: sensorprocessing_conv_vae\n",
      "    sp_run: sp_vae_256\n",
      "    subrun_name: null\n",
      "    time_started: '2025-06-02 18:26:10.292469'\n",
      "    training_data:\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_15_53'\n",
      "      - dev2\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_16_57'\n",
      "      - dev2\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_19_12'\n",
      "      - dev2\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_21_28'\n",
      "      - dev2\n",
      "    validation_data:\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_23_19'\n",
      "      - dev2\n",
      "    - - random-both-cameras\n",
      "      - '2025_03_08__14_24_52'\n",
      "      - dev2\n",
      "\n",
      "***ExpRun**: No system dependent experiment file\n",
      "\t G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sensorprocessing_conv_vae\\sp_vae_256_sysdep.yaml,\n",
      "\t that is ok, proceeding.\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_conv_vae/sp_vae_256 successfully loaded\n",
      "Warning: logging configuration file is not found in logger\\logger_config.json.\n"
     ]
    }
   ],
   "source": [
    "experiment = \"behavior_cloning\"\n",
    "# run = \"bc_mlp_00\"\n",
    "run = \"lstm_00\"\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint.pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "spexp = Config().get_experiment(exp[\"sp_experiment\"], exp[\"sp_run\"])\n",
    "sp = create_sp(spexp, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation data\n",
    "Create training and validation data from all the demonstrations of a certain task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_training_sequence_xy(x_seq, y_seq, sequence_length):\n",
    "    \"\"\"Create supervised training data for RNNs such as LSTM from two sequences. In this data, from a string of length sequence_length in x_seq we are predicting the next item in y_seq.\n",
    "    x_seq and y_seq are tensors\n",
    "    Returns the results as tensors\n",
    "    \"\"\"\n",
    "    # Prepare training data\n",
    "    total_length = x_seq.shape[0]\n",
    "    #total_length = len(x_seq)\n",
    "    #assert total_length == len(y_seq)\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(total_length - sequence_length):\n",
    "        # Input is a subsequence of length `sequence_length`\n",
    "        input_seq = x_seq[i:i + sequence_length]\n",
    "        # Shape: [sequence_length, latent_size]\n",
    "\n",
    "        # Target is the next vector after the input sequence\n",
    "        target = y_seq[i + sequence_length]\n",
    "        # Shape: [output_size]\n",
    "\n",
    "        # Append to lists\n",
    "        inputs.append(torch.tensor(input_seq))\n",
    "        targets.append(torch.tensor(target))\n",
    "\n",
    "    # Convert lists to tensors for training\n",
    "    inputs = torch.stack(inputs)   # Shape: [num_samples, sequence_length, latent_size]\n",
    "    targets = torch.stack(targets) # Shape: [num_samples, latent_size]\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bc_training_and_validation(exp, spexp, sp):\n",
    "    \"\"\"Creates training data for training and validation with the demonstrations specified in the exp/run. Caches the results into the input and target files specified in the exp/run. Remove those files to recalculate.\"\"\"\n",
    "    retval = {}\n",
    "    input_path = pathlib.Path(exp.data_dir(), \"training_input.pth\")\n",
    "    target_path = pathlib.Path(exp.data_dir(), \"training_target.pth\")\n",
    "\n",
    "    if input_path.exists():\n",
    "        inputs = torch.load(input_path, weights_only=True)\n",
    "        targets = torch.load(target_path, weights_only=True)\n",
    "    else:\n",
    "        all_demos_inputs = []\n",
    "        all_demos_targets = []\n",
    "        transform = get_transform_to_sp(spexp)\n",
    "        for val in exp[\"training_data\"]: # for all demonstrations\n",
    "            run, demo_name, camera = val\n",
    "            exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "            demo = Demonstration(exp_demo, demo_name)\n",
    "            # read the a and z \n",
    "            inputlist = []\n",
    "            targetlist = []\n",
    "            for i in range(demo.metadata[\"maxsteps\"]-1): # -1 because of lookahead\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)                \n",
    "                # inputlist.append(sensor_readings[0])\n",
    "                z = sp.process(sensor_readings)\n",
    "                inputlist.append(torch.from_numpy(z))\n",
    "                # the action we are choosing, is the next one\n",
    "                a = demo.get_action(i+1)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()        \n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "            inputlist_tensor = torch.stack(inputlist)\n",
    "            targetlist_tensor = torch.stack(targetlist)\n",
    "            inputs, targets = create_RNN_training_sequence_xy(inputlist_tensor, targetlist_tensor, sequence_length=exp[\"sequence_lenght\"])\n",
    "            all_demos_inputs.append(inputs)\n",
    "            all_demos_targets.append(targets)\n",
    "        inputs = torch.cat(all_demos_inputs)\n",
    "        targets = torch.cat(all_demos_targets)\n",
    "        torch.save(inputs, input_path)\n",
    "        torch.save(targets, target_path)\n",
    "\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    # rows = torch.randperm(inputs.size(0)) \n",
    "    rows = torch.randperm(inputs.shape[0]) \n",
    "    shuffled_inputs = inputs[rows]\n",
    "    shuffled_targets = targets[rows]\n",
    "\n",
    "    training_size = int( inputs.shape[0] * 0.67 )\n",
    "    inputs_training = shuffled_inputs[1:training_size]\n",
    "    targets_training = shuffled_targets[1:training_size]\n",
    "\n",
    "    inputs_validation = shuffled_inputs[training_size:]\n",
    "    targets_validation = shuffled_targets[training_size:] \n",
    "    return inputs_training, targets_training, inputs_validation, targets_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bc_rnn(model, criterion, inputs_validation, targets_validation):\n",
    "    \"\"\"Calculates the average validation error for the behavior cloning model using an RNN with the specific criterion function.     \n",
    "    The inputs and the targets a list of individual input and target. An input is a string of specific length. The target is a single value. \n",
    "    CHECK: I think that the target is supposed to be the last output of the RNN when the whole input string had been passed through it. \n",
    "    The model is reset before each of the strings (i.e. state is not transferred)\n",
    "    model: an LSTM or similar model that can consume a sequence of inputs\n",
    "    criterion: any function that calculates the distance between the targets\n",
    "    \"\"\"\n",
    "    num_sequences = inputs_validation.shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            # Forward pass\n",
    "            input_seq = inputs_validation[i]\n",
    "            target = targets_validation[i]\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, target)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_behavior_cloning(model, optimizer, criterion, inputs_training, targets_training, inputs_validation, targets_validation, num_epochs, writer = None):\n",
    "    \"\"\"Train a behavior cloning model of the LSTM class.\"\"\"\n",
    "    num_sequences = inputs_training.shape[0]\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_sequences):\n",
    "            # Prepare input and target\n",
    "            input_seq = inputs_training[i]\n",
    "            target = targets_training[i]\n",
    "\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "        avg_validation_loss = validate_bc_rnn(model, criterion, inputs_validation=inputs_validation, targets_validation=targets_validation)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"TrainingLoss\", avg_training_loss, epoch)\n",
    "            writer.add_scalar(\"ValidationLoss\", avg_validation_loss, epoch)\n",
    "            writer.flush()\n",
    "        if (epoch+1) % 2 == 0: # was 0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTMXYPredictor model \n",
    "\n",
    "Trains the single layer LSTM model LSTMXYPredictor. This is a baseline LSTM model. \n",
    "\n",
    "Training notes:\n",
    "* On the proprioception experiments, this reaches the performance:\n",
    "    Epoch [20/100], Training Loss: 0.0079 Validation Loss: 0.0080\n",
    "* No further improvement is observed from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'inputs' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMXYPredictor(latent_size\u001b[38;5;241m=\u001b[39mlatent_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, output_size \u001b[38;5;241m=\u001b[39m output_size, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m     12\u001b[0m exp\u001b[38;5;241m.\u001b[39mstart_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_preparation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m inputs_training, targets_training, inputs_validation, targets_validation \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bc_training_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m exp\u001b[38;5;241m.\u001b[39mend_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_preparation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Mean Squared Error for regression\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m, in \u001b[0;36mcreate_bc_training_and_validation\u001b[1;34m(exp, spexp, sp)\u001b[0m\n\u001b[0;32m     39\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(targets, target_path)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Separate the training and validation data. \u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# We will be shuffling the demonstrations \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# rows = torch.randperm(inputs.size(0)) \u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m rows \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m     46\u001b[0m shuffled_inputs \u001b[38;5;241m=\u001b[39m inputs[rows]\n\u001b[0;32m     47\u001b[0m shuffled_targets \u001b[38;5;241m=\u001b[39m targets[rows]\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'inputs' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "if exp[\"controller\"] != \"LSTMXYPredictor\":\n",
    "    raise Exception(f\"The controller specified {exp['controller']} is not LSTMXYPredictor\")\n",
    "\n",
    "latent_size = spexp[\"latent_size\"]\n",
    "output_size = exp[\"control_size\"]  # degrees of freedom in the robot\n",
    "num_layers = exp[\"controller_num_layers\"]\n",
    "hidden_size = exp[\"controller_hidden_size\"] \n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "\n",
    "exp.start_timer(\"data_preparation\")\n",
    "inputs_training, targets_training, inputs_validation, targets_validation = create_bc_training_and_validation(exp, spexp, sp)\n",
    "exp.end_timer(\"data_preparation\")\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = exp[\"epochs\"]\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# where does the logdir go???\n",
    "writer = SummaryWriter(logdir=\"/home/lboloni/runs/example\")\n",
    "exp.start_timer(\"train\")\n",
    "train_behavior_cloning(\n",
    "    model, optimizer, criterion,\n",
    "    inputs_training=inputs_training, \n",
    "    targets_training=targets_training, \n",
    "    inputs_validation=inputs_validation,\n",
    "    targets_validation=targets_validation,\n",
    "    num_epochs=num_epochs, writer=writer)\n",
    "writer.close()\n",
    "print(\"Training complete.\")\n",
    "exp.end_timer(\"train\")\n",
    "controller_path = pathlib.Path(exp.data_dir(), exp[\"controller_file\"])\n",
    "torch.save(model.state_dict(), controller_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
