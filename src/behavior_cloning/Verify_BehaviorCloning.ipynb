{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1aae1a",
   "metadata": {},
   "source": [
    "# Verify behavior cloning\n",
    "\n",
    "Visually verify a behavior cloning based robot controller.\n",
    "\n",
    "* Load the controller trained by the Train_BC notebook and the corresponding sp\n",
    "* Choose a demonstration, to which the SP/BC pair will be applied to the images\n",
    "* Visualize the path taken by the demonstration and what would be (autoregressively) recommended by the controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab463743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\pathlib\\__init__.py\n",
      "***ExpRun**: Loading pointer config file:\n",
      "\tC:\\Users\\lotzi\\.config\\BerryPicker\\mainsettings.yaml\n",
      "***ExpRun**: Loading machine-specific config file:\n",
      "\tc:\\Users\\lotzi\\Work\\_Config\\BerryPicker\\cfg\\settings.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from exp_run_config import Config\n",
    "Config.PROJECTNAME = \"BerryPicker\"\n",
    "\n",
    "import pathlib\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "# fixing the fact that it defaults to Type 3 fonts\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from bc_factory import create_bc_model\n",
    "# FIXME: factor this out\n",
    "from bc_LSTM_MDN import bc_LSTM_MDN, mdn_loss\n",
    "\n",
    "torch.manual_seed(1)\n",
    "from sensorprocessing.sp_helper import get_transform_to_sp\n",
    "from sensorprocessing.sp_factory import create_sp\n",
    "from demonstration.demonstration import Demonstration\n",
    "\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "import bc_factory \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9184821",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# *** Initialize the variables with default values \n",
    "# *** This cell should be tagged as parameters     \n",
    "# *** If papermill is used, some of the values will be overwritten \n",
    "\n",
    "# If it is set to true, the exprun will be recreated from scratch\n",
    "creation_style = \"exist-ok\"\n",
    "\n",
    "# If not None, set an external experiment path\n",
    "external_path = None\n",
    "\n",
    "# If not None, set an output path\n",
    "data_path = None\n",
    "\n",
    "experiment = \"behavior_cloning\"\n",
    "run = None\n",
    "# run = \"bc_verify_mlp_00\"\n",
    "# run = \"bc_verify_lstm_00\"\n",
    "# run = \"bc_verify_lstm_resid_00\"\n",
    "# run = \"bc_verify_lstm_mdn_00\"\n",
    "\n",
    "\n",
    "# **** The settings below are used to debug a particular run in flow \n",
    "# **** They do not affect the run of the flow, \n",
    "# **** because they are going to be overwritten by papermill when run in a flow\n",
    "creation_style = \"exist-ok\"\n",
    "data_path = \"c:/Users/lotzi/Work/_Data/BerryPicker-Flows/BC-touch-apple/result\"\n",
    "external_path = \"c:/Users/lotzi/Work/_Data/BerryPicker-Flows/BC-touch-apple/exprun\"\n",
    "experiment = \"behavior_cloning\"\n",
    "# run = \"_flow_bc_mlp_0001_verify\"\n",
    "run = \"_flow_bc_lstm_0001_verify\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066a74d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***ExpRun**: Experiment config path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\n",
      "***ExpRun**: Experiment sensorprocessing_conv_vae copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\\sensorprocessing_conv_vae\n",
      "***ExpRun**: Experiment robot_al5d copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\\robot_al5d\n",
      "***ExpRun**: Experiment demonstration copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\\demonstration\n",
      "***ExpRun**: Experiment behavior_cloning copied to\n",
      "c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\\behavior_cloning\n",
      "***ExpRun**: Experiment data path changed to c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\result\n",
      "***ExpRun**: Configuration for exp/run: behavior_cloning/_flow_bc_lstm_0001_verify successfully loaded\n",
      "Experiment:\n",
      "    batch_size: 64\n",
      "    control_size: 6\n",
      "    data_dir: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\result\\behavior_cloning\\_flow_bc_lstm_0001_verify\n",
      "    epochs: 10\n",
      "    exp_bc: behavior_cloning\n",
      "    exp_bc_verify: behavior_cloning\n",
      "    exp_robot: robot_al5d\n",
      "    exp_run_sys_indep_file: c:\\Users\\lotzi\\Work\\_Data\\BerryPicker-Flows\\BC-touch-apple\\exprun\\behavior_cloning\\_flow_bc_lstm_0001_verify.yaml\n",
      "    exp_sp: sensorprocessing_conv_vae\n",
      "    experiment_name: behavior_cloning\n",
      "    loss: MSELoss\n",
      "    name: LSTM 2 layers\n",
      "    optimizer: Adam\n",
      "    optimizer_lr: 0.001\n",
      "    run_bc: _flow_bc_lstm_0001\n",
      "    run_bc_verify: _flow_bc_lstm_0001_verify\n",
      "    run_name: _flow_bc_lstm_0001_verify\n",
      "    run_robot: position_controller_00\n",
      "    run_sp: _flow_sp_conv_vae_0001\n",
      "    sequence_length: 10\n",
      "    subrun_name: null\n",
      "    training_data:\n",
      "    - - touch-apple\n",
      "      - bc_training_00000\n",
      "      - dev0\n",
      "    - - touch-apple\n",
      "      - bc_training_00001\n",
      "      - dev0\n",
      "    - - touch-apple\n",
      "      - bc_training_00002\n",
      "      - dev0\n",
      "    - - touch-apple\n",
      "      - bc_training_00003\n",
      "      - dev0\n",
      "    validation_data:\n",
      "    - - touch-apple\n",
      "      - bc_validation_00000\n",
      "      - dev0\n",
      "    - - touch-apple\n",
      "      - bc_validation_00001\n",
      "      - dev0\n",
      "\n",
      "***ExpRun**: Configuration for exp/run: sensorprocessing_conv_vae/_flow_sp_conv_vae_0001 successfully loaded\n",
      "Warning: logging configuration file is not found in logger\\logger_config.json.\n",
      "***ExpRun**: Configuration for exp/run: robot_al5d/position_controller_00 successfully loaded\n",
      "***ExpRun**: Configuration for exp/run: behavior_cloning/_flow_bc_lstm_0001 successfully loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if external_path:\n",
    "    external_path = pathlib.Path(external_path)\n",
    "    assert external_path.exists()\n",
    "    Config().set_exprun_path(external_path)\n",
    "    Config().copy_experiment(\"sensorprocessing_conv_vae\")\n",
    "    Config().copy_experiment(\"robot_al5d\")\n",
    "    Config().copy_experiment(\"demonstration\")\n",
    "    Config().copy_experiment(\"behavior_cloning\")\n",
    "if data_path:\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    assert data_path.exists()\n",
    "    Config().set_results_path(data_path)\n",
    "    \n",
    "# temporary solution\n",
    "# Setting up a separate directory for generated and computed data\n",
    "\n",
    "exp = Config().get_experiment(experiment, run, creation_style=creation_style)\n",
    "pprint.pprint(exp)\n",
    "\n",
    "# Create the sp object described in the experiment\n",
    "exp_sp = Config().get_experiment(exp[\"exp_sp\"], exp[\"run_sp\"])\n",
    "sp = create_sp(exp_sp, device)\n",
    "transform = get_transform_to_sp(exp_sp)\n",
    "\n",
    "exp_robot = Config().get_experiment(exp[\"exp_robot\"], exp[\"run_robot\"])\n",
    "\n",
    "# Load the bc model described in the experiment\n",
    "exp_bc = Config().get_experiment(exp[\"exp_bc\"], exp[\"run_bc\"])\n",
    "model, _, _ = create_bc_model(exp_bc, exp_sp, device)\n",
    "model_path = pathlib.Path(exp_bc.data_dir(), exp_bc[\"controller_file\"])\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))   \n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b264e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad\n",
    "def run_teacher_forcing(sp, model, sequence_length, data_spec, pred_path, teacher_path):\n",
    "    \"\"\"Runs behavior cloning on a demonstration in teacher forcing mode. For each step, it creates \n",
    "    FIXME: model should be a more wrapped thing, not just the torch model\n",
    "    \"\"\"\n",
    "    a_teacher_list = []\n",
    "    a_pred_list = []\n",
    "    for val in data_spec: # for all demonstrations\n",
    "        run, demo_name, camera = val\n",
    "        exp_demo = Config().get_experiment(\"demonstration\", run)\n",
    "        demo = Demonstration(exp_demo, demo_name)\n",
    "        # perform the run for each of the demonstrations\n",
    "        for i in range(demo.metadata[\"maxsteps\"]-sequence_length): # -1 because of lookahead\n",
    "            #\n",
    "            # This is the part where we create the input to the controller\n",
    "            # \n",
    "            if sequence_length:\n",
    "                # if the sequence length is not zero, we read ahead that much, and create the list\n",
    "                input_list = []\n",
    "                # FIXME make it sliding window\n",
    "                for j in range(sequence_length):\n",
    "                    sensor_readings, _ = demo.get_image(i+j, device=device, transform=transform, camera=camera)\n",
    "                    #if sensor_readings is not None:\n",
    "                    #    print(f\"reading {i+j} successful\")\n",
    "                    #else:\n",
    "                    #    print(f\"reading {i+j} not successful, returned none\")\n",
    "                    z = sp.process(sensor_readings)\n",
    "                    input_list.append(z)\n",
    "                    input_nd = np.array(input_list)\n",
    "                    z_tensor = torch.tensor(input_nd).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                # sequence length it zero, it is just the current image\n",
    "                sensor_readings, _ = demo.get_image(i, device=device, transform=transform, camera=camera)\n",
    "                z = sp.process(sensor_readings)\n",
    "                z_tensor = torch.tensor(z).unsqueeze(0).to(device)\n",
    "            #\n",
    "            # Calling the controller\n",
    "            # \n",
    "            if not isinstance(model, bc_LSTM_MDN):                \n",
    "                a_pred = model(z_tensor)\n",
    "            else:\n",
    "                a_pred = model.forward_and_sample(z_tensor)\n",
    "            a_pred_list.append(a_pred.to(\"cpu\").squeeze(0))\n",
    "            rp = demo.get_action(i, \"rc-position-target\", exp_robot)\n",
    "            # a = demo.get_action(i)\n",
    "            # rp = RobotPosition.from_vector(exp_robot, a)\n",
    "            a_teacher = rp.to_normalized_vector(exp_robot)            \n",
    "            a_teacher_list.append(a_teacher)\n",
    "    a_pred_np = np.array(a_pred_list)\n",
    "    a_teacher_np = np.array(a_teacher_list)\n",
    "\n",
    "    # save the comparison terms\n",
    "    np.save(pred_path, a_pred_np)\n",
    "    np.save(teacher_path, a_teacher_np)   \n",
    "    return a_pred_np, a_teacher_np            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_forcing(sp, model, sequence_length, data_spec, pred_path, teacher_path):\n",
    "    \"\"\"Cached version of teacher forcing, returns the precomputed values if they exist.\"\"\"\n",
    "    if pred_path.exists() and teacher_path.exists():\n",
    "        a_pred_np = np.load(pred_path)\n",
    "        a_teacher_np = np.load(teacher_path)\n",
    "        print(f\"Teacher forcing: results successfully loaded from cache\")\n",
    "        return a_pred_np, a_teacher_np\n",
    "    else:\n",
    "        print(f\"Cached versions:\\n\\t{pred_path} and\\n\\t{teacher_path} do not exists, running teacher forcing\")\n",
    "        return run_teacher_forcing(sp = sp, model = model, sequence_length = sequence_length, data_spec = data_spec, pred_path = pred_path, teacher_path = teacher_path )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2406833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher forcing: results successfully loaded from cache\n",
      "Teacher forcing: results successfully loaded from cache\n"
     ]
    }
   ],
   "source": [
    "sequence_length = exp_bc[\"sequence_length\"]\n",
    "\n",
    "# run for the verification data \n",
    "pred_path = pathlib.Path(exp.data_dir(), \"teaforce_predicted_validdata.npy\").expanduser()\n",
    "teacher_path = pathlib.Path(exp.data_dir(), \"teaforce_teacher_validdata.npy\").expanduser()\n",
    "a_pred_verif_np, a_teacher_verif_np = teacher_forcing(sp, model, sequence_length, exp[\"validation_data\"], pred_path = pred_path, teacher_path = teacher_path)\n",
    "\n",
    "# run for the training data \n",
    "pred_path = pathlib.Path(exp.data_dir(), \"teaforce_predicted_traindata.npy\").expanduser()\n",
    "teacher_path = pathlib.Path(exp.data_dir(), \"teaforce_teacher_traindata.npy\").expanduser()\n",
    "a_pred_train_np, a_teacher_train_np = teacher_forcing(sp, model, sequence_length, exp[\"training_data\"], pred_path = pred_path, teacher_path = teacher_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b02810",
   "metadata": {},
   "source": [
    "## Plot the differences between the a_real and a_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_teacher_forcing_plots(a_pred_np, a_real_np, fileroot):\n",
    "    fig_verify_pdf_path = pathlib.Path(exp.data_dir(), f\"{fileroot}.pdf\").expanduser()\n",
    "    fig_verify_jpg_path = pathlib.Path(exp.data_dir(), f\"{fileroot}.jpg\").expanduser()\n",
    "    if fig_verify_pdf_path.exists() and fig_verify_jpg_path.exists():\n",
    "        print(\"generate_teacher_forcing_plots: Figures already exist, do not generate them.\")\n",
    "        return\n",
    "    fig, axs = plt.subplots(2,3, figsize=(8, 6))\n",
    "    for i in range(exp_bc[\"control_size\"]):\n",
    "        ax = axs[i//3, i%3]\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.plot(a_real_np[:,i], label=\"ground truth\")\n",
    "        if len(a_pred_np.shape) == 3:\n",
    "            ax.plot(a_pred_np[:,0,i], label=exp_bc[\"name\"])\n",
    "        else:\n",
    "            ax.plot(a_pred_np[:,i], label=exp_bc[\"name\"])\n",
    "        if i==0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, ncol=2,\n",
    "                bbox_to_anchor=(0.5, 0), loc=\"upper center\")\n",
    "        ax.set_title(RobotPosition.FIELDS[i])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_verify_pdf_path, bbox_inches='tight')\n",
    "    plt.savefig(fig_verify_jpg_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48842885",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_teacher_forcing_plots(a_pred_verif_np, a_teacher_verif_np, \"teacher_forcing_verify\")\n",
    "generate_teacher_forcing_plots(a_pred_train_np, a_teacher_train_np, \"teacher_forcing_training\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BerryPicker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
